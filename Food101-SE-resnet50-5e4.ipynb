{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from PIL import ImageFile\n",
    "\n",
    "from resnet import seresnet50\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seresnet50(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        #mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        #std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n",
    "        [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        #transform.append(transforms.RandomRotation(30))\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "        transform.append(transforms.ColorJitter(hue=.05, saturation=.05),)\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "data_dir = './data/food101/'\n",
    "train_data = datasets.ImageFolder(data_dir + 'train', transform=get_transform(random_crop=True))\n",
    "test_data = datasets.ImageFolder(data_dir + 'test', transform=get_transform(random_crop=False))\n",
    "tr_loader = data.DataLoader(dataset=train_data,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            #pin_memory=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=test_data,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,\n",
    "                             #pin_memory=True,                             \n",
    "                             num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26245973"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "test 0:00:30.963236\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "test_time = datetime.datetime.now()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()\n",
    "test_end = datetime.datetime.now() - test_time\n",
    "print('test {}'.format(test_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=30, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 99/296]\tBatch 64.594 (37.115)\tData 64.110 (36.630)\tLoss 4.59936 (4.63962)\tAcc@1   1.56 (  1.49)\tAcc@5   6.25 (  6.20)\tLearningRate 0.00475 (0.00425)\n",
      "Epoch: [0][199/296]\tBatch 120.943 (64.899)\tData 120.456 (64.412)\tLoss 4.48013 (4.59019)\tAcc@1   3.52 (  2.15)\tAcc@5  16.41 (  8.41)\tLearningRate 0.00698 (0.00500)\n",
      "0:02:52.021192 elapsed for 1\n",
      "Epoch: [1][ 99/296]\tBatch 64.711 (77.975)\tData 64.286 (77.492)\tLoss 4.20553 (4.46757)\tAcc@1   7.42 (  3.68)\tAcc@5  23.83 ( 12.90)\tLearningRate 0.01531 (0.00785)\n",
      "Epoch: [1][199/296]\tBatch 121.505 (81.079)\tData 121.000 (80.594)\tLoss 4.16423 (4.40946)\tAcc@1  10.16 (  4.58)\tAcc@5  21.88 ( 14.96)\tLearningRate 0.02134 (0.00995)\n",
      "0:02:52.228428 elapsed for 2\n",
      "Epoch: [2][ 99/296]\tBatch 65.952 (84.094)\tData 65.424 (83.612)\tLoss 3.96431 (4.29958)\tAcc@1  11.33 (  6.15)\tAcc@5  28.12 ( 18.61)\tLearningRate 0.03572 (0.01515)\n",
      "Epoch: [2][199/296]\tBatch 123.054 (85.433)\tData 122.620 (84.949)\tLoss 3.86217 (4.24325)\tAcc@1   9.38 (  6.98)\tAcc@5  29.69 ( 20.39)\tLearningRate 0.04393 (0.01827)\n",
      "0:02:53.335799 elapsed for 3\n",
      "Epoch: [3][ 99/296]\tBatch 65.047 (86.802)\tData 64.573 (86.319)\tLoss 3.54663 (4.13061)\tAcc@1  17.58 (  8.67)\tAcc@5  38.28 ( 23.86)\tLearningRate 0.06049 (0.02501)\n",
      "Epoch: [3][199/296]\tBatch 122.402 (87.451)\tData 121.940 (86.968)\tLoss 3.39513 (4.07332)\tAcc@1  21.48 (  9.55)\tAcc@5  43.75 ( 25.58)\tLearningRate 0.06868 (0.02866)\n",
      "0:02:53.230929 elapsed for 4\n",
      "Epoch: [4][ 99/296]\tBatch 65.600 (88.176)\tData 65.115 (87.693)\tLoss 3.28648 (3.95991)\tAcc@1  24.22 ( 11.39)\tAcc@5  49.22 ( 28.89)\tLearningRate 0.08299 (0.03590)\n",
      "Epoch: [4][199/296]\tBatch 121.544 (88.567)\tData 120.972 (88.084)\tLoss 3.23980 (3.90254)\tAcc@1  21.48 ( 12.38)\tAcc@5  49.22 ( 30.50)\tLearningRate 0.08896 (0.03953)\n",
      "0:02:52.903290 elapsed for 5\n",
      "Epoch: [5][ 99/296]\tBatch 65.027 (88.996)\tData 64.597 (88.514)\tLoss 2.87493 (3.79328)\tAcc@1  33.98 ( 14.28)\tAcc@5  57.03 ( 33.56)\tLearningRate 0.09717 (0.04622)\n",
      "Epoch: [5][199/296]\tBatch 121.339 (89.271)\tData 120.863 (88.788)\tLoss 2.94161 (3.73986)\tAcc@1  25.39 ( 15.25)\tAcc@5  56.25 ( 35.01)\tLearningRate 0.09932 (0.04933)\n",
      "0:02:52.529038 elapsed for 6\n",
      "Epoch: [6][ 99/296]\tBatch 65.566 (89.531)\tData 65.103 (89.049)\tLoss 2.57308 (3.63570)\tAcc@1  39.45 ( 17.17)\tAcc@5  60.94 ( 37.79)\tLearningRate 0.09995 (0.05461)\n",
      "Epoch: [6][199/296]\tBatch 121.629 (89.752)\tData 121.132 (89.269)\tLoss 2.43524 (3.58566)\tAcc@1  37.89 ( 18.10)\tAcc@5  69.92 ( 39.11)\tLearningRate 0.09980 (0.05690)\n",
      "0:02:53.191885 elapsed for 7\n",
      "Epoch: [7][ 99/296]\tBatch 65.577 (89.971)\tData 65.111 (89.488)\tLoss 2.63548 (3.49032)\tAcc@1  39.45 ( 19.93)\tAcc@5  60.94 ( 41.54)\tLearningRate 0.09923 (0.06075)\n",
      "Epoch: [7][199/296]\tBatch 120.366 (90.118)\tData 119.842 (89.635)\tLoss 2.40167 (3.44448)\tAcc@1  40.62 ( 20.80)\tAcc@5  67.19 ( 42.69)\tLearningRate 0.09880 (0.06243)\n",
      "0:02:52.022057 elapsed for 8\n",
      "Epoch: [8][ 99/296]\tBatch 65.752 (90.232)\tData 65.281 (89.749)\tLoss 2.03981 (3.35949)\tAcc@1  47.27 ( 22.44)\tAcc@5  78.12 ( 44.81)\tLearningRate 0.09767 (0.06528)\n",
      "Epoch: [8][199/296]\tBatch 121.949 (90.384)\tData 121.523 (89.901)\tLoss 2.26067 (3.31789)\tAcc@1  42.58 ( 23.25)\tAcc@5  68.36 ( 45.82)\tLearningRate 0.09696 (0.06653)\n",
      "0:02:52.662365 elapsed for 9\n",
      "Epoch: [9][ 99/296]\tBatch 64.643 (90.484)\tData 64.183 (90.001)\tLoss 2.12319 (3.24109)\tAcc@1  49.22 ( 24.77)\tAcc@5  73.44 ( 47.65)\tLearningRate 0.09529 (0.06863)\n",
      "Epoch: [9][199/296]\tBatch 121.350 (90.582)\tData 120.823 (90.099)\tLoss 2.32704 (3.20437)\tAcc@1  41.80 ( 25.50)\tAcc@5  71.48 ( 48.52)\tLearningRate 0.09431 (0.06954)\n",
      "0:02:52.120709 elapsed for 10\n",
      "Epoch: [10][ 99/296]\tBatch 65.457 (90.643)\tData 64.962 (90.160)\tLoss 2.16287 (3.13513)\tAcc@1  44.14 ( 26.89)\tAcc@5  71.48 ( 50.15)\tLearningRate 0.09214 (0.07106)\n",
      "Epoch: [10][199/296]\tBatch 121.793 (90.748)\tData 121.346 (90.265)\tLoss 2.00377 (3.10147)\tAcc@1  44.14 ( 27.57)\tAcc@5  75.78 ( 50.93)\tLearningRate 0.09091 (0.07171)\n",
      "0:02:53.125559 elapsed for 11\n",
      "Epoch: [11][ 99/296]\tBatch 64.955 (90.809)\tData 64.480 (90.326)\tLoss 2.00945 (3.03887)\tAcc@1  47.66 ( 28.84)\tAcc@5  75.78 ( 52.37)\tLearningRate 0.08827 (0.07275)\n",
      "Epoch: [11][199/296]\tBatch 121.916 (90.895)\tData 121.388 (90.411)\tLoss 1.86776 (3.00811)\tAcc@1  52.73 ( 29.47)\tAcc@5  78.12 ( 53.06)\tLearningRate 0.08681 (0.07318)\n",
      "0:02:52.735804 elapsed for 12\n",
      "Epoch: [12][ 99/296]\tBatch 63.759 (90.926)\tData 63.276 (90.443)\tLoss 1.86329 (2.95123)\tAcc@1  53.52 ( 30.63)\tAcc@5  75.78 ( 54.34)\tLearningRate 0.08374 (0.07383)\n",
      "Epoch: [12][199/296]\tBatch 120.762 (90.968)\tData 120.288 (90.484)\tLoss 1.78811 (2.92364)\tAcc@1  53.52 ( 31.20)\tAcc@5  80.08 ( 54.96)\tLearningRate 0.08208 (0.07407)\n",
      "0:02:51.875355 elapsed for 13\n",
      "Epoch: [13][ 99/296]\tBatch 65.219 (90.987)\tData 64.652 (90.504)\tLoss 1.99890 (2.87093)\tAcc@1  54.30 ( 32.29)\tAcc@5  77.73 ( 56.13)\tLearningRate 0.07864 (0.07438)\n",
      "Epoch: [13][199/296]\tBatch 121.580 (91.042)\tData 121.088 (90.559)\tLoss 1.65700 (2.84560)\tAcc@1  58.98 ( 32.81)\tAcc@5  79.69 ( 56.69)\tLearningRate 0.07680 (0.07447)\n",
      "0:02:52.059633 elapsed for 14\n",
      "Epoch: [14][ 99/296]\tBatch 65.074 (91.060)\tData 64.598 (90.578)\tLoss 1.92899 (2.79812)\tAcc@1  51.17 ( 33.80)\tAcc@5  79.30 ( 57.73)\tLearningRate 0.07304 (0.07449)\n",
      "Epoch: [14][199/296]\tBatch 120.868 (91.108)\tData 120.365 (90.625)\tLoss 1.84714 (2.77478)\tAcc@1  53.91 ( 34.29)\tAcc@5  78.91 ( 58.24)\tLearningRate 0.07106 (0.07443)\n",
      "0:02:51.288962 elapsed for 15\n",
      "Epoch: [15][ 99/296]\tBatch 64.890 (91.115)\tData 64.339 (90.632)\tLoss 1.80074 (2.73096)\tAcc@1  54.69 ( 35.21)\tAcc@5  79.30 ( 59.18)\tLearningRate 0.06705 (0.07420)\n",
      "Epoch: [15][199/296]\tBatch 120.866 (91.165)\tData 120.407 (90.682)\tLoss 1.72326 (2.70935)\tAcc@1  55.08 ( 35.66)\tAcc@5  80.86 ( 59.65)\tLearningRate 0.06496 (0.07402)\n",
      "0:02:51.837024 elapsed for 16\n",
      "Epoch: [16][ 99/296]\tBatch 66.039 (91.184)\tData 65.554 (90.701)\tLoss 1.66091 (2.66850)\tAcc@1  57.42 ( 36.53)\tAcc@5  82.42 ( 60.52)\tLearningRate 0.06077 (0.07357)\n",
      "Epoch: [16][199/296]\tBatch 122.101 (91.242)\tData 121.620 (90.760)\tLoss 1.71288 (2.64901)\tAcc@1  54.69 ( 36.94)\tAcc@5  79.69 ( 60.94)\tLearningRate 0.05860 (0.07329)\n",
      "0:02:52.939841 elapsed for 17\n",
      "Epoch: [17][ 99/296]\tBatch 65.007 (91.265)\tData 64.522 (90.783)\tLoss 1.62258 (2.61082)\tAcc@1  58.20 ( 37.76)\tAcc@5  84.38 ( 61.74)\tLearningRate 0.05431 (0.07265)\n",
      "Epoch: [17][199/296]\tBatch 120.886 (91.306)\tData 120.414 (90.823)\tLoss 1.63385 (2.59240)\tAcc@1  60.16 ( 38.15)\tAcc@5  83.20 ( 62.12)\tLearningRate 0.05210 (0.07227)\n",
      "0:02:51.588664 elapsed for 18\n",
      "Epoch: [18][ 99/296]\tBatch 65.584 (91.310)\tData 65.113 (90.827)\tLoss 1.51457 (2.55604)\tAcc@1  60.55 ( 38.92)\tAcc@5  85.55 ( 62.88)\tLearningRate 0.04777 (0.07147)\n",
      "Epoch: [18][199/296]\tBatch 121.743 (91.355)\tData 121.259 (90.872)\tLoss 1.50155 (2.53848)\tAcc@1  59.77 ( 39.30)\tAcc@5  86.72 ( 63.24)\tLearningRate 0.04556 (0.07102)\n",
      "0:02:52.574953 elapsed for 19\n",
      "Epoch: [19][ 99/296]\tBatch 64.697 (91.366)\tData 64.222 (90.884)\tLoss 1.48796 (2.50465)\tAcc@1  63.28 ( 40.02)\tAcc@5  82.42 ( 63.93)\tLearningRate 0.04127 (0.07007)\n",
      "Epoch: [19][199/296]\tBatch 120.157 (91.386)\tData 119.657 (90.904)\tLoss 1.52153 (2.48788)\tAcc@1  60.16 ( 40.38)\tAcc@5  85.16 ( 64.28)\tLearningRate 0.03910 (0.06956)\n",
      "0:02:51.783398 elapsed for 20\n",
      "Epoch: [20][ 99/296]\tBatch 64.604 (91.372)\tData 64.134 (90.890)\tLoss 1.34843 (2.45559)\tAcc@1  67.58 ( 41.09)\tAcc@5  83.98 ( 64.92)\tLearningRate 0.03492 (0.06850)\n",
      "Epoch: [20][199/296]\tBatch 121.135 (91.401)\tData 120.633 (90.918)\tLoss 1.39464 (2.43923)\tAcc@1  63.28 ( 41.44)\tAcc@5  87.50 ( 65.26)\tLearningRate 0.03282 (0.06793)\n",
      "0:02:52.591674 elapsed for 21\n",
      "Epoch: [21][ 99/296]\tBatch 65.554 (91.414)\tData 64.998 (90.931)\tLoss 1.57191 (2.40811)\tAcc@1  60.55 ( 42.12)\tAcc@5  82.03 ( 65.88)\tLearningRate 0.02882 (0.06678)\n",
      "Epoch: [21][199/296]\tBatch 121.305 (91.447)\tData 120.794 (90.965)\tLoss 1.24797 (2.39252)\tAcc@1  68.75 ( 42.46)\tAcc@5  87.11 ( 66.19)\tLearningRate 0.02684 (0.06617)\n",
      "0:02:52.243549 elapsed for 22\n",
      "Epoch: [22][ 99/296]\tBatch 65.309 (91.454)\tData 64.830 (90.971)\tLoss 1.39937 (2.36238)\tAcc@1  61.72 ( 43.13)\tAcc@5  85.16 ( 66.77)\tLearningRate 0.02309 (0.06495)\n",
      "Epoch: [22][199/296]\tBatch 121.427 (91.486)\tData 120.938 (91.004)\tLoss 1.50108 (2.34720)\tAcc@1  64.06 ( 43.46)\tAcc@5  82.03 ( 67.07)\tLearningRate 0.02126 (0.06431)\n",
      "0:02:52.524520 elapsed for 23\n",
      "Epoch: [23][ 99/296]\tBatch 64.776 (91.488)\tData 64.350 (91.006)\tLoss 1.40060 (2.31773)\tAcc@1  65.62 ( 44.11)\tAcc@5  85.94 ( 67.63)\tLearningRate 0.01782 (0.06304)\n",
      "Epoch: [23][199/296]\tBatch 121.315 (91.510)\tData 120.832 (91.028)\tLoss 1.19036 (2.30255)\tAcc@1  69.92 ( 44.45)\tAcc@5  88.67 ( 67.92)\tLearningRate 0.01616 (0.06238)\n",
      "0:02:52.936300 elapsed for 24\n",
      "Epoch: [24][ 99/296]\tBatch 65.442 (91.522)\tData 65.016 (91.040)\tLoss 1.05802 (2.27342)\tAcc@1  70.31 ( 45.09)\tAcc@5  89.84 ( 68.46)\tLearningRate 0.01310 (0.06108)\n",
      "Epoch: [24][199/296]\tBatch 121.888 (91.551)\tData 121.405 (91.069)\tLoss 1.08751 (2.25854)\tAcc@1  70.31 ( 45.43)\tAcc@5  88.67 ( 68.74)\tLearningRate 0.01165 (0.06042)\n",
      "0:02:53.156430 elapsed for 25\n",
      "Epoch: [25][ 99/296]\tBatch 64.529 (91.556)\tData 64.056 (91.074)\tLoss 1.11635 (2.22953)\tAcc@1  71.48 ( 46.08)\tAcc@5  89.84 ( 69.27)\tLearningRate 0.00901 (0.05911)\n",
      "Epoch: [25][199/296]\tBatch 121.601 (91.579)\tData 121.128 (91.096)\tLoss 1.16396 (2.21467)\tAcc@1  71.09 ( 46.41)\tAcc@5  87.11 ( 69.54)\tLearningRate 0.00779 (0.05844)\n",
      "0:02:52.961157 elapsed for 26\n",
      "Epoch: [26][ 99/296]\tBatch 65.429 (91.590)\tData 64.955 (91.108)\tLoss 0.85046 (2.18539)\tAcc@1  78.52 ( 47.07)\tAcc@5  91.80 ( 70.06)\tLearningRate 0.00563 (0.05714)\n",
      "Epoch: [26][199/296]\tBatch 122.055 (91.619)\tData 121.503 (91.136)\tLoss 0.99778 (2.17056)\tAcc@1  73.44 ( 47.41)\tAcc@5  91.41 ( 70.32)\tLearningRate 0.00465 (0.05648)\n",
      "0:02:53.058063 elapsed for 27\n",
      "Epoch: [27][ 99/296]\tBatch 65.712 (91.626)\tData 65.232 (91.144)\tLoss 0.96382 (2.14128)\tAcc@1  74.22 ( 48.08)\tAcc@5  90.62 ( 70.83)\tLearningRate 0.00300 (0.05520)\n",
      "Epoch: [27][199/296]\tBatch 121.768 (91.656)\tData 121.241 (91.173)\tLoss 1.13742 (2.12641)\tAcc@1  73.05 ( 48.42)\tAcc@5  86.72 ( 71.09)\tLearningRate 0.00229 (0.05456)\n",
      "0:02:52.333389 elapsed for 28\n",
      "Epoch: [28][ 99/296]\tBatch 64.822 (91.657)\tData 64.309 (91.174)\tLoss 0.75908 (2.09742)\tAcc@1  80.47 ( 49.09)\tAcc@5  95.31 ( 71.58)\tLearningRate 0.00117 (0.05333)\n",
      "Epoch: [28][199/296]\tBatch 120.680 (91.675)\tData 120.242 (91.192)\tLoss 0.91154 (2.08283)\tAcc@1  74.61 ( 49.43)\tAcc@5  92.58 ( 71.83)\tLearningRate 0.00075 (0.05271)\n",
      "0:02:52.074849 elapsed for 29\n",
      "Epoch: [29][ 99/296]\tBatch 65.259 (91.669)\tData 64.771 (91.187)\tLoss 0.89080 (2.05495)\tAcc@1  74.61 ( 50.08)\tAcc@5  92.58 ( 72.30)\tLearningRate 0.00019 (0.05153)\n",
      "Epoch: [29][199/296]\tBatch 120.907 (91.690)\tData 120.440 (91.208)\tLoss 0.95411 (2.04105)\tAcc@1  76.95 ( 50.41)\tAcc@5  91.02 ( 72.53)\tLearningRate 0.00004 (0.05094)\n",
      "0:02:52.132141 elapsed for 30\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/resnet50_fp16_sconv_ep030.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8205148514851485"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_val(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_results = [classification_val(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7856330699556754"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7881094623241472,\n",
       " 0.7854114472923492,\n",
       " 0.7869531701676623,\n",
       " 0.7803044902678743,\n",
       " 0.7797263441896319,\n",
       " 0.7850260165735209,\n",
       " 0.7908074773559453,\n",
       " 0.7862786664097129,\n",
       " 0.7879167469647331,\n",
       " 0.7857968780111775]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7725380612834843"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.425454679560811e-06"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025348480584762496"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7493465346534653"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_retrieval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result = [val_retrieval(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6885912507226826"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.684428598959337,\n",
       " 0.6872229716708421,\n",
       " 0.693582578531509,\n",
       " 0.6871266139911351,\n",
       " 0.6907882058200039,\n",
       " 0.6866448255925998,\n",
       " 0.6872229716708421,\n",
       " 0.690980921179418,\n",
       " 0.6823087300057814,\n",
       " 0.6956060898053575]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6178358065137791"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0038908581994208"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6151474272499519,\n",
       " 0.6104259009443053,\n",
       " 0.615436500289073,\n",
       " 0.6237232607438813,\n",
       " 0.6200616689150126,\n",
       " 0.6146656388514165,\n",
       " 0.6214106764309115,\n",
       " 0.6160146463673155,\n",
       " 0.6200616689150126,\n",
       " 0.6214106764309115]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
