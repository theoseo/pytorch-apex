{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        #self.se = SEBlock(self.inplanes, ratio=9)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        #output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(2, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(2, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(2, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)           \n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20449700"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n",
    "        #[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        #transform.append(transforms.RandomRotation(30))\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "        transform.append(transforms.ColorJitter(hue=.05, saturation=.05),)\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "class CarsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \n",
    "        self.pd_csv = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pd_csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "    \n",
    "        img_name = os.path.join(self.root_dir, \n",
    "                                self.pd_csv.iloc[index, 1])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        target = self.pd_csv.iloc[index, 0]\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CarsDataset('./data/sfcars/train.csv', './data/sfcars/train/',\n",
    "                                 transform=get_transform(random_crop=True))\n",
    "test_dataset = CarsDataset('./data/sfcars/test.csv', './data/sfcars/test/',\n",
    "                                 transform=get_transform(random_crop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "tr_loader = data.DataLoader(dataset=train_dataset,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=test_dataset,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,                            \n",
    "                             num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=100, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][31/32]\tBatch 29.287 (22.120)\tData 29.054 (21.802)\tLoss 5.25602 (5.30172)\tAcc@1   0.48 (  0.88)\tAcc@5   4.33 (  3.56)\tLearningRate 0.00459 (0.00421)\n",
      "0:00:29.476596 elapsed for 1\n",
      "Epoch: [1][31/32]\tBatch 30.223 (22.548)\tData 29.992 (22.233)\tLoss 5.27039 (5.25705)\tAcc@1   1.92 (  1.09)\tAcc@5   6.25 (  4.21)\tLearningRate 0.00636 (0.00481)\n",
      "0:00:30.395072 elapsed for 2\n",
      "Epoch: [2][31/32]\tBatch 30.096 (22.601)\tData 29.864 (22.282)\tLoss 5.17151 (5.22916)\tAcc@1   1.92 (  1.31)\tAcc@5   6.73 (  4.91)\tLearningRate 0.00925 (0.00579)\n",
      "0:00:30.254716 elapsed for 3\n",
      "Epoch: [3][31/32]\tBatch 29.859 (22.493)\tData 29.626 (22.175)\tLoss 5.28092 (5.20646)\tAcc@1   0.00 (  1.52)\tAcc@5   3.37 (  5.52)\tLearningRate 0.01319 (0.00714)\n",
      "0:00:30.011335 elapsed for 4\n",
      "Epoch: [4][31/32]\tBatch 29.934 (22.407)\tData 29.702 (22.088)\tLoss 5.22880 (5.18755)\tAcc@1   2.40 (  1.69)\tAcc@5   7.21 (  6.01)\tLearningRate 0.01810 (0.00884)\n",
      "0:00:30.092768 elapsed for 5\n",
      "Epoch: [5][31/32]\tBatch 29.588 (22.274)\tData 29.358 (21.956)\tLoss 5.03406 (5.16872)\tAcc@1   3.37 (  1.85)\tAcc@5  11.54 (  6.62)\tLearningRate 0.02384 (0.01087)\n",
      "0:00:29.736213 elapsed for 6\n",
      "Epoch: [6][31/32]\tBatch 29.696 (22.353)\tData 29.464 (22.034)\tLoss 4.94804 (5.14627)\tAcc@1   3.85 (  2.04)\tAcc@5  12.02 (  7.23)\tLearningRate 0.03028 (0.01319)\n",
      "0:00:29.848791 elapsed for 7\n",
      "Epoch: [7][31/32]\tBatch 29.616 (22.375)\tData 29.384 (22.057)\tLoss 4.91196 (5.12421)\tAcc@1   1.44 (  2.20)\tAcc@5  11.06 (  7.81)\tLearningRate 0.03726 (0.01577)\n",
      "0:00:29.774804 elapsed for 8\n",
      "Epoch: [8][31/32]\tBatch 29.693 (22.361)\tData 29.461 (22.042)\tLoss 5.01253 (5.10293)\tAcc@1   2.88 (  2.37)\tAcc@5   9.62 (  8.33)\tLearningRate 0.04460 (0.01858)\n",
      "0:00:29.835205 elapsed for 9\n",
      "Epoch: [9][31/32]\tBatch 29.106 (22.304)\tData 28.873 (21.985)\tLoss 4.92351 (5.07854)\tAcc@1   4.81 (  2.54)\tAcc@5  12.98 (  8.84)\tLearningRate 0.05212 (0.02157)\n",
      "0:00:29.248679 elapsed for 10\n",
      "Epoch: [10][31/32]\tBatch 29.332 (22.266)\tData 29.100 (21.947)\tLoss 4.88468 (5.05409)\tAcc@1   2.40 (  2.72)\tAcc@5  14.90 (  9.41)\tLearningRate 0.05964 (0.02470)\n",
      "0:00:29.476734 elapsed for 11\n",
      "Epoch: [11][31/32]\tBatch 28.909 (22.186)\tData 28.676 (21.867)\tLoss 4.76478 (5.02875)\tAcc@1   4.81 (  2.92)\tAcc@5  14.42 (  9.93)\tLearningRate 0.06697 (0.02792)\n",
      "0:00:29.044095 elapsed for 12\n",
      "Epoch: [12][31/32]\tBatch 30.349 (22.176)\tData 30.118 (21.857)\tLoss 4.70238 (5.00081)\tAcc@1   4.33 (  3.13)\tAcc@5  17.31 ( 10.55)\tLearningRate 0.07393 (0.03121)\n",
      "0:00:30.484038 elapsed for 13\n",
      "Epoch: [13][31/32]\tBatch 28.823 (22.117)\tData 28.590 (21.798)\tLoss 4.56599 (4.97230)\tAcc@1   8.17 (  3.35)\tAcc@5  22.12 ( 11.18)\tLearningRate 0.08035 (0.03450)\n",
      "0:00:28.972107 elapsed for 14\n",
      "Epoch: [14][31/32]\tBatch 29.132 (22.094)\tData 28.899 (21.775)\tLoss 4.44334 (4.94383)\tAcc@1   5.77 (  3.57)\tAcc@5  22.12 ( 11.79)\tLearningRate 0.08607 (0.03776)\n",
      "0:00:29.276408 elapsed for 15\n",
      "Epoch: [15][31/32]\tBatch 29.202 (22.061)\tData 28.969 (21.742)\tLoss 4.35801 (4.91353)\tAcc@1  10.10 (  3.82)\tAcc@5  26.92 ( 12.49)\tLearningRate 0.09094 (0.04094)\n",
      "0:00:29.342300 elapsed for 16\n",
      "Epoch: [16][31/32]\tBatch 29.552 (22.070)\tData 29.320 (21.751)\tLoss 4.19885 (4.88062)\tAcc@1   9.13 (  4.10)\tAcc@5  28.85 ( 13.23)\tLearningRate 0.09486 (0.04400)\n",
      "0:00:29.688374 elapsed for 17\n",
      "Epoch: [17][31/32]\tBatch 29.759 (22.057)\tData 29.526 (21.737)\tLoss 4.34195 (4.84711)\tAcc@1   9.13 (  4.38)\tAcc@5  27.88 ( 13.96)\tLearningRate 0.09772 (0.04691)\n",
      "0:00:29.914089 elapsed for 18\n",
      "Epoch: [18][31/32]\tBatch 29.923 (22.075)\tData 29.691 (21.756)\tLoss 4.11408 (4.81206)\tAcc@1  11.06 (  4.69)\tAcc@5  29.33 ( 14.74)\tLearningRate 0.09944 (0.04964)\n",
      "0:00:30.066717 elapsed for 19\n",
      "Epoch: [19][31/32]\tBatch 29.513 (22.044)\tData 29.282 (21.724)\tLoss 4.47300 (4.77662)\tAcc@1   7.21 (  5.01)\tAcc@5  22.12 ( 15.57)\tLearningRate 0.10000 (0.05215)\n",
      "0:00:29.659046 elapsed for 20\n",
      "Epoch: [20][31/32]\tBatch 28.974 (21.995)\tData 28.741 (21.676)\tLoss 4.16069 (4.73849)\tAcc@1  11.06 (  5.38)\tAcc@5  28.85 ( 16.48)\tLearningRate 0.09996 (0.05443)\n",
      "0:00:29.116604 elapsed for 21\n",
      "Epoch: [21][31/32]\tBatch 29.442 (21.972)\tData 29.210 (21.652)\tLoss 3.89546 (4.69740)\tAcc@1  12.50 (  5.82)\tAcc@5  33.17 ( 17.43)\tLearningRate 0.09984 (0.05649)\n",
      "0:00:29.583835 elapsed for 22\n",
      "Epoch: [22][31/32]\tBatch 29.395 (21.955)\tData 29.163 (21.635)\tLoss 3.68074 (4.65588)\tAcc@1  14.90 (  6.25)\tAcc@5  40.87 ( 18.39)\tLearningRate 0.09965 (0.05838)\n",
      "0:00:29.538287 elapsed for 23\n",
      "Epoch: [23][31/32]\tBatch 29.350 (21.947)\tData 29.118 (21.627)\tLoss 3.69374 (4.61126)\tAcc@1  17.79 (  6.76)\tAcc@5  40.38 ( 19.44)\tLearningRate 0.09937 (0.06009)\n",
      "0:00:29.506999 elapsed for 24\n",
      "Epoch: [24][31/32]\tBatch 29.681 (21.967)\tData 29.449 (21.647)\tLoss 3.60999 (4.56553)\tAcc@1  20.19 (  7.29)\tAcc@5  44.23 ( 20.52)\tLearningRate 0.09903 (0.06165)\n",
      "0:00:29.820582 elapsed for 25\n",
      "Epoch: [25][31/32]\tBatch 29.701 (21.976)\tData 29.469 (21.656)\tLoss 3.42114 (4.51802)\tAcc@1  21.63 (  7.89)\tAcc@5  48.08 ( 21.69)\tLearningRate 0.09860 (0.06308)\n",
      "0:00:29.836635 elapsed for 26\n",
      "Epoch: [26][31/32]\tBatch 29.969 (21.976)\tData 29.738 (21.656)\tLoss 3.31002 (4.46768)\tAcc@1  26.44 (  8.56)\tAcc@5  52.88 ( 22.89)\tLearningRate 0.09811 (0.06439)\n",
      "0:00:30.101413 elapsed for 27\n",
      "Epoch: [27][31/32]\tBatch 29.836 (21.991)\tData 29.604 (21.671)\tLoss 3.00770 (4.41626)\tAcc@1  25.00 (  9.24)\tAcc@5  58.17 ( 24.09)\tLearningRate 0.09753 (0.06558)\n",
      "0:00:29.975831 elapsed for 28\n",
      "Epoch: [28][31/32]\tBatch 29.764 (21.978)\tData 29.533 (21.658)\tLoss 2.86497 (4.36280)\tAcc@1  29.33 (  9.98)\tAcc@5  58.17 ( 25.36)\tLearningRate 0.09689 (0.06667)\n",
      "0:00:29.900434 elapsed for 29\n",
      "Epoch: [29][31/32]\tBatch 29.731 (21.985)\tData 29.499 (21.666)\tLoss 2.90289 (4.31043)\tAcc@1  28.37 ( 10.74)\tAcc@5  61.54 ( 26.58)\tLearningRate 0.09617 (0.06767)\n",
      "0:00:29.886383 elapsed for 30\n",
      "Epoch: [30][31/32]\tBatch 29.200 (21.964)\tData 28.967 (21.645)\tLoss 2.76889 (4.25630)\tAcc@1  36.06 ( 11.55)\tAcc@5  62.02 ( 27.83)\tLearningRate 0.09538 (0.06858)\n",
      "0:00:29.338510 elapsed for 31\n",
      "Epoch: [31][31/32]\tBatch 29.905 (21.978)\tData 29.674 (21.658)\tLoss 2.68755 (4.20145)\tAcc@1  30.29 ( 12.37)\tAcc@5  59.13 ( 29.08)\tLearningRate 0.09452 (0.06940)\n",
      "0:00:30.044799 elapsed for 32\n",
      "Epoch: [32][31/32]\tBatch 29.402 (21.957)\tData 29.169 (21.638)\tLoss 2.37814 (4.14631)\tAcc@1  41.83 ( 13.25)\tAcc@5  66.83 ( 30.31)\tLearningRate 0.09359 (0.07015)\n",
      "0:00:29.535899 elapsed for 33\n",
      "Epoch: [33][31/32]\tBatch 29.933 (21.956)\tData 29.702 (21.637)\tLoss 2.34609 (4.09164)\tAcc@1  45.19 ( 14.12)\tAcc@5  71.63 ( 31.53)\tLearningRate 0.09260 (0.07082)\n",
      "0:00:30.073760 elapsed for 34\n",
      "Epoch: [34][31/32]\tBatch 29.882 (21.965)\tData 29.649 (21.645)\tLoss 2.33187 (4.03594)\tAcc@1  41.83 ( 15.05)\tAcc@5  73.08 ( 32.75)\tLearningRate 0.09154 (0.07143)\n",
      "0:00:30.028758 elapsed for 35\n",
      "Epoch: [35][31/32]\tBatch 29.493 (21.982)\tData 29.261 (21.662)\tLoss 2.27051 (3.98113)\tAcc@1  46.63 ( 15.97)\tAcc@5  71.15 ( 33.93)\tLearningRate 0.09041 (0.07197)\n",
      "0:00:29.625727 elapsed for 36\n",
      "Epoch: [36][31/32]\tBatch 29.347 (21.973)\tData 29.115 (21.654)\tLoss 2.08693 (3.92599)\tAcc@1  49.04 ( 16.91)\tAcc@5  72.60 ( 35.11)\tLearningRate 0.08923 (0.07245)\n",
      "0:00:29.487598 elapsed for 37\n",
      "Epoch: [37][31/32]\tBatch 29.656 (21.975)\tData 29.425 (21.655)\tLoss 1.80298 (3.87129)\tAcc@1  57.21 ( 17.87)\tAcc@5  78.85 ( 36.29)\tLearningRate 0.08798 (0.07288)\n",
      "0:00:29.793719 elapsed for 38\n",
      "Epoch: [38][31/32]\tBatch 29.645 (21.985)\tData 29.413 (21.666)\tLoss 1.73129 (3.81765)\tAcc@1  57.69 ( 18.84)\tAcc@5  78.85 ( 37.41)\tLearningRate 0.08667 (0.07325)\n",
      "0:00:29.792779 elapsed for 39\n",
      "Epoch: [39][31/32]\tBatch 29.244 (21.973)\tData 29.012 (21.654)\tLoss 1.75893 (3.76419)\tAcc@1  53.85 ( 19.80)\tAcc@5  77.88 ( 38.52)\tLearningRate 0.08531 (0.07357)\n",
      "0:00:29.394909 elapsed for 40\n",
      "Epoch: [40][31/32]\tBatch 29.810 (21.974)\tData 29.577 (21.655)\tLoss 1.53524 (3.71121)\tAcc@1  62.02 ( 20.78)\tAcc@5  82.69 ( 39.61)\tLearningRate 0.08390 (0.07383)\n",
      "0:00:29.941496 elapsed for 41\n",
      "Epoch: [41][31/32]\tBatch 29.882 (21.986)\tData 29.651 (21.667)\tLoss 1.77434 (3.66021)\tAcc@1  57.21 ( 21.72)\tAcc@5  78.37 ( 40.65)\tLearningRate 0.08243 (0.07406)\n",
      "0:00:30.040422 elapsed for 42\n",
      "Epoch: [42][31/32]\tBatch 29.562 (21.982)\tData 29.329 (21.662)\tLoss 1.50662 (3.60975)\tAcc@1  61.54 ( 22.65)\tAcc@5  84.62 ( 41.67)\tLearningRate 0.08091 (0.07423)\n",
      "0:00:29.711072 elapsed for 43\n",
      "Epoch: [43][31/32]\tBatch 29.414 (21.976)\tData 29.182 (21.657)\tLoss 1.58229 (3.56050)\tAcc@1  61.54 ( 23.58)\tAcc@5  83.17 ( 42.66)\tLearningRate 0.07934 (0.07437)\n",
      "0:00:29.563242 elapsed for 44\n",
      "Epoch: [44][31/32]\tBatch 29.621 (21.981)\tData 29.388 (21.661)\tLoss 1.66415 (3.51314)\tAcc@1  59.62 ( 24.48)\tAcc@5  82.21 ( 43.61)\tLearningRate 0.07773 (0.07446)\n",
      "0:00:29.774161 elapsed for 45\n",
      "Epoch: [45][31/32]\tBatch 29.117 (21.972)\tData 28.885 (21.652)\tLoss 1.25434 (3.46637)\tAcc@1  71.63 ( 25.38)\tAcc@5  87.98 ( 44.52)\tLearningRate 0.07607 (0.07451)\n",
      "0:00:29.258289 elapsed for 46\n",
      "Epoch: [46][31/32]\tBatch 29.442 (21.966)\tData 29.212 (21.646)\tLoss 1.42400 (3.41960)\tAcc@1  64.42 ( 26.30)\tAcc@5  85.10 ( 45.44)\tLearningRate 0.07438 (0.07453)\n",
      "0:00:29.579450 elapsed for 47\n",
      "Epoch: [47][31/32]\tBatch 29.545 (21.961)\tData 29.313 (21.641)\tLoss 1.40859 (3.37423)\tAcc@1  66.83 ( 27.19)\tAcc@5  82.69 ( 46.32)\tLearningRate 0.07264 (0.07450)\n",
      "0:00:29.679089 elapsed for 48\n",
      "Epoch: [48][31/32]\tBatch 29.547 (21.950)\tData 29.315 (21.630)\tLoss 1.20297 (3.33044)\tAcc@1  69.23 ( 28.04)\tAcc@5  87.98 ( 47.16)\tLearningRate 0.07088 (0.07445)\n",
      "0:00:29.690554 elapsed for 49\n",
      "Epoch: [49][31/32]\tBatch 29.114 (21.943)\tData 28.882 (21.623)\tLoss 1.43579 (3.28756)\tAcc@1  63.46 ( 28.89)\tAcc@5  84.13 ( 47.98)\tLearningRate 0.06908 (0.07436)\n",
      "0:00:29.260330 elapsed for 50\n",
      "Epoch: [50][31/32]\tBatch 29.693 (21.940)\tData 29.461 (21.620)\tLoss 1.24123 (3.24524)\tAcc@1  67.31 ( 29.72)\tAcc@5  86.54 ( 48.79)\tLearningRate 0.06725 (0.07424)\n",
      "0:00:29.832316 elapsed for 51\n",
      "Epoch: [51][31/32]\tBatch 29.829 (21.948)\tData 29.597 (21.629)\tLoss 0.98940 (3.20364)\tAcc@1  74.52 ( 30.55)\tAcc@5  90.87 ( 49.58)\tLearningRate 0.06539 (0.07408)\n",
      "0:00:29.956506 elapsed for 52\n",
      "Epoch: [52][31/32]\tBatch 28.641 (21.932)\tData 28.409 (21.613)\tLoss 0.82333 (3.16305)\tAcc@1  78.37 ( 31.36)\tAcc@5  93.75 ( 50.34)\tLearningRate 0.06351 (0.07390)\n",
      "0:00:28.778805 elapsed for 53\n",
      "Epoch: [53][31/32]\tBatch 29.382 (21.932)\tData 29.149 (21.613)\tLoss 1.07776 (3.12302)\tAcc@1  72.12 ( 32.18)\tAcc@5  89.42 ( 51.09)\tLearningRate 0.06161 (0.07369)\n",
      "0:00:29.519460 elapsed for 54\n",
      "Epoch: [54][31/32]\tBatch 29.877 (21.926)\tData 29.645 (21.606)\tLoss 1.01777 (3.08433)\tAcc@1  72.12 ( 32.97)\tAcc@5  92.31 ( 51.81)\tLearningRate 0.05969 (0.07345)\n",
      "0:00:30.015717 elapsed for 55\n",
      "Epoch: [55][31/32]\tBatch 29.965 (21.951)\tData 29.733 (21.631)\tLoss 0.80002 (3.04592)\tAcc@1  78.37 ( 33.75)\tAcc@5  93.75 ( 52.51)\tLearningRate 0.05776 (0.07319)\n",
      "0:00:30.108978 elapsed for 56\n",
      "Epoch: [56][31/32]\tBatch 29.435 (21.948)\tData 29.204 (21.628)\tLoss 1.09532 (3.00964)\tAcc@1  69.71 ( 34.48)\tAcc@5  89.90 ( 53.19)\tLearningRate 0.05582 (0.07290)\n",
      "0:00:29.575365 elapsed for 57\n",
      "Epoch: [57][31/32]\tBatch 29.219 (21.943)\tData 28.987 (21.623)\tLoss 1.05420 (2.97356)\tAcc@1  73.56 ( 35.22)\tAcc@5  88.94 ( 53.85)\tLearningRate 0.05386 (0.07259)\n",
      "0:00:29.376439 elapsed for 58\n",
      "Epoch: [58][31/32]\tBatch 30.116 (21.954)\tData 29.884 (21.634)\tLoss 0.83046 (2.93754)\tAcc@1  79.33 ( 35.97)\tAcc@5  91.83 ( 54.49)\tLearningRate 0.05190 (0.07225)\n",
      "0:00:30.257838 elapsed for 59\n",
      "Epoch: [59][31/32]\tBatch 29.435 (21.963)\tData 29.203 (21.644)\tLoss 1.04290 (2.90315)\tAcc@1  75.48 ( 36.68)\tAcc@5  87.50 ( 55.11)\tLearningRate 0.04994 (0.07190)\n",
      "0:00:29.564417 elapsed for 60\n",
      "Epoch: [60][31/32]\tBatch 28.785 (21.955)\tData 28.553 (21.635)\tLoss 0.98412 (2.86928)\tAcc@1  76.44 ( 37.38)\tAcc@5  88.46 ( 55.72)\tLearningRate 0.04798 (0.07152)\n",
      "0:00:28.934413 elapsed for 61\n",
      "Epoch: [61][31/32]\tBatch 29.276 (21.947)\tData 29.044 (21.628)\tLoss 0.85194 (2.83603)\tAcc@1  79.33 ( 38.07)\tAcc@5  94.71 ( 56.31)\tLearningRate 0.04602 (0.07113)\n",
      "0:00:29.416200 elapsed for 62\n",
      "Epoch: [62][31/32]\tBatch 30.061 (21.958)\tData 29.829 (21.638)\tLoss 0.81278 (2.80385)\tAcc@1  80.29 ( 38.74)\tAcc@5  92.31 ( 56.88)\tLearningRate 0.04406 (0.07071)\n",
      "0:00:30.210194 elapsed for 63\n",
      "Epoch: [63][31/32]\tBatch 29.916 (21.967)\tData 29.684 (21.647)\tLoss 0.88689 (2.77232)\tAcc@1  78.37 ( 39.41)\tAcc@5  91.83 ( 57.43)\tLearningRate 0.04212 (0.07028)\n",
      "0:00:30.052963 elapsed for 64\n",
      "Epoch: [64][31/32]\tBatch 29.286 (21.966)\tData 29.054 (21.646)\tLoss 0.68945 (2.74124)\tAcc@1  83.17 ( 40.06)\tAcc@5  94.23 ( 57.97)\tLearningRate 0.04019 (0.06983)\n",
      "0:00:29.430096 elapsed for 65\n",
      "Epoch: [65][31/32]\tBatch 29.900 (21.977)\tData 29.668 (21.658)\tLoss 0.85784 (2.71067)\tAcc@1  80.29 ( 40.71)\tAcc@5  90.38 ( 58.51)\tLearningRate 0.03827 (0.06937)\n",
      "0:00:30.054274 elapsed for 66\n",
      "Epoch: [66][31/32]\tBatch 29.422 (21.981)\tData 29.190 (21.661)\tLoss 0.77563 (2.68114)\tAcc@1  82.69 ( 41.33)\tAcc@5  92.31 ( 59.02)\tLearningRate 0.03637 (0.06889)\n",
      "0:00:29.564552 elapsed for 67\n",
      "Epoch: [67][31/32]\tBatch 28.793 (21.969)\tData 28.561 (21.650)\tLoss 0.55582 (2.65219)\tAcc@1  86.54 ( 41.95)\tAcc@5  94.71 ( 59.52)\tLearningRate 0.03449 (0.06839)\n",
      "0:00:28.941616 elapsed for 68\n",
      "Epoch: [68][31/32]\tBatch 30.188 (21.984)\tData 29.955 (21.665)\tLoss 0.68168 (2.62366)\tAcc@1  84.62 ( 42.56)\tAcc@5  93.75 ( 60.02)\tLearningRate 0.03264 (0.06789)\n",
      "0:00:30.324118 elapsed for 69\n",
      "Epoch: [69][31/32]\tBatch 29.896 (21.992)\tData 29.664 (21.673)\tLoss 0.82162 (2.59627)\tAcc@1  79.33 ( 43.14)\tAcc@5  90.87 ( 60.49)\tLearningRate 0.03081 (0.06737)\n",
      "0:00:30.031599 elapsed for 70\n",
      "Epoch: [70][31/32]\tBatch 30.092 (21.994)\tData 29.860 (21.675)\tLoss 0.57267 (2.56874)\tAcc@1  87.98 ( 43.73)\tAcc@5  95.67 ( 60.96)\tLearningRate 0.02901 (0.06684)\n",
      "0:00:30.232835 elapsed for 71\n",
      "Epoch: [71][31/32]\tBatch 29.428 (21.986)\tData 29.195 (21.667)\tLoss 0.57572 (2.54185)\tAcc@1  87.98 ( 44.30)\tAcc@5  95.19 ( 61.42)\tLearningRate 0.02725 (0.06631)\n",
      "0:00:29.574830 elapsed for 72\n",
      "Epoch: [72][31/32]\tBatch 30.348 (21.992)\tData 30.115 (21.673)\tLoss 0.61638 (2.51568)\tAcc@1  81.73 ( 44.85)\tAcc@5  96.15 ( 61.87)\tLearningRate 0.02552 (0.06576)\n",
      "0:00:30.493095 elapsed for 73\n",
      "Epoch: [73][31/32]\tBatch 29.646 (21.989)\tData 29.413 (21.670)\tLoss 0.59199 (2.48963)\tAcc@1  84.62 ( 45.42)\tAcc@5  95.67 ( 62.31)\tLearningRate 0.02382 (0.06520)\n",
      "0:00:29.781349 elapsed for 74\n",
      "Epoch: [74][31/32]\tBatch 29.526 (21.990)\tData 29.295 (21.671)\tLoss 0.53084 (2.46415)\tAcc@1  87.02 ( 45.96)\tAcc@5  94.71 ( 62.75)\tLearningRate 0.02217 (0.06464)\n",
      "0:00:29.668678 elapsed for 75\n",
      "Epoch: [75][31/32]\tBatch 29.575 (21.987)\tData 29.344 (21.668)\tLoss 0.57109 (2.43929)\tAcc@1  85.58 ( 46.50)\tAcc@5  92.79 ( 63.17)\tLearningRate 0.02056 (0.06407)\n",
      "0:00:29.704805 elapsed for 76\n",
      "Epoch: [76][31/32]\tBatch 29.098 (21.981)\tData 28.866 (21.662)\tLoss 0.57124 (2.41471)\tAcc@1  84.13 ( 47.03)\tAcc@5  95.19 ( 63.58)\tLearningRate 0.01900 (0.06349)\n",
      "0:00:29.241900 elapsed for 77\n",
      "Epoch: [77][31/32]\tBatch 29.355 (21.978)\tData 29.123 (21.659)\tLoss 0.52788 (2.39065)\tAcc@1  87.98 ( 47.55)\tAcc@5  95.19 ( 63.98)\tLearningRate 0.01748 (0.06291)\n",
      "0:00:29.496909 elapsed for 78\n",
      "Epoch: [78][31/32]\tBatch 30.005 (21.981)\tData 29.773 (21.662)\tLoss 0.57456 (2.36724)\tAcc@1  85.10 ( 48.05)\tAcc@5  94.23 ( 64.37)\tLearningRate 0.01602 (0.06233)\n",
      "0:00:30.145472 elapsed for 79\n",
      "Epoch: [79][31/32]\tBatch 29.507 (21.982)\tData 29.275 (21.663)\tLoss 0.47431 (2.34399)\tAcc@1  88.46 ( 48.56)\tAcc@5  94.71 ( 64.76)\tLearningRate 0.01460 (0.06174)\n",
      "0:00:29.651040 elapsed for 80\n",
      "Epoch: [80][31/32]\tBatch 29.832 (21.987)\tData 29.600 (21.668)\tLoss 0.54526 (2.32095)\tAcc@1  86.54 ( 49.05)\tAcc@5  94.23 ( 65.14)\tLearningRate 0.01324 (0.06115)\n",
      "0:00:29.977787 elapsed for 81\n",
      "Epoch: [81][31/32]\tBatch 29.423 (21.985)\tData 29.191 (21.666)\tLoss 0.44764 (2.29846)\tAcc@1  89.90 ( 49.54)\tAcc@5  95.67 ( 65.52)\tLearningRate 0.01194 (0.06056)\n",
      "0:00:29.579219 elapsed for 82\n",
      "Epoch: [82][31/32]\tBatch 29.105 (21.984)\tData 28.873 (21.665)\tLoss 0.42000 (2.27636)\tAcc@1  90.38 ( 50.02)\tAcc@5  96.63 ( 65.88)\tLearningRate 0.01070 (0.05996)\n",
      "0:00:29.243745 elapsed for 83\n",
      "Epoch: [83][31/32]\tBatch 30.053 (21.988)\tData 29.820 (21.669)\tLoss 0.44610 (2.25461)\tAcc@1  89.90 ( 50.49)\tAcc@5  95.19 ( 66.24)\tLearningRate 0.00951 (0.05937)\n",
      "0:00:30.187621 elapsed for 84\n",
      "Epoch: [84][31/32]\tBatch 29.423 (21.984)\tData 29.191 (21.665)\tLoss 0.49151 (2.23353)\tAcc@1  88.94 ( 50.95)\tAcc@5  95.19 ( 66.59)\tLearningRate 0.00839 (0.05878)\n",
      "0:00:29.558324 elapsed for 85\n",
      "Epoch: [85][31/32]\tBatch 29.168 (21.976)\tData 28.936 (21.657)\tLoss 0.44598 (2.21275)\tAcc@1  88.94 ( 51.40)\tAcc@5  97.12 ( 66.93)\tLearningRate 0.00734 (0.05819)\n",
      "0:00:29.307324 elapsed for 86\n",
      "Epoch: [86][31/32]\tBatch 30.029 (21.978)\tData 29.798 (21.658)\tLoss 0.39044 (2.19227)\tAcc@1  90.38 ( 51.85)\tAcc@5  97.12 ( 67.26)\tLearningRate 0.00635 (0.05759)\n",
      "0:00:30.169920 elapsed for 87\n",
      "Epoch: [87][31/32]\tBatch 28.863 (21.968)\tData 28.630 (21.649)\tLoss 0.36976 (2.17193)\tAcc@1  90.87 ( 52.29)\tAcc@5  97.60 ( 67.59)\tLearningRate 0.00542 (0.05701)\n",
      "0:00:29.010091 elapsed for 88\n",
      "Epoch: [88][31/32]\tBatch 30.198 (21.976)\tData 29.967 (21.657)\tLoss 0.41744 (2.15225)\tAcc@1  90.38 ( 52.72)\tAcc@5  96.15 ( 67.92)\tLearningRate 0.00457 (0.05642)\n",
      "0:00:30.343798 elapsed for 89\n",
      "Epoch: [89][31/32]\tBatch 29.763 (21.981)\tData 29.531 (21.662)\tLoss 0.30026 (2.13270)\tAcc@1  94.23 ( 53.15)\tAcc@5  98.56 ( 68.23)\tLearningRate 0.00378 (0.05584)\n",
      "0:00:29.905404 elapsed for 90\n",
      "Epoch: [90][31/32]\tBatch 29.887 (21.986)\tData 29.656 (21.667)\tLoss 0.33501 (2.11349)\tAcc@1  90.87 ( 53.57)\tAcc@5  97.12 ( 68.55)\tLearningRate 0.00307 (0.05527)\n",
      "0:00:30.032673 elapsed for 91\n",
      "Epoch: [91][31/32]\tBatch 29.112 (21.977)\tData 28.880 (21.657)\tLoss 0.20279 (2.09442)\tAcc@1  96.15 ( 53.99)\tAcc@5  98.08 ( 68.85)\tLearningRate 0.00243 (0.05469)\n",
      "0:00:29.261382 elapsed for 92\n",
      "Epoch: [92][31/32]\tBatch 29.359 (21.975)\tData 29.126 (21.655)\tLoss 0.38129 (2.07564)\tAcc@1  92.31 ( 54.40)\tAcc@5  95.67 ( 69.15)\tLearningRate 0.00186 (0.05413)\n",
      "0:00:29.501515 elapsed for 93\n",
      "Epoch: [93][31/32]\tBatch 29.651 (21.974)\tData 29.420 (21.654)\tLoss 0.30642 (2.05728)\tAcc@1  92.79 ( 54.80)\tAcc@5  97.60 ( 69.45)\tLearningRate 0.00137 (0.05357)\n",
      "0:00:29.806449 elapsed for 94\n",
      "Epoch: [94][31/32]\tBatch 29.301 (21.971)\tData 29.070 (21.652)\tLoss 0.39548 (2.03939)\tAcc@1  91.35 ( 55.19)\tAcc@5  96.15 ( 69.74)\tLearningRate 0.00095 (0.05302)\n",
      "0:00:29.444003 elapsed for 95\n",
      "Epoch: [95][31/32]\tBatch 29.340 (21.968)\tData 29.108 (21.649)\tLoss 0.41933 (2.02183)\tAcc@1  87.98 ( 55.57)\tAcc@5  96.63 ( 70.02)\tLearningRate 0.00061 (0.05247)\n",
      "0:00:29.494559 elapsed for 96\n",
      "Epoch: [96][31/32]\tBatch 29.391 (21.963)\tData 29.159 (21.644)\tLoss 0.29758 (2.00455)\tAcc@1  93.27 ( 55.95)\tAcc@5  98.08 ( 70.29)\tLearningRate 0.00034 (0.05194)\n",
      "0:00:29.523306 elapsed for 97\n",
      "Epoch: [97][31/32]\tBatch 29.837 (21.975)\tData 29.604 (21.656)\tLoss 0.35989 (1.98751)\tAcc@1  92.31 ( 56.33)\tAcc@5  96.63 ( 70.57)\tLearningRate 0.00015 (0.05141)\n",
      "0:00:29.976858 elapsed for 98\n",
      "Epoch: [98][31/32]\tBatch 29.630 (21.974)\tData 29.397 (21.655)\tLoss 0.39652 (1.97103)\tAcc@1  90.38 ( 56.69)\tAcc@5  94.23 ( 70.83)\tLearningRate 0.00004 (0.05089)\n",
      "0:00:29.767629 elapsed for 99\n",
      "Epoch: [99][31/32]\tBatch 29.535 (21.974)\tData 29.304 (21.655)\tLoss 0.29297 (1.95477)\tAcc@1  92.31 ( 57.05)\tAcc@5  98.56 ( 71.09)\tLearningRate 0.00000 (0.05038)\n",
      "0:00:29.680828 elapsed for 100\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    progress.display(i)            \n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7513990797164531"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_val(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6756622310657878"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_retrieval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/sfcar_darknet25_ep100.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
