{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        #self.se = SEBlock(self.inplanes, ratio=9)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        #output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(8, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(8, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(4, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((1,3,224,224))\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 150])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40738678"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1\n",
    "                      , momentum=0.9, weight_decay=5e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)\n",
    "\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image_id, sample, target) where target is class_index of\n",
    "                the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        #print(path)\n",
    "        #print(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        image_id = path.split('/')[-1]\n",
    "\n",
    "        return image_id, sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "data_dir = 'train/train_data'\n",
    "\n",
    "dataset = CustomDataset(data_dir, transform=get_transform(random_crop=True))\n",
    "\n",
    "split_size = int(len(dataset) * 0.9)\n",
    "train_set, valid_set = data.random_split(dataset, [split_size, len(dataset) - split_size])\n",
    "tr_loader = data.DataLoader(dataset=train_set,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=valid_set,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,\n",
    "                            pin_memory=True,                             \n",
    "                            num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    _, inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=30, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 99/365]\tBatch 58.359 (33.972)\tData 57.962 (33.559)\tLoss 4.16356 (4.49867)\tAcc@1   5.86 (  4.92)\tAcc@5  25.39 ( 17.07)\tLearningRate 0.00449 (0.00417)\n",
      "Epoch: [0][199/365]\tBatch 108.985 (59.196)\tData 108.589 (58.782)\tLoss 3.92597 (4.21799)\tAcc@1   9.38 (  8.03)\tAcc@5  34.38 ( 24.53)\tLearningRate 0.00596 (0.00466)\n",
      "Epoch: [0][299/365]\tBatch 158.525 (84.463)\tData 158.128 (84.048)\tLoss 3.56610 (4.03671)\tAcc@1  16.02 ( 10.15)\tAcc@5  41.80 ( 29.52)\tLearningRate 0.00838 (0.00548)\n",
      "0:03:10.139135 elapsed for 1\n",
      "Epoch: [1][ 99/365]\tBatch 57.872 (86.261)\tData 57.449 (85.846)\tLoss 3.35775 (3.80906)\tAcc@1  17.97 ( 13.31)\tAcc@5  43.75 ( 36.09)\tLearningRate 0.01430 (0.00750)\n",
      "Epoch: [1][199/365]\tBatch 109.194 (85.648)\tData 108.786 (85.233)\tLoss 3.10887 (3.71698)\tAcc@1  26.17 ( 14.64)\tAcc@5  55.86 ( 38.66)\tLearningRate 0.01893 (0.00910)\n",
      "Epoch: [1][299/365]\tBatch 159.726 (92.859)\tData 159.317 (92.445)\tLoss 3.26801 (3.62189)\tAcc@1  20.31 ( 16.16)\tAcc@5  51.56 ( 41.28)\tLearningRate 0.02425 (0.01098)\n",
      "0:03:10.938771 elapsed for 2\n",
      "Epoch: [2][ 99/365]\tBatch 57.997 (92.263)\tData 57.598 (91.849)\tLoss 2.72781 (3.47045)\tAcc@1  30.08 ( 18.77)\tAcc@5  61.33 ( 45.28)\tLearningRate 0.03421 (0.01459)\n",
      "Epoch: [2][199/365]\tBatch 107.993 (91.369)\tData 107.585 (90.955)\tLoss 2.81631 (3.39803)\tAcc@1  29.69 ( 20.02)\tAcc@5  63.28 ( 47.15)\tLearningRate 0.04077 (0.01705)\n",
      "Epoch: [2][299/365]\tBatch 157.398 (95.438)\tData 157.001 (95.024)\tLoss 2.73846 (3.32645)\tAcc@1  31.25 ( 21.34)\tAcc@5  67.19 ( 48.97)\tLearningRate 0.04756 (0.01969)\n",
      "0:03:07.823047 elapsed for 3\n",
      "Epoch: [3][ 99/365]\tBatch 58.514 (94.578)\tData 58.117 (94.164)\tLoss 2.70227 (3.21132)\tAcc@1  33.98 ( 23.49)\tAcc@5  70.70 ( 51.82)\tLearningRate 0.05890 (0.02433)\n",
      "Epoch: [3][199/365]\tBatch 111.834 (93.781)\tData 111.439 (93.366)\tLoss 2.32747 (3.14862)\tAcc@1  41.80 ( 24.71)\tAcc@5  69.14 ( 53.31)\tLearningRate 0.06562 (0.02726)\n",
      "Epoch: [3][299/365]\tBatch 157.494 (96.695)\tData 157.087 (96.281)\tLoss 2.28637 (3.08934)\tAcc@1  43.36 ( 25.85)\tAcc@5  75.39 ( 54.71)\tLearningRate 0.07207 (0.03024)\n",
      "0:03:07.339023 elapsed for 4\n",
      "Epoch: [4][ 99/365]\tBatch 58.357 (95.810)\tData 57.960 (95.395)\tLoss 2.05279 (2.99504)\tAcc@1  46.88 ( 27.71)\tAcc@5  76.95 ( 56.86)\tLearningRate 0.08173 (0.03519)\n",
      "Epoch: [4][199/365]\tBatch 106.995 (95.065)\tData 106.599 (94.651)\tLoss 2.30238 (2.94260)\tAcc@1  36.33 ( 28.76)\tAcc@5  73.05 ( 58.01)\tLearningRate 0.08682 (0.03816)\n",
      "Epoch: [4][299/365]\tBatch 158.947 (97.246)\tData 158.527 (96.832)\tLoss 2.03884 (2.89354)\tAcc@1  49.61 ( 29.75)\tAcc@5  77.34 ( 59.10)\tLearningRate 0.09119 (0.04105)\n",
      "0:03:09.765356 elapsed for 5\n",
      "Epoch: [5][ 99/365]\tBatch 59.790 (96.686)\tData 59.361 (96.271)\tLoss 2.07808 (2.81568)\tAcc@1  44.53 ( 31.34)\tAcc@5  76.17 ( 60.78)\tLearningRate 0.09660 (0.04560)\n",
      "Epoch: [5][199/365]\tBatch 109.662 (96.129)\tData 109.254 (95.714)\tLoss 1.85604 (2.77161)\tAcc@1  51.56 ( 32.22)\tAcc@5  80.08 ( 61.73)\tLearningRate 0.09868 (0.04817)\n",
      "Epoch: [5][299/365]\tBatch 159.797 (97.942)\tData 159.382 (97.528)\tLoss 1.73970 (2.73007)\tAcc@1  52.34 ( 33.08)\tAcc@5  80.86 ( 62.59)\tLearningRate 0.09980 (0.05058)\n",
      "0:03:09.673154 elapsed for 6\n",
      "Epoch: [6][ 99/365]\tBatch 59.816 (97.371)\tData 59.401 (96.957)\tLoss 1.68182 (2.66459)\tAcc@1  54.69 ( 34.45)\tAcc@5  82.81 ( 63.95)\tLearningRate 0.09997 (0.05414)\n",
      "Epoch: [6][199/365]\tBatch 110.160 (96.849)\tData 109.764 (96.434)\tLoss 1.56633 (2.62744)\tAcc@1  58.98 ( 35.24)\tAcc@5  86.33 ( 64.71)\tLearningRate 0.09987 (0.05605)\n",
      "Epoch: [6][299/365]\tBatch 160.586 (98.393)\tData 160.157 (97.979)\tLoss 1.82044 (2.59331)\tAcc@1  52.73 ( 35.97)\tAcc@5  81.64 ( 65.42)\tLearningRate 0.09971 (0.05781)\n",
      "0:03:09.271105 elapsed for 7\n",
      "Epoch: [7][ 99/365]\tBatch 59.405 (97.900)\tData 59.001 (97.486)\tLoss 1.59995 (2.53865)\tAcc@1  55.47 ( 37.14)\tAcc@5  84.77 ( 66.51)\tLearningRate 0.09930 (0.06040)\n",
      "Epoch: [7][199/365]\tBatch 110.045 (97.431)\tData 109.633 (97.016)\tLoss 1.56049 (2.50695)\tAcc@1  55.47 ( 37.82)\tAcc@5  84.77 ( 67.14)\tLearningRate 0.09897 (0.06181)\n",
      "Epoch: [7][299/365]\tBatch 159.150 (98.755)\tData 158.729 (98.340)\tLoss 1.66565 (2.47688)\tAcc@1  55.86 ( 38.48)\tAcc@5  83.98 ( 67.73)\tLearningRate 0.09858 (0.06310)\n",
      "0:03:08.523877 elapsed for 8\n",
      "Epoch: [8][ 99/365]\tBatch 58.209 (98.246)\tData 57.805 (97.832)\tLoss 1.55882 (2.42933)\tAcc@1  58.98 ( 39.49)\tAcc@5  83.59 ( 68.66)\tLearningRate 0.09780 (0.06502)\n",
      "Epoch: [8][199/365]\tBatch 108.913 (97.795)\tData 108.373 (97.380)\tLoss 1.35613 (2.40180)\tAcc@1  66.80 ( 40.10)\tAcc@5  86.33 ( 69.19)\tLearningRate 0.09724 (0.06606)\n",
      "Epoch: [8][299/365]\tBatch 160.897 (98.965)\tData 160.501 (98.551)\tLoss 1.38262 (2.37623)\tAcc@1  61.72 ( 40.65)\tAcc@5  90.62 ( 69.68)\tLearningRate 0.09662 (0.06702)\n",
      "0:03:10.686296 elapsed for 9\n",
      "Epoch: [9][ 99/365]\tBatch 58.682 (98.566)\tData 58.266 (98.152)\tLoss 1.58054 (2.33581)\tAcc@1  60.94 ( 41.53)\tAcc@5  83.59 ( 70.45)\tLearningRate 0.09547 (0.06844)\n",
      "Epoch: [9][199/365]\tBatch 109.344 (98.183)\tData 108.948 (97.769)\tLoss 1.50536 (2.31238)\tAcc@1  58.59 ( 42.03)\tAcc@5  84.38 ( 70.90)\tLearningRate 0.09470 (0.06920)\n",
      "Epoch: [9][299/365]\tBatch 158.796 (99.149)\tData 158.399 (98.735)\tLoss 1.38960 (2.29061)\tAcc@1  60.94 ( 42.51)\tAcc@5  87.11 ( 71.31)\tLearningRate 0.09386 (0.06990)\n",
      "0:03:08.536498 elapsed for 10\n",
      "Epoch: [10][ 99/365]\tBatch 60.906 (98.707)\tData 60.502 (98.293)\tLoss 1.43164 (2.25525)\tAcc@1  61.33 ( 43.30)\tAcc@5  87.89 ( 71.96)\tLearningRate 0.09237 (0.07092)\n",
      "Epoch: [10][199/365]\tBatch 109.634 (98.367)\tData 109.222 (97.953)\tLoss 1.34579 (2.23489)\tAcc@1  63.67 ( 43.74)\tAcc@5  89.06 ( 72.34)\tLearningRate 0.09139 (0.07147)\n",
      "Epoch: [10][299/365]\tBatch 158.766 (99.298)\tData 158.338 (98.884)\tLoss 1.63675 (2.21486)\tAcc@1  58.59 ( 44.19)\tAcc@5  83.59 ( 72.71)\tLearningRate 0.09036 (0.07196)\n",
      "0:03:07.504977 elapsed for 11\n",
      "Epoch: [11][ 99/365]\tBatch 59.384 (98.880)\tData 58.965 (98.466)\tLoss 1.43329 (2.18316)\tAcc@1  60.55 ( 44.88)\tAcc@5  87.11 ( 73.30)\tLearningRate 0.08854 (0.07266)\n",
      "Epoch: [11][199/365]\tBatch 110.548 (98.543)\tData 110.117 (98.129)\tLoss 1.43729 (2.16471)\tAcc@1  61.33 ( 45.29)\tAcc@5  89.45 ( 73.63)\tLearningRate 0.08737 (0.07302)\n",
      "Epoch: [11][299/365]\tBatch 161.212 (99.409)\tData 160.797 (98.995)\tLoss 1.39329 (2.14746)\tAcc@1  62.89 ( 45.68)\tAcc@5  87.11 ( 73.95)\tLearningRate 0.08616 (0.07334)\n",
      "0:03:10.648909 elapsed for 12\n",
      "Epoch: [12][ 99/365]\tBatch 61.403 (99.073)\tData 60.993 (98.659)\tLoss 1.33313 (2.11891)\tAcc@1  62.11 ( 46.31)\tAcc@5  88.67 ( 74.46)\tLearningRate 0.08405 (0.07377)\n",
      "Epoch: [12][199/365]\tBatch 109.746 (98.775)\tData 109.350 (98.360)\tLoss 1.37581 (2.10258)\tAcc@1  61.33 ( 46.67)\tAcc@5  87.50 ( 74.76)\tLearningRate 0.08272 (0.07398)\n",
      "Epoch: [12][299/365]\tBatch 159.459 (99.542)\tData 159.056 (99.128)\tLoss 1.36875 (2.08681)\tAcc@1  65.62 ( 47.03)\tAcc@5  87.89 ( 75.04)\tLearningRate 0.08134 (0.07416)\n",
      "0:03:10.046676 elapsed for 13\n",
      "Epoch: [13][ 99/365]\tBatch 60.030 (99.225)\tData 59.633 (98.811)\tLoss 1.25567 (2.06105)\tAcc@1  66.02 ( 47.61)\tAcc@5  90.23 ( 75.50)\tLearningRate 0.07898 (0.07436)\n",
      "Epoch: [13][199/365]\tBatch 110.157 (98.922)\tData 109.753 (98.508)\tLoss 1.37644 (2.04585)\tAcc@1  61.72 ( 47.95)\tAcc@5  88.67 ( 75.76)\tLearningRate 0.07750 (0.07444)\n",
      "Epoch: [13][299/365]\tBatch 160.921 (99.668)\tData 160.511 (99.254)\tLoss 1.18309 (2.03141)\tAcc@1  67.19 ( 48.28)\tAcc@5  90.23 ( 76.01)\tLearningRate 0.07599 (0.07448)\n",
      "0:03:09.842053 elapsed for 14\n",
      "Epoch: [14][ 99/365]\tBatch 63.109 (99.379)\tData 62.699 (98.964)\tLoss 1.22834 (2.00735)\tAcc@1  67.19 ( 48.83)\tAcc@5  90.23 ( 76.43)\tLearningRate 0.07342 (0.07449)\n",
      "Epoch: [14][199/365]\tBatch 111.914 (99.142)\tData 111.499 (98.728)\tLoss 1.39548 (1.99347)\tAcc@1  62.50 ( 49.14)\tAcc@5  87.89 ( 76.67)\tLearningRate 0.07182 (0.07446)\n",
      "Epoch: [14][299/365]\tBatch 159.630 (99.817)\tData 159.220 (99.402)\tLoss 1.42898 (1.98011)\tAcc@1  61.33 ( 49.45)\tAcc@5  89.84 ( 76.91)\tLearningRate 0.07019 (0.07439)\n",
      "0:03:10.725256 elapsed for 15\n",
      "Epoch: [15][ 99/365]\tBatch 61.041 (99.547)\tData 60.642 (99.133)\tLoss 1.06335 (1.95818)\tAcc@1  69.92 ( 49.95)\tAcc@5  91.41 ( 77.28)\tLearningRate 0.06745 (0.07423)\n",
      "Epoch: [15][199/365]\tBatch 108.507 (99.297)\tData 108.111 (98.883)\tLoss 1.19635 (1.94560)\tAcc@1  66.02 ( 50.24)\tAcc@5  88.28 ( 77.50)\tLearningRate 0.06576 (0.07409)\n",
      "Epoch: [15][299/365]\tBatch 158.345 (99.898)\tData 157.884 (99.483)\tLoss 1.19228 (1.93326)\tAcc@1  68.75 ( 50.52)\tAcc@5  89.84 ( 77.71)\tLearningRate 0.06405 (0.07393)\n",
      "0:03:09.178227 elapsed for 16\n",
      "Epoch: [16][ 99/365]\tBatch 59.647 (99.607)\tData 59.251 (99.193)\tLoss 1.01217 (1.91278)\tAcc@1  71.88 ( 50.99)\tAcc@5  92.19 ( 78.05)\tLearningRate 0.06118 (0.07362)\n",
      "Epoch: [16][199/365]\tBatch 106.591 (99.351)\tData 106.195 (98.937)\tLoss 1.13314 (1.90090)\tAcc@1  70.31 ( 51.27)\tAcc@5  87.89 ( 78.25)\tLearningRate 0.05943 (0.07340)\n",
      "Epoch: [16][299/365]\tBatch 158.822 (99.888)\tData 158.425 (99.474)\tLoss 1.15674 (1.88960)\tAcc@1  66.02 ( 51.53)\tAcc@5  90.23 ( 78.44)\tLearningRate 0.05766 (0.07316)\n",
      "0:03:10.157315 elapsed for 17\n",
      "Epoch: [17][ 99/365]\tBatch 58.806 (99.639)\tData 58.409 (99.224)\tLoss 1.14882 (1.87067)\tAcc@1  66.41 ( 51.96)\tAcc@5  91.80 ( 78.75)\tLearningRate 0.05473 (0.07271)\n",
      "Epoch: [17][199/365]\tBatch 107.976 (99.412)\tData 107.558 (98.998)\tLoss 1.10323 (1.85933)\tAcc@1  68.75 ( 52.23)\tAcc@5  92.19 ( 78.94)\tLearningRate 0.05294 (0.07242)\n",
      "Epoch: [17][299/365]\tBatch 160.421 (99.966)\tData 159.993 (99.552)\tLoss 1.22150 (1.84872)\tAcc@1  68.75 ( 52.48)\tAcc@5  87.89 ( 79.12)\tLearningRate 0.05115 (0.07210)\n",
      "0:03:09.971369 elapsed for 18\n",
      "Epoch: [18][ 99/365]\tBatch 57.663 (99.710)\tData 57.267 (99.296)\tLoss 1.23560 (1.83113)\tAcc@1  68.75 ( 52.89)\tAcc@5  87.50 ( 79.41)\tLearningRate 0.04819 (0.07155)\n",
      "Epoch: [18][199/365]\tBatch 111.317 (99.495)\tData 110.907 (99.081)\tLoss 1.23544 (1.82056)\tAcc@1  64.45 ( 53.14)\tAcc@5  90.62 ( 79.58)\tLearningRate 0.04640 (0.07119)\n",
      "Epoch: [18][299/365]\tBatch 161.971 (100.028)\tData 161.576 (99.614)\tLoss 1.15699 (1.81053)\tAcc@1  67.97 ( 53.37)\tAcc@5  88.67 ( 79.75)\tLearningRate 0.04461 (0.07082)\n",
      "0:03:08.619604 elapsed for 19\n",
      "Epoch: [19][ 99/365]\tBatch 60.189 (99.780)\tData 59.781 (99.366)\tLoss 1.06177 (1.79341)\tAcc@1  66.41 ( 53.77)\tAcc@5  92.19 ( 80.02)\tLearningRate 0.04168 (0.07017)\n",
      "Epoch: [19][199/365]\tBatch 107.957 (99.561)\tData 107.554 (99.147)\tLoss 1.26690 (1.78362)\tAcc@1  62.89 ( 54.00)\tAcc@5  89.06 ( 80.18)\tLearningRate 0.03992 (0.06976)\n",
      "Epoch: [19][299/365]\tBatch 157.329 (100.023)\tData 156.926 (99.608)\tLoss 1.10912 (1.77388)\tAcc@1  68.75 ( 54.23)\tAcc@5  90.62 ( 80.33)\tLearningRate 0.03817 (0.06933)\n",
      "0:03:08.286158 elapsed for 20\n",
      "Epoch: [20][ 99/365]\tBatch 58.348 (99.769)\tData 57.923 (99.355)\tLoss 0.98009 (1.75782)\tAcc@1  74.22 ( 54.60)\tAcc@5  92.97 ( 80.59)\tLearningRate 0.03532 (0.06860)\n",
      "Epoch: [20][199/365]\tBatch 109.783 (99.563)\tData 109.348 (99.149)\tLoss 0.93926 (1.74836)\tAcc@1  75.78 ( 54.83)\tAcc@5  91.80 ( 80.73)\tLearningRate 0.03361 (0.06815)\n",
      "Epoch: [20][299/365]\tBatch 159.895 (100.031)\tData 159.472 (99.616)\tLoss 1.06791 (1.73919)\tAcc@1  69.14 ( 55.04)\tAcc@5  91.41 ( 80.88)\tLearningRate 0.03193 (0.06768)\n",
      "0:03:08.233288 elapsed for 21\n",
      "Epoch: [21][ 99/365]\tBatch 59.458 (99.810)\tData 59.054 (99.396)\tLoss 1.14643 (1.72365)\tAcc@1  70.31 ( 55.41)\tAcc@5  90.62 ( 81.12)\tLearningRate 0.02921 (0.06689)\n",
      "Epoch: [21][199/365]\tBatch 108.137 (99.608)\tData 107.730 (99.194)\tLoss 0.98522 (1.71450)\tAcc@1  71.48 ( 55.63)\tAcc@5  92.58 ( 81.26)\tLearningRate 0.02759 (0.06640)\n",
      "Epoch: [21][299/365]\tBatch 159.683 (100.037)\tData 159.286 (99.622)\tLoss 0.91017 (1.70541)\tAcc@1  72.27 ( 55.85)\tAcc@5  94.53 ( 81.40)\tLearningRate 0.02600 (0.06591)\n",
      "0:03:07.833656 elapsed for 22\n",
      "Epoch: [22][ 99/365]\tBatch 60.752 (99.819)\tData 60.349 (99.405)\tLoss 0.91725 (1.69024)\tAcc@1  75.78 ( 56.21)\tAcc@5  94.92 ( 81.63)\tLearningRate 0.02345 (0.06507)\n",
      "Epoch: [22][199/365]\tBatch 109.652 (99.633)\tData 109.256 (99.219)\tLoss 0.91185 (1.68129)\tAcc@1  76.95 ( 56.43)\tAcc@5  94.14 ( 81.77)\tLearningRate 0.02195 (0.06456)\n",
      "Epoch: [22][299/365]\tBatch 159.864 (100.055)\tData 159.452 (99.641)\tLoss 0.93802 (1.67233)\tAcc@1  75.00 ( 56.64)\tAcc@5  94.53 ( 81.90)\tLearningRate 0.02048 (0.06404)\n",
      "0:03:08.138627 elapsed for 23\n",
      "Epoch: [23][ 99/365]\tBatch 60.007 (99.855)\tData 59.610 (99.440)\tLoss 0.82385 (1.65756)\tAcc@1  75.78 ( 57.00)\tAcc@5  94.14 ( 82.12)\tLearningRate 0.01815 (0.06317)\n",
      "Epoch: [23][199/365]\tBatch 109.633 (99.677)\tData 109.229 (99.263)\tLoss 0.96501 (1.64877)\tAcc@1  72.66 ( 57.21)\tAcc@5  92.58 ( 82.25)\tLearningRate 0.01678 (0.06263)\n",
      "Epoch: [23][299/365]\tBatch 161.922 (100.093)\tData 161.510 (99.679)\tLoss 0.98594 (1.63992)\tAcc@1  71.48 ( 57.42)\tAcc@5  93.36 ( 82.38)\tLearningRate 0.01547 (0.06210)\n",
      "0:03:11.341927 elapsed for 24\n",
      "Epoch: [24][ 99/365]\tBatch 59.079 (99.905)\tData 58.659 (99.491)\tLoss 0.79635 (1.62479)\tAcc@1  77.73 ( 57.79)\tAcc@5  94.53 ( 82.60)\tLearningRate 0.01339 (0.06121)\n",
      "Epoch: [25][199/365]\tBatch 110.567 (99.773)\tData 110.170 (99.359)\tLoss 0.79395 (1.58319)\tAcc@1  78.52 ( 58.81)\tAcc@5  96.88 ( 83.19)\tLearningRate 0.00824 (0.05869)\n",
      "Epoch: [25][299/365]\tBatch 156.613 (100.141)\tData 156.189 (99.726)\tLoss 0.70995 (1.57424)\tAcc@1  78.52 ( 59.03)\tAcc@5  95.70 ( 83.31)\tLearningRate 0.00728 (0.05815)\n",
      "0:03:08.309989 elapsed for 26\n",
      "Epoch: [26][ 99/365]\tBatch 60.429 (99.950)\tData 60.032 (99.536)\tLoss 0.66142 (1.55928)\tAcc@1  78.91 ( 59.40)\tAcc@5  96.09 ( 83.52)\tLearningRate 0.00582 (0.05727)\n",
      "Epoch: [26][199/365]\tBatch 109.628 (99.795)\tData 109.216 (99.381)\tLoss 0.58617 (1.55021)\tAcc@1  83.20 ( 59.63)\tAcc@5  97.66 ( 83.64)\tLearningRate 0.00501 (0.05673)\n",
      "Epoch: [26][299/365]\tBatch 158.484 (100.143)\tData 158.087 (99.728)\tLoss 0.59121 (1.54110)\tAcc@1  82.81 ( 59.85)\tAcc@5  98.44 ( 83.76)\tLearningRate 0.00426 (0.05620)\n",
      "0:03:08.066479 elapsed for 27\n",
      "Epoch: [27][ 99/365]\tBatch 58.255 (99.950)\tData 57.836 (99.536)\tLoss 0.59073 (1.52607)\tAcc@1  83.20 ( 60.22)\tAcc@5  97.27 ( 83.96)\tLearningRate 0.00314 (0.05533)\n",
      "Epoch: [27][199/365]\tBatch 109.932 (99.790)\tData 109.504 (99.376)\tLoss 0.68592 (1.51692)\tAcc@1  82.81 ( 60.45)\tAcc@5  94.53 ( 84.08)\tLearningRate 0.00255 (0.05481)\n",
      "Epoch: [27][299/365]\tBatch 159.272 (100.128)\tData 158.876 (99.714)\tLoss 0.67754 (1.50797)\tAcc@1  82.03 ( 60.68)\tAcc@5  94.53 ( 84.20)\tLearningRate 0.00201 (0.05429)\n",
      "0:03:08.606752 elapsed for 28\n",
      "Epoch: [28][ 99/365]\tBatch 58.220 (99.948)\tData 57.799 (99.534)\tLoss 0.56089 (1.49301)\tAcc@1  84.77 ( 61.05)\tAcc@5  96.88 ( 84.39)\tLearningRate 0.00127 (0.05345)\n",
      "Epoch: [28][199/365]\tBatch 111.301 (99.809)\tData 110.889 (99.394)\tLoss 0.60147 (1.48400)\tAcc@1  84.38 ( 61.28)\tAcc@5  96.09 ( 84.50)\tLearningRate 0.00090 (0.05294)\n",
      "Epoch: [28][299/365]\tBatch 159.249 (100.147)\tData 158.829 (99.732)\tLoss 0.68366 (1.47513)\tAcc@1  83.20 ( 61.51)\tAcc@5  94.53 ( 84.62)\tLearningRate 0.00059 (0.05245)\n",
      "0:03:07.694201 elapsed for 29\n",
      "Epoch: [29][ 99/365]\tBatch 59.171 (99.980)\tData 58.771 (99.566)\tLoss 0.40392 (1.46070)\tAcc@1  88.67 ( 61.88)\tAcc@5  97.27 ( 84.80)\tLearningRate 0.00022 (0.05164)\n",
      "Epoch: [29][199/365]\tBatch 110.213 (99.846)\tData 109.817 (99.432)\tLoss 0.52614 (1.45215)\tAcc@1  82.81 ( 62.10)\tAcc@5  97.66 ( 84.91)\tLearningRate 0.00009 (0.05117)\n",
      "Epoch: [29][299/365]\tBatch 160.166 (100.164)\tData 159.743 (99.750)\tLoss 0.43636 (1.44363)\tAcc@1  88.28 ( 62.31)\tAcc@5  98.05 ( 85.02)\tLearningRate 0.00001 (0.05070)\n",
      "0:03:09.654024 elapsed for 30\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (_, inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/darknet53_fp16_5e4_ep030.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            _, images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_results = [classification_val(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7880709192522642"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024864015877712558"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7857005203314704,\n",
       " 0.7870495278473695,\n",
       " 0.7844478704952784,\n",
       " 0.7871458855270765,\n",
       " 0.7878203892850261,\n",
       " 0.7919637695124302,\n",
       " 0.7898439005588745,\n",
       " 0.785218731932935,\n",
       " 0.7910965503950664,\n",
       " 0.790422046637117]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (_, images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result = [val_retrieval(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.701416457891694"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0033551434134026586"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7089034496049336,\n",
       " 0.7012911929080747,\n",
       " 0.7007130468298324,\n",
       " 0.703025631142802,\n",
       " 0.6996531123530545,\n",
       " 0.7017729813066101,\n",
       " 0.6961842358835999,\n",
       " 0.6970514550009635,\n",
       " 0.7027365581036809,\n",
       " 0.7028329157833879]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
