{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, planes, ratio):\n",
    "\n",
    "      super(SEBlock, self).__init__()\n",
    "\n",
    "      self.se_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "      self.se_fc1 = nn.Linear(planes, planes // ratio)\n",
    "      self.relu = nn.ReLU(inplace=True)\n",
    "      self.se_fc2 = nn.Linear(planes // ratio, planes)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "      out = self.se_pool(x)\n",
    "      out = torch.flatten(out, 1)\n",
    "      out = self.se_fc1(out)\n",
    "      #print(out.shape)\n",
    "      out = F.relu(out)\n",
    "      out = self.se_fc2(out)\n",
    "      out = torch.sigmoid(out)\n",
    "      out = out.view(out.size(0), out.size(1), 1, 1)\n",
    "      #print(x.shape)\n",
    "      #print(out.shape)\n",
    "      out = torch.mul(out.expand_as(x), x)\n",
    "\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        self.se = SEBlock(self.inplanes, ratio=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(2, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(2, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(2, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((1,3,224,224))\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 101])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20705145"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)\n",
    "'''\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image_id, sample, target) where target is class_index of\n",
    "                the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        #print(path)\n",
    "        #print(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        image_id = path.split('/')[-1]\n",
    "\n",
    "        return image_id, sample, target\n",
    "''' \n",
    "data_dir = './data/food101/'\n",
    "train_data = datasets.ImageFolder(data_dir + 'train', transform=get_transform(random_crop=True))\n",
    "test_data = datasets.ImageFolder(data_dir + 'test', transform=get_transform(random_crop=False))\n",
    "#testdata=datasets.ImageFolder(data_dir + r'\\test', transform=test_transforms)\n",
    "\n",
    "#trainloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "#testloader = torch.utils.data.DataLoader(test_data, batch_size=128)\n",
    "#test_loader=torch.utils.data.DataLoader(testdata, batch_size=64)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "#data_dir = 'train/train_data'\n",
    "\n",
    "#dataset = CustomDataset(data_dir, transform=get_transform(random_crop=True))\n",
    "\n",
    "#split_size = int(len(dataset) * 0.9)\n",
    "#train_set, valid_set = data.random_split(dataset, [split_size, len(dataset) - split_size])\n",
    "tr_loader = data.DataLoader(dataset=train_data,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=18)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=test_data,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True,                             \n",
    "                             num_workers=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 224, 224])\n",
      "tensor([ 38,  63,   8,  36,  35,  89,  54,  36,  96,  63,  48,  12,  42,  91,\n",
      "         80,  88,  24,   9,  31,  46,  25,  48,  39,  76,   3,  56,  52,  93,\n",
      "         55,   7,  89,  76,   7,  10,  76,  31,   5,  29,   8,  84,  94,  84,\n",
      "          7,   6,  31,  58,  60,  70,   0,  52,   2,  12,  27,  92,  19,  12,\n",
      "         15,   5,  17,  28,  99,  72,  15,  30,  67,  86,  57,  58,  47,  55,\n",
      "         56,  29,  31,  85,   2,   2,  26,  56,  83,  46,  34,  19,  58,  51,\n",
      "         34,  98,  79,  18,  15,  76,  11,  40,  85,  47,  38,   5,  36,  27,\n",
      "         82,   5,   9,  99,  81,  87,  81,   1,  56,  73,   4,  34,  55,   2,\n",
      "         72,  71,   6,  75,  50,  67,  91,  94,  72,   6,  27,  28,  65,  93,\n",
      "         32,  56,  13,  56,  29,  29,  90,  25,  30,  73,  86,  32,  23,  73,\n",
      "         42,  86,  57,  65,  50,  51,  78,  60,  37,  95,  41,  25,  94,  27,\n",
      "         42,  72,  84,  40,  56,  64,  41,  47,  37,  83,  89,  64,  76,  37,\n",
      "         66,  98,  14,  19,  24,  22,  80,  69,  63,  72,  13,  14,   4,  29,\n",
      "         59,   4,  48,  43,  29,  62,  65,  84,  96,  35,  81,  12,  90,  59,\n",
      "         66,  24,  29,  29,  59,   7,  31,  90,  24,  34,  98,   7,  38,  51,\n",
      "         89,  65,  63,  85,  34,  19,   3,  13,  69,  74,  14,  36,   3,  77,\n",
      "         25,  91,  56,   5,  97,  99,  75,  52,   2,  13,  10,   1,  97,  30,\n",
      "         26,  18,  65, 100,  87,  61,   7,  36,  82,  31,  86,  67,  88,  96,\n",
      "         51,  38,  19,  57])\n",
      "torch.Size([256, 3, 224, 224])\n",
      "tensor([85, 24, 16, 62,  2,  1, 28, 12, 65,  8, 16,  3, 17, 97, 71, 82, 44, 23,\n",
      "        61, 94,  0, 84, 60, 63, 76, 10, 54, 81, 15, 83,  5, 51, 32, 50, 77, 46,\n",
      "        41, 35, 98,  6, 35,  9, 58, 98, 30, 46, 77,  6, 38, 49, 41, 58, 55,  6,\n",
      "        61, 51, 32,  4, 66, 79, 31, 22, 82, 49, 75, 82, 72, 92, 59,  5, 27, 18,\n",
      "        78, 37, 40, 69, 85,  2, 67, 12, 64, 81, 13, 57, 36, 46, 41, 92, 13, 91,\n",
      "         2, 77, 89, 11,  0, 91, 96, 98, 70, 86, 42,  8, 38, 75, 93, 83, 40, 29,\n",
      "        82, 33, 86, 21, 38, 97, 83, 79, 61, 29, 89, 85, 40, 74, 16, 39, 14, 61,\n",
      "        87, 18, 18, 81, 22, 82, 63, 58, 43, 32, 33, 86, 45, 21, 37,  1, 28, 89,\n",
      "        41, 86, 39, 30, 25, 57,  1,  2, 45, 27, 38, 20, 91, 49, 35, 96, 84, 71,\n",
      "        93, 71, 69,  0, 96, 29, 50, 75, 36, 58, 36, 40, 21, 35,  7, 11, 41, 91,\n",
      "         2, 48, 35, 60, 34, 72, 38, 21, 70, 27, 19, 13, 23, 40, 69, 35,  1, 79,\n",
      "        90, 94, 65, 16, 30, 33,  4,  8,  5,  8, 95, 62, 82, 12, 45, 32, 33, 62,\n",
      "         2, 30,  4, 60, 44, 48, 72, 52, 58, 25, 87, 90, 67, 62, 71, 83, 89, 79,\n",
      "        23, 46, 10, 81, 80, 14, 64, 69, 14, 94, 99, 27, 85, 87, 19, 30, 88, 65,\n",
      "        73, 40, 69, 10])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    \n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=30, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 99/296]\tBatch 44.661 (28.632)\tData 44.342 (28.302)\tLoss 4.21942 (4.38323)\tAcc@1   8.59 (  4.89)\tAcc@5  22.27 ( 16.20)\tLearningRate 0.00475 (0.00425)\n",
      "Epoch: [0][199/296]\tBatch 77.997 (44.993)\tData 77.678 (44.663)\tLoss 4.09829 (4.26025)\tAcc@1   8.98 (  6.38)\tAcc@5  29.30 ( 20.02)\tLearningRate 0.00698 (0.00500)\n",
      "0:01:51.036102 elapsed for 1\n",
      "Epoch: [1][ 99/296]\tBatch 43.815 (52.291)\tData 43.515 (51.965)\tLoss 3.65328 (4.08870)\tAcc@1  14.45 (  8.79)\tAcc@5  37.50 ( 25.41)\tLearningRate 0.01531 (0.00785)\n",
      "Epoch: [1][199/296]\tBatch 79.065 (54.208)\tData 78.658 (53.882)\tLoss 3.68304 (4.00803)\tAcc@1  15.62 ( 10.09)\tAcc@5  42.97 ( 27.92)\tLearningRate 0.02134 (0.00995)\n",
      "0:01:52.293800 elapsed for 2\n",
      "Epoch: [2][ 99/296]\tBatch 42.755 (55.911)\tData 42.454 (55.587)\tLoss 3.29422 (3.85278)\tAcc@1  23.44 ( 12.66)\tAcc@5  50.00 ( 32.48)\tLearningRate 0.03572 (0.01515)\n",
      "Epoch: [2][199/296]\tBatch 77.163 (56.455)\tData 76.852 (56.131)\tLoss 3.39590 (3.77792)\tAcc@1  21.48 ( 14.00)\tAcc@5  45.70 ( 34.59)\tLearningRate 0.04393 (0.01827)\n",
      "0:01:50.884199 elapsed for 3\n",
      "Epoch: [3][ 99/296]\tBatch 43.589 (57.054)\tData 43.291 (56.730)\tLoss 3.07855 (3.63145)\tAcc@1  25.78 ( 16.68)\tAcc@5  53.12 ( 38.59)\tLearningRate 0.06049 (0.02501)\n",
      "Epoch: [3][199/296]\tBatch 80.159 (57.508)\tData 79.859 (57.184)\tLoss 2.69584 (3.56049)\tAcc@1  32.42 ( 18.00)\tAcc@5  57.81 ( 40.47)\tLearningRate 0.06868 (0.02866)\n",
      "0:01:52.949651 elapsed for 4\n",
      "Epoch: [4][ 99/296]\tBatch 42.765 (57.970)\tData 42.450 (57.647)\tLoss 2.79094 (3.42971)\tAcc@1  30.47 ( 20.49)\tAcc@5  62.89 ( 43.85)\tLearningRate 0.08299 (0.03590)\n",
      "Epoch: [4][199/296]\tBatch 79.044 (58.188)\tData 78.718 (57.865)\tLoss 2.57352 (3.36956)\tAcc@1  36.33 ( 21.65)\tAcc@5  66.80 ( 45.37)\tLearningRate 0.08896 (0.03953)\n",
      "0:01:52.176074 elapsed for 5\n",
      "Epoch: [5][ 99/296]\tBatch 43.916 (58.468)\tData 43.589 (58.145)\tLoss 2.43217 (3.25846)\tAcc@1  40.23 ( 23.84)\tAcc@5  68.75 ( 48.07)\tLearningRate 0.09717 (0.04622)\n",
      "Epoch: [5][199/296]\tBatch 79.562 (58.667)\tData 79.183 (58.344)\tLoss 2.25277 (3.20539)\tAcc@1  41.80 ( 24.90)\tAcc@5  68.75 ( 49.34)\tLearningRate 0.09932 (0.04933)\n",
      "0:01:51.291832 elapsed for 6\n",
      "Epoch: [6][ 99/296]\tBatch 43.589 (58.817)\tData 43.289 (58.495)\tLoss 2.20037 (3.10999)\tAcc@1  44.92 ( 26.83)\tAcc@5  75.78 ( 51.59)\tLearningRate 0.09995 (0.05461)\n",
      "Epoch: [6][199/296]\tBatch 80.536 (58.976)\tData 80.198 (58.653)\tLoss 2.28951 (3.06392)\tAcc@1  46.09 ( 27.78)\tAcc@5  71.09 ( 52.64)\tLearningRate 0.09980 (0.05690)\n",
      "0:01:53.194156 elapsed for 7\n",
      "Epoch: [7][ 99/296]\tBatch 42.636 (59.156)\tData 42.333 (58.834)\tLoss 2.12463 (2.98001)\tAcc@1  52.73 ( 29.52)\tAcc@5  71.48 ( 54.56)\tLearningRate 0.09923 (0.06075)\n",
      "Epoch: [7][199/296]\tBatch 78.974 (59.259)\tData 78.631 (58.937)\tLoss 2.05870 (2.94021)\tAcc@1  49.61 ( 30.34)\tAcc@5  78.12 ( 55.44)\tLearningRate 0.09880 (0.06243)\n",
      "0:01:52.232615 elapsed for 8\n",
      "Epoch: [8][ 99/296]\tBatch 43.061 (59.332)\tData 42.758 (59.010)\tLoss 1.96744 (2.86589)\tAcc@1  50.78 ( 31.91)\tAcc@5  81.25 ( 57.09)\tLearningRate 0.09767 (0.06528)\n",
      "Epoch: [8][199/296]\tBatch 77.961 (59.379)\tData 77.639 (59.057)\tLoss 1.90549 (2.83090)\tAcc@1  55.86 ( 32.64)\tAcc@5  75.39 ( 57.86)\tLearningRate 0.09696 (0.06653)\n",
      "0:01:51.543089 elapsed for 9\n",
      "Epoch: [9][ 99/296]\tBatch 43.740 (59.411)\tData 43.439 (59.090)\tLoss 1.94301 (2.76630)\tAcc@1  53.91 ( 34.02)\tAcc@5  76.56 ( 59.26)\tLearningRate 0.09529 (0.06863)\n",
      "Epoch: [9][199/296]\tBatch 79.189 (59.502)\tData 78.881 (59.180)\tLoss 2.04895 (2.73567)\tAcc@1  52.34 ( 34.68)\tAcc@5  72.66 ( 59.91)\tLearningRate 0.09431 (0.06954)\n",
      "0:01:51.588010 elapsed for 10\n",
      "Epoch: [10][ 99/296]\tBatch 42.597 (59.533)\tData 42.296 (59.211)\tLoss 1.92640 (2.67919)\tAcc@1  51.95 ( 35.87)\tAcc@5  74.61 ( 61.12)\tLearningRate 0.09214 (0.07106)\n",
      "Epoch: [10][199/296]\tBatch 78.437 (59.585)\tData 78.073 (59.263)\tLoss 1.84738 (2.65180)\tAcc@1  53.52 ( 36.46)\tAcc@5  79.69 ( 61.69)\tLearningRate 0.09091 (0.07171)\n",
      "0:01:51.385854 elapsed for 11\n",
      "Epoch: [11][ 99/296]\tBatch 43.311 (59.606)\tData 42.992 (59.285)\tLoss 1.69082 (2.60142)\tAcc@1  56.25 ( 37.54)\tAcc@5  83.20 ( 62.76)\tLearningRate 0.08827 (0.07275)\n",
      "Epoch: [11][199/296]\tBatch 79.873 (59.662)\tData 79.460 (59.341)\tLoss 1.88908 (2.57687)\tAcc@1  53.52 ( 38.06)\tAcc@5  79.30 ( 63.28)\tLearningRate 0.08681 (0.07318)\n",
      "0:01:52.253906 elapsed for 12\n",
      "Epoch: [12][ 99/296]\tBatch 43.237 (59.690)\tData 42.922 (59.368)\tLoss 1.61559 (2.53037)\tAcc@1  60.94 ( 39.08)\tAcc@5  82.81 ( 64.22)\tLearningRate 0.08374 (0.07383)\n",
      "Epoch: [12][199/296]\tBatch 78.688 (59.730)\tData 78.300 (59.408)\tLoss 1.82540 (2.50828)\tAcc@1  53.12 ( 39.56)\tAcc@5  77.73 ( 64.67)\tLearningRate 0.08208 (0.07407)\n",
      "0:01:52.379332 elapsed for 13\n",
      "Epoch: [13][ 99/296]\tBatch 44.631 (59.773)\tData 44.282 (59.451)\tLoss 1.61941 (2.46683)\tAcc@1  59.77 ( 40.46)\tAcc@5  83.20 ( 65.51)\tLearningRate 0.07864 (0.07438)\n",
      "Epoch: [13][199/296]\tBatch 80.661 (59.838)\tData 80.358 (59.516)\tLoss 1.59954 (2.44622)\tAcc@1  58.98 ( 40.90)\tAcc@5  85.55 ( 65.92)\tLearningRate 0.07680 (0.07447)\n",
      "0:01:53.270344 elapsed for 14\n",
      "Epoch: [14][ 99/296]\tBatch 43.370 (59.886)\tData 43.071 (59.565)\tLoss 1.50811 (2.40863)\tAcc@1  57.03 ( 41.71)\tAcc@5  87.50 ( 66.67)\tLearningRate 0.07304 (0.07449)\n",
      "Epoch: [14][199/296]\tBatch 80.937 (59.943)\tData 80.640 (59.621)\tLoss 1.75538 (2.39001)\tAcc@1  54.30 ( 42.11)\tAcc@5  80.08 ( 67.04)\tLearningRate 0.07106 (0.07443)\n",
      "0:01:52.837813 elapsed for 15\n",
      "Epoch: [15][ 99/296]\tBatch 43.557 (59.984)\tData 43.253 (59.663)\tLoss 1.37092 (2.35461)\tAcc@1  64.84 ( 42.89)\tAcc@5  86.33 ( 67.74)\tLearningRate 0.06705 (0.07420)\n",
      "Epoch: [15][199/296]\tBatch 78.013 (59.999)\tData 77.646 (59.677)\tLoss 1.64217 (2.33771)\tAcc@1  59.38 ( 43.26)\tAcc@5  81.64 ( 68.06)\tLearningRate 0.06496 (0.07402)\n",
      "0:01:51.679988 elapsed for 16\n",
      "Epoch: [16][ 99/296]\tBatch 43.890 (59.989)\tData 43.589 (59.667)\tLoss 1.46898 (2.30470)\tAcc@1  61.72 ( 43.99)\tAcc@5  83.98 ( 68.70)\tLearningRate 0.06077 (0.07357)\n",
      "Epoch: [16][199/296]\tBatch 79.457 (60.027)\tData 79.087 (59.705)\tLoss 1.50588 (2.28869)\tAcc@1  60.94 ( 44.34)\tAcc@5  83.98 ( 69.02)\tLearningRate 0.05860 (0.07329)\n",
      "0:01:52.472672 elapsed for 17\n",
      "Epoch: [17][ 99/296]\tBatch 42.960 (60.047)\tData 42.648 (59.725)\tLoss 1.50470 (2.25829)\tAcc@1  63.67 ( 45.01)\tAcc@5  85.16 ( 69.60)\tLearningRate 0.05431 (0.07265)\n",
      "Epoch: [17][199/296]\tBatch 78.936 (60.073)\tData 78.634 (59.751)\tLoss 1.54533 (2.24309)\tAcc@1  59.77 ( 45.35)\tAcc@5  83.98 ( 69.89)\tLearningRate 0.05210 (0.07227)\n",
      "0:01:51.804076 elapsed for 18\n",
      "Epoch: [18][ 99/296]\tBatch 44.377 (60.075)\tData 43.997 (59.754)\tLoss 1.43932 (2.21407)\tAcc@1  63.67 ( 45.99)\tAcc@5  85.16 ( 70.44)\tLearningRate 0.04777 (0.07147)\n",
      "Epoch: [18][199/296]\tBatch 80.066 (60.108)\tData 79.769 (59.787)\tLoss 1.21358 (2.19985)\tAcc@1  69.14 ( 46.31)\tAcc@5  88.28 ( 70.70)\tLearningRate 0.04556 (0.07102)\n",
      "0:01:52.473996 elapsed for 19\n",
      "Epoch: [19][ 99/296]\tBatch 43.430 (60.117)\tData 43.126 (59.796)\tLoss 1.42418 (2.17240)\tAcc@1  64.84 ( 46.92)\tAcc@5  87.50 ( 71.22)\tLearningRate 0.04127 (0.07007)\n",
      "Epoch: [19][199/296]\tBatch 80.238 (60.136)\tData 79.831 (59.815)\tLoss 1.28080 (2.15865)\tAcc@1  67.58 ( 47.23)\tAcc@5  87.11 ( 71.47)\tLearningRate 0.03910 (0.06956)\n",
      "0:01:52.318291 elapsed for 20\n",
      "Epoch: [20][ 99/296]\tBatch 44.325 (60.152)\tData 44.025 (59.831)\tLoss 1.45076 (2.13225)\tAcc@1  63.28 ( 47.82)\tAcc@5  82.42 ( 71.96)\tLearningRate 0.03492 (0.06850)\n",
      "Epoch: [20][199/296]\tBatch 80.417 (60.184)\tData 80.120 (59.863)\tLoss 1.32672 (2.11922)\tAcc@1  64.45 ( 48.11)\tAcc@5  87.11 ( 72.20)\tLearningRate 0.03282 (0.06793)\n",
      "0:01:52.800598 elapsed for 21\n",
      "Epoch: [21][ 99/296]\tBatch 43.878 (60.200)\tData 43.564 (59.879)\tLoss 1.28102 (2.09388)\tAcc@1  64.84 ( 48.68)\tAcc@5  89.06 ( 72.66)\tLearningRate 0.02882 (0.06678)\n",
      "Epoch: [21][199/296]\tBatch 79.076 (60.220)\tData 78.735 (59.899)\tLoss 1.29847 (2.08103)\tAcc@1  68.75 ( 48.96)\tAcc@5  86.72 ( 72.89)\tLearningRate 0.02684 (0.06617)\n",
      "0:01:52.005385 elapsed for 22\n",
      "Epoch: [22][ 99/296]\tBatch 42.732 (60.218)\tData 42.421 (59.897)\tLoss 1.28196 (2.05621)\tAcc@1  65.23 ( 49.52)\tAcc@5  87.89 ( 73.34)\tLearningRate 0.02309 (0.06495)\n",
      "Epoch: [22][199/296]\tBatch 78.632 (60.235)\tData 78.318 (59.914)\tLoss 1.05593 (2.04386)\tAcc@1  75.00 ( 49.80)\tAcc@5  91.02 ( 73.56)\tLearningRate 0.02126 (0.06431)\n",
      "0:01:51.963689 elapsed for 23\n",
      "Epoch: [23][ 99/296]\tBatch 41.913 (60.229)\tData 41.597 (59.908)\tLoss 1.28880 (2.01953)\tAcc@1  66.02 ( 50.35)\tAcc@5  86.72 ( 73.98)\tLearningRate 0.01782 (0.06304)\n",
      "Epoch: [23][199/296]\tBatch 77.816 (60.232)\tData 77.506 (59.911)\tLoss 1.12373 (2.00671)\tAcc@1  70.70 ( 50.64)\tAcc@5  88.28 ( 74.21)\tLearningRate 0.01616 (0.06238)\n",
      "0:01:51.077027 elapsed for 24\n",
      "Epoch: [24][ 99/296]\tBatch 42.783 (60.209)\tData 42.468 (59.888)\tLoss 1.12492 (1.98265)\tAcc@1  70.70 ( 51.20)\tAcc@5  88.28 ( 74.61)\tLearningRate 0.01310 (0.06108)\n",
      "Epoch: [24][199/296]\tBatch 78.191 (60.216)\tData 77.882 (59.895)\tLoss 0.95576 (1.97014)\tAcc@1  75.78 ( 51.49)\tAcc@5  91.02 ( 74.82)\tLearningRate 0.01165 (0.06042)\n",
      "0:01:51.660511 elapsed for 25\n",
      "Epoch: [25][ 99/296]\tBatch 43.031 (60.200)\tData 42.716 (59.880)\tLoss 1.11462 (1.94618)\tAcc@1  69.53 ( 52.04)\tAcc@5  89.06 ( 75.23)\tLearningRate 0.00901 (0.05911)\n",
      "Epoch: [25][199/296]\tBatch 76.490 (60.194)\tData 76.167 (59.873)\tLoss 0.91063 (1.93345)\tAcc@1  77.73 ( 52.34)\tAcc@5  92.58 ( 75.44)\tLearningRate 0.00779 (0.05844)\n",
      "0:01:49.440947 elapsed for 26\n",
      "Epoch: [26][ 99/296]\tBatch 42.305 (60.155)\tData 41.963 (59.834)\tLoss 0.84191 (1.90885)\tAcc@1  76.17 ( 52.90)\tAcc@5  94.14 ( 75.84)\tLearningRate 0.00563 (0.05714)\n",
      "Epoch: [26][199/296]\tBatch 78.777 (60.162)\tData 78.402 (59.841)\tLoss 0.98461 (1.89611)\tAcc@1  74.61 ( 53.20)\tAcc@5  89.45 ( 76.04)\tLearningRate 0.00465 (0.05648)\n",
      "0:01:50.958088 elapsed for 27\n",
      "Epoch: [27][ 99/296]\tBatch 42.213 (60.152)\tData 41.916 (59.831)\tLoss 0.70269 (1.87156)\tAcc@1  78.52 ( 53.78)\tAcc@5  95.31 ( 76.44)\tLearningRate 0.00300 (0.05520)\n",
      "Epoch: [27][199/296]\tBatch 77.851 (60.153)\tData 77.488 (59.832)\tLoss 0.85977 (1.85883)\tAcc@1  77.34 ( 54.08)\tAcc@5  91.80 ( 76.64)\tLearningRate 0.00229 (0.05456)\n",
      "0:01:50.094977 elapsed for 28\n",
      "Epoch: [28][ 99/296]\tBatch 43.163 (60.131)\tData 42.864 (59.810)\tLoss 0.82346 (1.83407)\tAcc@1  78.52 ( 54.66)\tAcc@5  91.80 ( 77.03)\tLearningRate 0.00117 (0.05333)\n",
      "Epoch: [28][199/296]\tBatch 78.508 (60.134)\tData 78.137 (59.814)\tLoss 0.85876 (1.82169)\tAcc@1  75.00 ( 54.96)\tAcc@5  91.41 ( 77.22)\tLearningRate 0.00075 (0.05271)\n",
      "0:01:50.090948 elapsed for 29\n",
      "Epoch: [29][ 99/296]\tBatch 42.514 (60.111)\tData 42.218 (59.790)\tLoss 0.69022 (1.79789)\tAcc@1  81.64 ( 55.53)\tAcc@5  94.53 ( 77.58)\tLearningRate 0.00019 (0.05153)\n",
      "Epoch: [29][199/296]\tBatch 77.914 (60.111)\tData 77.609 (59.791)\tLoss 0.87013 (1.78582)\tAcc@1  80.47 ( 55.82)\tAcc@5  91.41 ( 77.77)\tLearningRate 0.00004 (0.05094)\n",
      "0:01:50.052531 elapsed for 30\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/food_darknet25_fp16_5e4_ep030.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_results = [classification_val(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8245940594059405"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8245940594059405,\n",
       " 0.8245940594059405,\n",
       " 0.8245940594059405,\n",
       " 0.8245940594059405,\n",
       " 0.8245940594059405,\n",
       " 0.8245940594059405,\n",
       " 0.8245940594059405,\n",
       " 0.8245940594059405,\n",
       " 0.8245940594059405,\n",
       " 0.8245940594059405]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7828868760840239"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002308887647354633"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7867604548082482,\n",
       " 0.7841587974561572,\n",
       " 0.783773366737329,\n",
       " 0.7795336288302178,\n",
       " 0.7843515128155714,\n",
       " 0.7842551551358643,\n",
       " 0.7832915783387936,\n",
       " 0.7815571401040663,\n",
       " 0.7826170745808441,\n",
       " 0.778570052033147]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result = [val_retrieval(model, val_loader) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7564752475247524"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7564752475247525, 0.7564752475247525, 0.7564752475247525]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6494603969936404"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00523044716416355"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6546540759298516,\n",
       " 0.6394295625361341,\n",
       " 0.6442474465214878,\n",
       " 0.6489689728271343,\n",
       " 0.6478126806706495,\n",
       " 0.6587974561572557,\n",
       " 0.6544613605704375,\n",
       " 0.6481981113894777,\n",
       " 0.6481017537097706,\n",
       " 0.6499325496242051]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
