{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        #self.se = SEBlock(self.inplanes, ratio=9)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        #output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(2, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(2, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(2, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)           \n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20449700"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n",
    "        #[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        #transform.append(transforms.RandomRotation(30))\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "        transform.append(transforms.ColorJitter(hue=.05, saturation=.05),)\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "class CarsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \n",
    "        self.pd_csv = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pd_csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "    \n",
    "        img_name = os.path.join(self.root_dir, \n",
    "                                self.pd_csv.iloc[index, 1])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        target = self.pd_csv.iloc[index, 0]\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CarsDataset('./data/sfcars/train.csv', './data/sfcars/train/',\n",
    "                                 transform=get_transform(random_crop=True))\n",
    "test_dataset = CarsDataset('./data/sfcars/test.csv', './data/sfcars/test/',\n",
    "                                 transform=get_transform(random_crop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "tr_loader = data.DataLoader(dataset=train_dataset,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=test_dataset,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,                            \n",
    "                             num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=1e-3, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.8, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=100, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][31/32]\tBatch 28.396 (21.134)\tData 28.152 (20.803)\tLoss 6.04815 (5.61159)\tAcc@1   0.96 (  0.70)\tAcc@5   2.88 (  3.35)\tLearningRate 0.03674 (0.03366)\n",
      "0:00:48.448695 elapsed for 1 : val loss 0.005223230941425196\n",
      "Epoch: [1][31/32]\tBatch 28.344 (21.165)\tData 28.102 (20.834)\tLoss 5.37491 (5.50602)\tAcc@1   1.44 (  0.87)\tAcc@5   3.85 (  3.67)\tLearningRate 0.05085 (0.03845)\n",
      "0:00:47.973560 elapsed for 2 : val loss 0.010322099241387887\n",
      "Epoch: [2][31/32]\tBatch 28.474 (21.067)\tData 28.229 (20.738)\tLoss 5.26079 (5.41889)\tAcc@1   1.92 (  0.98)\tAcc@5   5.29 (  4.20)\tLearningRate 0.07398 (0.04632)\n",
      "0:00:47.704747 elapsed for 3 : val loss 0.017410769804750655\n",
      "Epoch: [3][31/32]\tBatch 27.984 (20.914)\tData 27.740 (20.583)\tLoss 5.18436 (5.37114)\tAcc@1   0.96 (  1.11)\tAcc@5   8.17 (  4.68)\tLearningRate 0.10556 (0.05714)\n",
      "0:00:47.870729 elapsed for 4 : val loss 0.011814450938937944\n",
      "Epoch: [4][31/32]\tBatch 27.213 (20.778)\tData 26.969 (20.447)\tLoss 5.16498 (5.33236)\tAcc@1   2.40 (  1.16)\tAcc@5   7.69 (  4.97)\tLearningRate 0.14480 (0.07075)\n",
      "0:00:47.151546 elapsed for 5 : val loss 0.013431165277950503\n",
      "Epoch: [5][31/32]\tBatch 27.586 (20.722)\tData 27.343 (20.392)\tLoss 5.11107 (5.29858)\tAcc@1   1.44 (  1.26)\tAcc@5   7.69 (  5.32)\tLearningRate 0.19075 (0.08695)\n",
      "0:00:47.170437 elapsed for 6 : val loss 0.022012187538863325\n",
      "Epoch: [6][31/32]\tBatch 28.113 (20.745)\tData 27.871 (20.415)\tLoss 5.10756 (5.26779)\tAcc@1   1.44 (  1.36)\tAcc@5   8.65 (  5.70)\tLearningRate 0.24226 (0.10552)\n",
      "0:00:47.349562 elapsed for 7 : val loss 0.01654023131451312\n",
      "Epoch: [7][31/32]\tBatch 28.643 (20.780)\tData 28.399 (20.451)\tLoss 5.06025 (5.24185)\tAcc@1   2.40 (  1.49)\tAcc@5   9.13 (  6.07)\tLearningRate 0.29806 (0.12617)\n",
      "0:00:48.105467 elapsed for 8 : val loss 0.025991792065663474\n",
      "Epoch: [8][31/32]\tBatch 28.075 (20.765)\tData 27.830 (20.435)\tLoss 4.94589 (5.21747)\tAcc@1   4.33 (  1.58)\tAcc@5  11.06 (  6.44)\tLearningRate 0.35677 (0.14861)\n",
      "0:00:47.876471 elapsed for 9 : val loss 0.03245864942171372\n",
      "Epoch: [9][31/32]\tBatch 27.713 (20.713)\tData 27.469 (20.382)\tLoss 5.00108 (5.19490)\tAcc@1   1.44 (  1.71)\tAcc@5   9.13 (  6.81)\tLearningRate 0.41694 (0.17252)\n",
      "0:00:47.444973 elapsed for 10 : val loss 0.02101728640716329\n",
      "Epoch: [10][31/32]\tBatch 28.463 (20.763)\tData 28.221 (20.433)\tLoss 4.99614 (5.17589)\tAcc@1   3.85 (  1.79)\tAcc@5  12.50 (  7.14)\tLearningRate 0.47710 (0.19757)\n",
      "0:00:47.422574 elapsed for 11 : val loss 0.015172242258425568\n",
      "Epoch: [11][31/32]\tBatch 29.440 (20.797)\tData 29.196 (20.465)\tLoss 4.89591 (5.15790)\tAcc@1   3.85 (  1.88)\tAcc@5  15.38 (  7.47)\tLearningRate 0.53574 (0.22340)\n",
      "0:00:49.353397 elapsed for 12 : val loss 0.022012187538863325\n",
      "Epoch: [12][31/32]\tBatch 29.027 (20.858)\tData 28.784 (20.526)\tLoss 4.93798 (5.14178)\tAcc@1   1.92 (  1.96)\tAcc@5  13.46 (  7.76)\tLearningRate 0.59142 (0.24966)\n",
      "0:00:49.855096 elapsed for 13 : val loss 0.013306802636487999\n",
      "Epoch: [13][31/32]\tBatch 29.114 (20.914)\tData 28.869 (20.582)\tLoss 4.95351 (5.12504)\tAcc@1   2.88 (  2.06)\tAcc@5  12.02 (  8.04)\tLearningRate 0.64278 (0.27599)\n",
      "0:00:49.922516 elapsed for 14 : val loss 0.017783857729138167\n",
      "Epoch: [14][31/32]\tBatch 29.594 (20.951)\tData 29.349 (20.619)\tLoss 4.92551 (5.10972)\tAcc@1   4.81 (  2.16)\tAcc@5   9.13 (  8.29)\tLearningRate 0.68853 (0.30205)\n",
      "0:00:50.082547 elapsed for 15 : val loss 0.03419972640218878\n",
      "Epoch: [15][31/32]\tBatch 30.004 (21.053)\tData 29.758 (20.721)\tLoss 4.89888 (5.09470)\tAcc@1   3.85 (  2.23)\tAcc@5   9.13 (  8.56)\tLearningRate 0.72755 (0.32750)\n",
      "0:00:50.818578 elapsed for 16 : val loss 0.02487252829250093\n",
      "Epoch: [16][31/32]\tBatch 29.978 (21.129)\tData 29.733 (20.797)\tLoss 4.88368 (5.07996)\tAcc@1   3.85 (  2.32)\tAcc@5  12.50 (  8.86)\tLearningRate 0.75887 (0.35202)\n",
      "0:00:50.032680 elapsed for 17 : val loss 0.010819549807237906\n",
      "Epoch: [17][31/32]\tBatch 29.584 (21.192)\tData 29.341 (20.860)\tLoss 4.94258 (5.06584)\tAcc@1   3.37 (  2.42)\tAcc@5   7.69 (  9.15)\tLearningRate 0.78173 (0.37532)\n",
      "0:00:49.123886 elapsed for 18 : val loss 0.01977365999253824\n",
      "Epoch: [18][31/32]\tBatch 28.600 (21.181)\tData 28.355 (20.850)\tLoss 4.74310 (5.05148)\tAcc@1   6.73 (  2.50)\tAcc@5  17.79 (  9.42)\tLearningRate 0.79555 (0.39712)\n",
      "0:00:48.391186 elapsed for 19 : val loss 0.028727770177838577\n",
      "Epoch: [19][31/32]\tBatch 28.195 (21.146)\tData 27.951 (20.815)\tLoss 4.73044 (5.03667)\tAcc@1   4.33 (  2.58)\tAcc@5  15.87 (  9.73)\tLearningRate 0.80000 (0.41720)\n",
      "0:00:48.427040 elapsed for 20 : val loss 0.014177341126725532\n",
      "Epoch: [20][31/32]\tBatch 28.439 (21.134)\tData 28.195 (20.802)\tLoss 4.72564 (5.02167)\tAcc@1   5.77 (  2.66)\tAcc@5  16.35 ( 10.02)\tLearningRate 0.79967 (0.43542)\n",
      "0:00:48.670010 elapsed for 21 : val loss 0.02051983584131327\n",
      "Epoch: [21][31/32]\tBatch 28.088 (21.112)\tData 27.843 (20.781)\tLoss 4.66351 (5.00697)\tAcc@1   4.81 (  2.76)\tAcc@5  20.67 ( 10.31)\tLearningRate 0.79873 (0.45196)\n",
      "0:00:47.933471 elapsed for 22 : val loss 0.01790822037060067\n",
      "Epoch: [22][31/32]\tBatch 28.347 (21.092)\tData 28.102 (20.760)\tLoss 4.51753 (4.99160)\tAcc@1   4.81 (  2.85)\tAcc@5  19.23 ( 10.62)\tLearningRate 0.79717 (0.46700)\n",
      "0:00:48.107617 elapsed for 23 : val loss 0.022758363387638352\n",
      "Epoch: [23][31/32]\tBatch 28.862 (21.092)\tData 28.617 (20.761)\tLoss 4.72421 (4.97615)\tAcc@1   2.40 (  2.93)\tAcc@5  11.54 ( 10.93)\tLearningRate 0.79500 (0.48072)\n",
      "0:00:48.478355 elapsed for 24 : val loss 0.014052978485263027\n",
      "Epoch: [24][31/32]\tBatch 28.239 (21.084)\tData 27.995 (20.753)\tLoss 4.61691 (4.96191)\tAcc@1   5.77 (  3.04)\tAcc@5  19.71 ( 11.19)\tLearningRate 0.79222 (0.49323)\n",
      "0:00:48.322403 elapsed for 25 : val loss 0.03817933092898893\n",
      "Epoch: [25][31/32]\tBatch 28.595 (21.092)\tData 28.352 (20.761)\tLoss 4.61629 (4.94691)\tAcc@1   6.73 (  3.12)\tAcc@5  20.19 ( 11.49)\tLearningRate 0.78883 (0.50467)\n",
      "0:00:48.514249 elapsed for 26 : val loss 0.048625792811839326\n",
      "Epoch: [26][31/32]\tBatch 28.166 (21.068)\tData 27.923 (20.737)\tLoss 4.61130 (4.93056)\tAcc@1   4.33 (  3.23)\tAcc@5  18.75 ( 11.81)\tLearningRate 0.78485 (0.51512)\n",
      "0:00:47.858441 elapsed for 27 : val loss 0.03407536376072628\n",
      "Epoch: [27][31/32]\tBatch 28.652 (21.093)\tData 28.409 (20.762)\tLoss 4.46054 (4.91353)\tAcc@1   8.65 (  3.34)\tAcc@5  20.67 ( 12.14)\tLearningRate 0.78027 (0.52467)\n",
      "0:00:48.355066 elapsed for 28 : val loss 0.02375326451933839\n",
      "Epoch: [28][31/32]\tBatch 29.123 (21.100)\tData 28.879 (20.768)\tLoss 4.42118 (4.89625)\tAcc@1   5.77 (  3.46)\tAcc@5  20.19 ( 12.52)\tLearningRate 0.77511 (0.53339)\n",
      "0:00:48.405263 elapsed for 29 : val loss 0.06529038676781494\n",
      "Epoch: [29][31/32]\tBatch 27.967 (21.094)\tData 27.723 (20.762)\tLoss 4.31669 (4.87763)\tAcc@1   9.13 (  3.58)\tAcc@5  23.08 ( 12.92)\tLearningRate 0.76936 (0.54135)\n",
      "0:00:47.891130 elapsed for 30 : val loss 0.032334286780251215\n",
      "Epoch: [30][31/32]\tBatch 28.050 (21.083)\tData 27.806 (20.752)\tLoss 4.22250 (4.85924)\tAcc@1   7.21 (  3.71)\tAcc@5  27.88 ( 13.30)\tLearningRate 0.76305 (0.54860)\n",
      "0:00:47.704170 elapsed for 31 : val loss 0.06317622186295237\n",
      "Epoch: [31][31/32]\tBatch 28.907 (21.116)\tData 28.664 (20.785)\tLoss 4.18034 (4.83920)\tAcc@1  12.02 (  3.88)\tAcc@5  32.21 ( 13.75)\tLearningRate 0.75618 (0.55520)\n",
      "0:00:48.592916 elapsed for 32 : val loss 0.04340256187041413\n",
      "Epoch: [32][31/32]\tBatch 28.376 (21.129)\tData 28.132 (20.798)\tLoss 4.06638 (4.81821)\tAcc@1  10.10 (  4.04)\tAcc@5  30.77 ( 14.20)\tLearningRate 0.74876 (0.56117)\n",
      "0:00:48.325981 elapsed for 33 : val loss 0.03245864942171372\n",
      "Epoch: [33][31/32]\tBatch 28.372 (21.127)\tData 28.128 (20.796)\tLoss 4.18836 (4.79930)\tAcc@1  10.10 (  4.21)\tAcc@5  26.92 ( 14.62)\tLearningRate 0.74080 (0.56657)\n",
      "0:00:47.710504 elapsed for 34 : val loss 0.03357791319487626\n",
      "Epoch: [34][31/32]\tBatch 27.899 (21.118)\tData 27.656 (20.787)\tLoss 3.90673 (4.77697)\tAcc@1  11.54 (  4.40)\tAcc@5  31.73 ( 15.13)\tLearningRate 0.73232 (0.57142)\n",
      "0:00:47.743264 elapsed for 35 : val loss 0.05546573809227708\n",
      "Epoch: [35][31/32]\tBatch 27.616 (21.104)\tData 27.371 (20.772)\tLoss 3.95486 (4.75333)\tAcc@1  11.54 (  4.62)\tAcc@5  37.02 ( 15.66)\tLearningRate 0.72332 (0.57577)\n",
      "0:00:47.515529 elapsed for 36 : val loss 0.025245616216888447\n",
      "Epoch: [36][31/32]\tBatch 27.961 (21.096)\tData 27.717 (20.765)\tLoss 3.88902 (4.72938)\tAcc@1  12.50 (  4.84)\tAcc@5  36.06 ( 16.24)\tLearningRate 0.71382 (0.57962)\n",
      "0:00:47.786481 elapsed for 37 : val loss 0.12075612486009203\n",
      "Epoch: [37][31/32]\tBatch 28.617 (21.088)\tData 28.373 (20.756)\tLoss 3.93110 (4.70371)\tAcc@1  16.35 (  5.12)\tAcc@5  35.10 ( 16.85)\tLearningRate 0.70384 (0.58302)\n",
      "0:00:48.128841 elapsed for 38 : val loss 0.10956348712846661\n",
      "Epoch: [38][31/32]\tBatch 29.587 (21.107)\tData 29.343 (20.775)\tLoss 3.88989 (4.67815)\tAcc@1  11.54 (  5.40)\tAcc@5  36.06 ( 17.46)\tLearningRate 0.69340 (0.58598)\n",
      "0:00:49.088196 elapsed for 39 : val loss 0.04763089168013929\n",
      "Epoch: [39][31/32]\tBatch 28.384 (21.097)\tData 28.140 (20.765)\tLoss 3.68587 (4.65259)\tAcc@1  15.87 (  5.69)\tAcc@5  43.27 ( 18.09)\tLearningRate 0.68250 (0.58853)\n",
      "0:00:48.259924 elapsed for 40 : val loss 0.1146623554284293\n",
      "Epoch: [40][31/32]\tBatch 28.323 (21.097)\tData 28.078 (20.765)\tLoss 3.63019 (4.62585)\tAcc@1  17.31 (  6.00)\tAcc@5  43.27 ( 18.75)\tLearningRate 0.67116 (0.59068)\n",
      "0:00:48.059127 elapsed for 41 : val loss 0.032334286780251215\n",
      "Epoch: [41][31/32]\tBatch 28.671 (21.105)\tData 28.426 (20.773)\tLoss 3.53346 (4.59915)\tAcc@1  17.79 (  6.30)\tAcc@5  43.75 ( 19.39)\tLearningRate 0.65941 (0.59245)\n",
      "0:00:48.479361 elapsed for 42 : val loss 0.10819549807237906\n",
      "Epoch: [42][31/32]\tBatch 28.289 (21.100)\tData 28.046 (20.768)\tLoss 3.59217 (4.57246)\tAcc@1  17.79 (  6.63)\tAcc@5  41.83 ( 20.04)\tLearningRate 0.64725 (0.59386)\n",
      "0:00:47.751738 elapsed for 43 : val loss 0.028727770177838577\n",
      "Epoch: [43][31/32]\tBatch 28.890 (21.100)\tData 28.646 (20.769)\tLoss 3.24491 (4.54516)\tAcc@1  24.52 (  6.97)\tAcc@5  50.00 ( 20.70)\tLearningRate 0.63472 (0.59493)\n",
      "0:00:48.541199 elapsed for 44 : val loss 0.16987936823778138\n",
      "Epoch: [44][31/32]\tBatch 28.412 (21.097)\tData 28.166 (20.766)\tLoss 3.45937 (4.51831)\tAcc@1  18.75 (  7.31)\tAcc@5  42.31 ( 21.36)\tLearningRate 0.62182 (0.59567)\n",
      "0:00:48.423006 elapsed for 45 : val loss 0.07909463997015297\n",
      "Epoch: [45][31/32]\tBatch 28.322 (21.086)\tData 28.079 (20.754)\tLoss 3.20433 (4.49079)\tAcc@1  25.48 (  7.68)\tAcc@5  51.44 ( 22.03)\tLearningRate 0.60858 (0.59609)\n",
      "0:00:48.369064 elapsed for 46 : val loss 0.1042158935455789\n",
      "Epoch: [46][31/32]\tBatch 28.162 (21.078)\tData 27.918 (20.746)\tLoss 3.24242 (4.46319)\tAcc@1  20.19 (  8.05)\tAcc@5  53.37 ( 22.70)\tLearningRate 0.59502 (0.59620)\n",
      "0:00:47.554664 elapsed for 47 : val loss 0.1361770924014426\n",
      "Epoch: [47][31/32]\tBatch 28.536 (21.088)\tData 28.293 (20.756)\tLoss 3.22992 (4.43426)\tAcc@1  22.12 (  8.44)\tAcc@5  54.33 ( 23.41)\tLearningRate 0.58116 (0.59603)\n",
      "0:00:48.373067 elapsed for 48 : val loss 0.12212411391617958\n",
      "Epoch: [48][31/32]\tBatch 28.458 (21.087)\tData 28.214 (20.755)\tLoss 3.06500 (4.40598)\tAcc@1  24.52 (  8.82)\tAcc@5  56.25 ( 24.09)\tLearningRate 0.56702 (0.59558)\n",
      "0:00:47.715387 elapsed for 49 : val loss 0.146623554284293\n",
      "Epoch: [49][31/32]\tBatch 28.701 (21.085)\tData 28.455 (20.753)\tLoss 3.09484 (4.37739)\tAcc@1  27.88 (  9.21)\tAcc@5  55.77 ( 24.79)\tLearningRate 0.55262 (0.59486)\n",
      "0:00:48.665938 elapsed for 50 : val loss 0.1920159184181072\n",
      "Epoch: [50][31/32]\tBatch 28.396 (21.082)\tData 28.153 (20.750)\tLoss 2.97919 (4.35004)\tAcc@1  31.73 (  9.58)\tAcc@5  59.62 ( 25.45)\tLearningRate 0.53799 (0.59388)\n",
      "0:00:48.558113 elapsed for 51 : val loss 0.2958587240392986\n",
      "Epoch: [51][31/32]\tBatch 28.671 (21.091)\tData 28.428 (20.759)\tLoss 2.77148 (4.32187)\tAcc@1  32.69 (  9.98)\tAcc@5  59.62 ( 26.12)\tLearningRate 0.52314 (0.59266)\n",
      "0:00:48.627558 elapsed for 52 : val loss 0.2854122621564482\n",
      "Epoch: [52][31/32]\tBatch 28.497 (21.093)\tData 28.253 (20.761)\tLoss 3.09417 (4.29302)\tAcc@1  26.44 ( 10.42)\tAcc@5  54.33 ( 26.79)\tLearningRate 0.50810 (0.59120)\n",
      "0:00:48.383034 elapsed for 53 : val loss 0.2645193383907474\n",
      "Epoch: [53][31/32]\tBatch 28.971 (21.107)\tData 28.726 (20.775)\tLoss 2.98662 (4.26468)\tAcc@1  27.88 ( 10.84)\tAcc@5  57.69 ( 27.45)\tLearningRate 0.49290 (0.58952)\n",
      "0:00:48.846587 elapsed for 54 : val loss 0.27024001989802265\n",
      "Epoch: [54][31/32]\tBatch 29.336 (21.109)\tData 29.092 (20.777)\tLoss 2.82014 (4.23666)\tAcc@1  32.21 ( 11.27)\tAcc@5  64.90 ( 28.12)\tLearningRate 0.47756 (0.58762)\n",
      "0:00:49.078797 elapsed for 55 : val loss 0.19313518219126974\n",
      "Epoch: [55][31/32]\tBatch 28.067 (21.100)\tData 27.824 (20.767)\tLoss 2.70380 (4.20850)\tAcc@1  34.13 ( 11.70)\tAcc@5  62.98 ( 28.78)\tLearningRate 0.46209 (0.58551)\n",
      "0:00:47.930288 elapsed for 56 : val loss 0.32881482402686235\n",
      "Epoch: [56][31/32]\tBatch 28.365 (21.097)\tData 28.121 (20.765)\tLoss 2.61952 (4.17992)\tAcc@1  32.69 ( 12.14)\tAcc@5  65.38 ( 29.45)\tLearningRate 0.44653 (0.58321)\n",
      "0:00:48.227952 elapsed for 57 : val loss 0.3821663972142768\n",
      "Epoch: [57][31/32]\tBatch 28.166 (21.093)\tData 27.922 (20.761)\tLoss 2.65515 (4.15180)\tAcc@1  35.10 ( 12.60)\tAcc@5  63.46 ( 30.10)\tLearningRate 0.43090 (0.58071)\n",
      "0:00:47.867635 elapsed for 58 : val loss 0.30468847158313644\n",
      "Epoch: [58][31/32]\tBatch 28.744 (21.096)\tData 28.500 (20.763)\tLoss 2.56393 (4.12376)\tAcc@1  37.98 ( 13.05)\tAcc@5  70.67 ( 30.75)\tLearningRate 0.41521 (0.57803)\n",
      "0:00:48.341450 elapsed for 59 : val loss 0.3524437259047382\n",
      "Epoch: [59][31/32]\tBatch 29.484 (21.125)\tData 29.240 (20.792)\tLoss 2.35881 (4.09613)\tAcc@1  43.27 ( 13.50)\tAcc@5  70.67 ( 31.39)\tLearningRate 0.39951 (0.57519)\n",
      "0:00:49.289118 elapsed for 60 : val loss 0.38738962815570205\n",
      "Epoch: [60][31/32]\tBatch 28.928 (21.122)\tData 28.684 (20.789)\tLoss 2.34643 (4.06841)\tAcc@1  42.31 ( 13.95)\tAcc@5  70.67 ( 32.02)\tLearningRate 0.38381 (0.57217)\n",
      "0:00:49.533239 elapsed for 61 : val loss 0.42581768436761597\n",
      "Epoch: [61][31/32]\tBatch 29.478 (21.137)\tData 29.233 (20.804)\tLoss 2.51865 (4.04070)\tAcc@1  40.87 ( 14.39)\tAcc@5  66.83 ( 32.65)\tLearningRate 0.36813 (0.56900)\n",
      "0:00:49.921599 elapsed for 62 : val loss 0.347717945529163\n",
      "Epoch: [62][31/32]\tBatch 29.334 (21.143)\tData 29.090 (20.811)\tLoss 2.29682 (4.01294)\tAcc@1  41.35 ( 14.85)\tAcc@5  71.63 ( 33.28)\tLearningRate 0.35250 (0.56569)\n",
      "0:00:49.810569 elapsed for 63 : val loss 0.4158686730506156\n",
      "Epoch: [63][31/32]\tBatch 29.630 (21.152)\tData 29.385 (20.820)\tLoss 2.35227 (3.98577)\tAcc@1  42.79 ( 15.30)\tAcc@5  72.12 ( 33.89)\tLearningRate 0.33694 (0.56223)\n",
      "0:00:49.684037 elapsed for 64 : val loss 0.4564108941673921\n",
      "Epoch: [64][31/32]\tBatch 29.024 (21.163)\tData 28.780 (20.830)\tLoss 2.19489 (3.95802)\tAcc@1  48.56 ( 15.78)\tAcc@5  70.19 ( 34.50)\tLearningRate 0.32148 (0.55864)\n",
      "0:00:49.102248 elapsed for 65 : val loss 0.3895037930605646\n",
      "Epoch: [65][31/32]\tBatch 28.345 (21.160)\tData 28.099 (20.827)\tLoss 2.04174 (3.93086)\tAcc@1  51.92 ( 16.22)\tAcc@5  75.00 ( 35.10)\tLearningRate 0.30615 (0.55493)\n",
      "0:00:48.178083 elapsed for 66 : val loss 0.48949135679641836\n",
      "Epoch: [66][31/32]\tBatch 28.376 (21.160)\tData 28.131 (20.828)\tLoss 2.16552 (3.90337)\tAcc@1  48.08 ( 16.70)\tAcc@5  73.08 ( 35.71)\tLearningRate 0.29095 (0.55110)\n",
      "0:00:48.194251 elapsed for 67 : val loss 0.48849645566471833\n",
      "Epoch: [67][31/32]\tBatch 28.072 (21.155)\tData 27.828 (20.822)\tLoss 2.18526 (3.87589)\tAcc@1  50.48 ( 17.18)\tAcc@5  73.08 ( 36.32)\tLearningRate 0.27593 (0.54716)\n",
      "0:00:47.762922 elapsed for 68 : val loss 0.49471458773784355\n",
      "Epoch: [68][31/32]\tBatch 28.595 (21.162)\tData 28.350 (20.830)\tLoss 1.89659 (3.84913)\tAcc@1  52.40 ( 17.64)\tAcc@5  79.33 ( 36.90)\tLearningRate 0.26109 (0.54312)\n",
      "0:00:48.214791 elapsed for 69 : val loss 0.4824026862330556\n",
      "Epoch: [69][31/32]\tBatch 28.767 (21.156)\tData 28.523 (20.823)\tLoss 1.91150 (3.82209)\tAcc@1  50.48 ( 18.12)\tAcc@5  78.37 ( 37.48)\tLearningRate 0.24648 (0.53898)\n",
      "0:00:48.087137 elapsed for 70 : val loss 0.46474319114537993\n",
      "Epoch: [70][31/32]\tBatch 28.836 (21.162)\tData 28.591 (20.830)\tLoss 1.84635 (3.79564)\tAcc@1  51.44 ( 18.59)\tAcc@5  79.33 ( 38.05)\tLearningRate 0.23209 (0.53476)\n",
      "0:00:48.300214 elapsed for 71 : val loss 0.5379927869667952\n",
      "Epoch: [71][31/32]\tBatch 28.575 (21.165)\tData 28.329 (20.832)\tLoss 1.80544 (3.76905)\tAcc@1  52.40 ( 19.07)\tAcc@5  80.29 ( 38.62)\tLearningRate 0.21797 (0.53045)\n",
      "0:00:48.892677 elapsed for 72 : val loss 0.547071259793558\n",
      "Epoch: [72][31/32]\tBatch 28.082 (21.158)\tData 27.837 (20.826)\tLoss 1.82379 (3.74184)\tAcc@1  51.92 ( 19.56)\tAcc@5  81.25 ( 39.20)\tLearningRate 0.20413 (0.52607)\n",
      "0:00:48.192286 elapsed for 73 : val loss 0.5546573809227708\n",
      "Epoch: [73][31/32]\tBatch 28.496 (21.157)\tData 28.253 (20.825)\tLoss 1.85000 (3.71483)\tAcc@1  53.37 ( 20.06)\tAcc@5  78.85 ( 39.77)\tLearningRate 0.19058 (0.52163)\n",
      "0:00:48.064684 elapsed for 74 : val loss 0.5889814699664221\n",
      "Epoch: [74][31/32]\tBatch 28.751 (21.155)\tData 28.507 (20.822)\tLoss 1.87761 (3.68787)\tAcc@1  55.77 ( 20.56)\tAcc@5  81.25 ( 40.33)\tLearningRate 0.17737 (0.51712)\n",
      "0:00:48.510911 elapsed for 75 : val loss 0.5815197114786718\n",
      "Epoch: [75][31/32]\tBatch 28.494 (21.152)\tData 28.251 (20.819)\tLoss 1.65595 (3.66125)\tAcc@1  59.62 ( 21.06)\tAcc@5  83.17 ( 40.88)\tLearningRate 0.16449 (0.51256)\n",
      "0:00:48.250493 elapsed for 76 : val loss 0.5905981843054346\n",
      "Epoch: [76][31/32]\tBatch 28.376 (21.150)\tData 28.131 (20.817)\tLoss 1.81145 (3.63417)\tAcc@1  54.81 ( 21.57)\tAcc@5  80.77 ( 41.43)\tLearningRate 0.15198 (0.50796)\n",
      "0:00:48.433990 elapsed for 77 : val loss 0.6295236910831986\n",
      "Epoch: [77][31/32]\tBatch 28.163 (21.144)\tData 27.919 (20.812)\tLoss 1.49679 (3.60722)\tAcc@1  63.46 ( 22.09)\tAcc@5  84.62 ( 41.98)\tLearningRate 0.13985 (0.50331)\n",
      "0:00:48.126078 elapsed for 78 : val loss 0.640467603531899\n",
      "Epoch: [78][31/32]\tBatch 28.196 (21.138)\tData 27.952 (20.806)\tLoss 1.66169 (3.58074)\tAcc@1  57.69 ( 22.60)\tAcc@5  81.25 ( 42.52)\tLearningRate 0.12812 (0.49864)\n",
      "0:00:47.451175 elapsed for 79 : val loss 0.638477801268499\n",
      "Epoch: [79][31/32]\tBatch 28.689 (21.136)\tData 28.446 (20.803)\tLoss 1.47173 (3.55344)\tAcc@1  60.10 ( 23.13)\tAcc@5  88.46 ( 43.07)\tLearningRate 0.11681 (0.49393)\n",
      "0:00:47.824930 elapsed for 80 : val loss 0.6715582638975252\n",
      "Epoch: [80][31/32]\tBatch 28.521 (21.133)\tData 28.277 (20.801)\tLoss 1.26174 (3.52652)\tAcc@1  70.67 ( 23.66)\tAcc@5  88.94 ( 43.60)\tLearningRate 0.10594 (0.48921)\n",
      "0:00:48.172062 elapsed for 81 : val loss 0.6780251212535754\n",
      "Epoch: [81][31/32]\tBatch 28.220 (21.125)\tData 27.976 (20.793)\tLoss 1.36243 (3.49979)\tAcc@1  68.75 ( 24.19)\tAcc@5  84.13 ( 44.13)\tLearningRate 0.09552 (0.48447)\n",
      "0:00:48.258966 elapsed for 82 : val loss 0.6923268250217635\n",
      "Epoch: [82][31/32]\tBatch 27.161 (21.109)\tData 26.918 (20.777)\tLoss 1.08447 (3.47296)\tAcc@1  73.56 ( 24.72)\tAcc@5  90.87 ( 44.66)\tLearningRate 0.08557 (0.47972)\n",
      "0:00:47.132031 elapsed for 83 : val loss 0.6920780997388385\n",
      "Epoch: [83][31/32]\tBatch 28.361 (21.112)\tData 28.117 (20.780)\tLoss 1.32173 (3.44630)\tAcc@1  64.42 ( 25.25)\tAcc@5  85.58 ( 45.18)\tLearningRate 0.07611 (0.47497)\n",
      "0:00:47.771245 elapsed for 84 : val loss 0.7242880238776271\n",
      "Epoch: [84][31/32]\tBatch 28.603 (21.108)\tData 28.359 (20.776)\tLoss 1.19978 (3.41927)\tAcc@1  71.15 ( 25.80)\tAcc@5  87.50 ( 45.70)\tLearningRate 0.06714 (0.47022)\n",
      "0:00:48.015753 elapsed for 85 : val loss 0.7235418480288521\n",
      "Epoch: [85][31/32]\tBatch 28.928 (21.109)\tData 28.684 (20.776)\tLoss 1.22042 (3.39235)\tAcc@1  72.12 ( 26.35)\tAcc@5  87.50 ( 46.21)\tLearningRate 0.05869 (0.46548)\n",
      "0:00:48.638034 elapsed for 86 : val loss 0.7288894416117399\n",
      "Epoch: [86][31/32]\tBatch 27.744 (21.106)\tData 27.498 (20.774)\tLoss 1.08094 (3.36512)\tAcc@1  73.56 ( 26.91)\tAcc@5  89.42 ( 46.73)\tLearningRate 0.05077 (0.46076)\n",
      "0:00:47.695399 elapsed for 87 : val loss 0.7456783982091779\n",
      "Epoch: [87][31/32]\tBatch 27.058 (21.091)\tData 26.814 (20.758)\tLoss 0.98852 (3.33811)\tAcc@1  74.52 ( 27.46)\tAcc@5  91.35 ( 47.23)\tLearningRate 0.04338 (0.45606)\n",
      "0:00:46.543304 elapsed for 88 : val loss 0.7528914314140033\n",
      "Epoch: [88][31/32]\tBatch 28.344 (21.095)\tData 28.099 (20.762)\tLoss 0.90846 (3.31116)\tAcc@1  78.37 ( 28.03)\tAcc@5  91.83 ( 47.73)\tLearningRate 0.03654 (0.45138)\n",
      "0:00:47.692827 elapsed for 89 : val loss 0.7609750031090661\n",
      "Epoch: [89][31/32]\tBatch 28.572 (21.096)\tData 28.328 (20.763)\tLoss 0.81852 (3.28404)\tAcc@1  77.88 ( 28.60)\tAcc@5  95.19 ( 48.23)\tLearningRate 0.03026 (0.44673)\n",
      "0:00:47.874406 elapsed for 90 : val loss 0.771545827633379\n",
      "Epoch: [90][31/32]\tBatch 28.681 (21.096)\tData 28.436 (20.764)\tLoss 0.71464 (3.25699)\tAcc@1  86.06 ( 29.18)\tAcc@5  92.79 ( 48.72)\tLearningRate 0.02456 (0.44212)\n",
      "0:00:48.593402 elapsed for 91 : val loss 0.7858475314015669\n",
      "Epoch: [91][31/32]\tBatch 28.258 (21.096)\tData 28.012 (20.763)\tLoss 0.66206 (3.23028)\tAcc@1  83.17 ( 29.74)\tAcc@5  95.67 ( 49.20)\tLearningRate 0.01943 (0.43756)\n",
      "0:00:48.268898 elapsed for 92 : val loss 0.7928118393234672\n",
      "Epoch: [92][31/32]\tBatch 27.656 (21.088)\tData 27.413 (20.756)\tLoss 0.67175 (3.20380)\tAcc@1  87.98 ( 30.30)\tAcc@5  92.31 ( 49.67)\tLearningRate 0.01489 (0.43303)\n",
      "0:00:47.452379 elapsed for 93 : val loss 0.7923143887576172\n",
      "Epoch: [93][31/32]\tBatch 29.001 (21.090)\tData 28.757 (20.757)\tLoss 0.77356 (3.17709)\tAcc@1  85.58 ( 30.87)\tAcc@5  92.31 ( 50.14)\tLearningRate 0.01094 (0.42856)\n",
      "0:00:48.106181 elapsed for 94 : val loss 0.8010197736599926\n",
      "Epoch: [94][31/32]\tBatch 28.582 (21.094)\tData 28.338 (20.762)\tLoss 0.79578 (3.15085)\tAcc@1  82.21 ( 31.44)\tAcc@5  91.83 ( 50.60)\tLearningRate 0.00759 (0.42415)\n",
      "0:00:47.948749 elapsed for 95 : val loss 0.8076109936575053\n",
      "Epoch: [95][31/32]\tBatch 28.293 (21.089)\tData 28.050 (20.757)\tLoss 0.62490 (3.12466)\tAcc@1  83.65 ( 32.00)\tAcc@5  96.15 ( 51.06)\tLearningRate 0.00485 (0.41979)\n",
      "0:00:47.992614 elapsed for 96 : val loss 0.8067404551672678\n",
      "Epoch: [96][31/32]\tBatch 28.237 (21.086)\tData 27.993 (20.754)\tLoss 0.55805 (3.09897)\tAcc@1  88.46 ( 32.56)\tAcc@5  95.67 ( 51.51)\tLearningRate 0.00272 (0.41550)\n",
      "0:00:48.247621 elapsed for 97 : val loss 0.8161920159184181\n",
      "Epoch: [97][31/32]\tBatch 28.372 (21.082)\tData 28.128 (20.749)\tLoss 0.65098 (3.07362)\tAcc@1  87.02 ( 33.11)\tAcc@5  93.75 ( 51.95)\tLearningRate 0.00120 (0.41128)\n",
      "0:00:48.196947 elapsed for 98 : val loss 0.8151971147867181\n",
      "Epoch: [98][31/32]\tBatch 28.380 (21.082)\tData 28.135 (20.749)\tLoss 0.68867 (3.04873)\tAcc@1  86.54 ( 33.65)\tAcc@5  93.75 ( 52.38)\tLearningRate 0.00029 (0.40714)\n",
      "0:00:47.685032 elapsed for 99 : val loss 0.8140778510135556\n",
      "Epoch: [99][31/32]\tBatch 29.135 (21.091)\tData 28.890 (20.758)\tLoss 0.59873 (3.02439)\tAcc@1  87.50 ( 34.18)\tAcc@5  95.19 ( 52.80)\tLearningRate 0.00000 (0.40307)\n",
      "0:00:48.935391 elapsed for 100 : val loss 0.8156945653525681\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    progress.display(i) \n",
    "    val_loss = classification_val(model, val_loader)\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {} : val loss {}'.format(elapsed, epoch+1, val_loss))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8156945653525681"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_val(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7588608382042035"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_retrieval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/sfcar_darknet25_ep100_wd1e3.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
