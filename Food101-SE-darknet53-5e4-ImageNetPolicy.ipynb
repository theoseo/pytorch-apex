{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from autoaugment import ImageNetPolicy\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, planes, ratio):\n",
    "\n",
    "      super(SEBlock, self).__init__()\n",
    "\n",
    "      self.se_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "      self.se_fc1 = nn.Linear(planes, planes // ratio)\n",
    "      self.relu = nn.ReLU(inplace=True)\n",
    "      self.se_fc2 = nn.Linear(planes // ratio, planes)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "      out = self.se_pool(x)\n",
    "      out = torch.flatten(out, 1)\n",
    "      out = self.se_fc1(out)\n",
    "      #print(out.shape)\n",
    "      out = F.relu(out)\n",
    "      out = self.se_fc2(out)\n",
    "      out = torch.sigmoid(out)\n",
    "      out = out.view(out.size(0), out.size(1), 1, 1)\n",
    "      #print(x.shape)\n",
    "      #print(out.shape)\n",
    "      out = torch.mul(out.expand_as(x), x)\n",
    "\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        self.se = SEBlock(self.inplanes, ratio=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(8, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(8, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(4, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)          \n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((1,3,224,224))\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 101])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41556249"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1\n",
    "                      , momentum=0.9, weight_decay=5e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        #mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        #std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n",
    "        [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        #transform.append(transforms.RandomRotation(30))\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "        transform.append(ImageNetPolicy())\n",
    "        #transform.append(transforms.ColorJitter(hue=.05, saturation=.05),)\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "data_dir = './data/food101/'\n",
    "train_data = datasets.ImageFolder(data_dir + 'train', transform=get_transform(random_crop=True))\n",
    "test_data = datasets.ImageFolder(data_dir + 'test', transform=get_transform(random_crop=False))\n",
    "\n",
    "tr_loader = data.DataLoader(dataset=train_data,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=test_data,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,\n",
    "                            pin_memory=True,                             \n",
    "                            num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=30, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 99/296]\tBatch 323.986 (172.222)\tData 323.536 (171.770)\tLoss 4.59541 (4.66438)\tAcc@1   1.17 (  1.37)\tAcc@5  10.16 (  6.03)\tLearningRate 0.00475 (0.00425)\n",
      "Epoch: [0][199/296]\tBatch 602.427 (316.386)\tData 601.984 (315.936)\tLoss 4.48689 (4.62652)\tAcc@1   3.52 (  1.83)\tAcc@5  14.06 (  7.47)\tLearningRate 0.00698 (0.00500)\n",
      "0:13:57.230714 elapsed for 1\n",
      "Epoch: [1][ 99/296]\tBatch 54.822 (345.882)\tData 54.383 (345.430)\tLoss 4.34870 (4.55412)\tAcc@1   2.73 (  2.72)\tAcc@5  18.75 ( 10.44)\tLearningRate 0.01531 (0.00785)\n",
      "Epoch: [1][199/296]\tBatch 99.825 (291.783)\tData 99.381 (291.332)\tLoss 4.36847 (4.51954)\tAcc@1   5.08 (  3.13)\tAcc@5  16.41 ( 11.75)\tLearningRate 0.02134 (0.00995)\n",
      "0:02:23.153586 elapsed for 2\n",
      "Epoch: [2][ 99/296]\tBatch 53.691 (230.526)\tData 53.241 (230.073)\tLoss 4.20818 (4.45567)\tAcc@1   7.42 (  3.94)\tAcc@5  21.09 ( 13.90)\tLearningRate 0.03572 (0.01515)\n",
      "Epoch: [2][199/296]\tBatch 98.644 (211.069)\tData 98.190 (210.616)\tLoss 4.12714 (4.42516)\tAcc@1   7.03 (  4.36)\tAcc@5  25.00 ( 14.94)\tLearningRate 0.04393 (0.01827)\n",
      "0:02:22.015575 elapsed for 3\n",
      "Epoch: [3][ 99/296]\tBatch 53.026 (183.979)\tData 52.583 (183.525)\tLoss 4.13897 (4.37362)\tAcc@1   7.81 (  5.03)\tAcc@5  23.44 ( 16.53)\tLearningRate 0.06049 (0.02501)\n",
      "Epoch: [3][199/296]\tBatch 98.027 (174.033)\tData 97.575 (173.580)\tLoss 4.06846 (4.34913)\tAcc@1  10.94 (  5.35)\tAcc@5  27.73 ( 17.32)\tLearningRate 0.06868 (0.02866)\n",
      "0:02:21.198519 elapsed for 4\n",
      "Epoch: [4][ 99/296]\tBatch 53.788 (158.860)\tData 53.337 (158.407)\tLoss 3.90829 (4.29829)\tAcc@1  11.72 (  6.04)\tAcc@5  31.64 ( 18.90)\tLearningRate 0.08299 (0.03590)\n",
      "Epoch: [4][199/296]\tBatch 98.775 (152.912)\tData 98.330 (152.459)\tLoss 3.79167 (4.27254)\tAcc@1  15.62 (  6.42)\tAcc@5  34.77 ( 19.71)\tLearningRate 0.08896 (0.03953)\n",
      "0:02:22.164268 elapsed for 5\n",
      "Epoch: [5][ 99/296]\tBatch 54.427 (143.304)\tData 53.987 (142.851)\tLoss 3.86634 (4.22094)\tAcc@1   9.38 (  7.16)\tAcc@5  33.98 ( 21.29)\tLearningRate 0.09717 (0.04622)\n",
      "Epoch: [5][199/296]\tBatch 99.178 (139.361)\tData 98.731 (138.908)\tLoss 3.77057 (4.19184)\tAcc@1  13.28 (  7.59)\tAcc@5  34.77 ( 22.19)\tLearningRate 0.09932 (0.04933)\n",
      "0:02:22.425669 elapsed for 6\n",
      "Epoch: [6][ 99/296]\tBatch 54.290 (132.685)\tData 53.837 (132.233)\tLoss 3.44091 (4.13319)\tAcc@1  20.31 (  8.50)\tAcc@5  45.70 ( 23.98)\tLearningRate 0.09995 (0.05461)\n",
      "Epoch: [6][199/296]\tBatch 99.236 (129.866)\tData 98.781 (129.414)\tLoss 3.27386 (4.10282)\tAcc@1  23.05 (  8.99)\tAcc@5  47.27 ( 24.88)\tLearningRate 0.09980 (0.05690)\n",
      "0:02:22.735484 elapsed for 7\n",
      "Epoch: [7][ 99/296]\tBatch 53.474 (124.929)\tData 53.030 (124.476)\tLoss 3.23769 (4.04184)\tAcc@1  23.05 ( 10.02)\tAcc@5  50.00 ( 26.68)\tLearningRate 0.09923 (0.06075)\n",
      "Epoch: [7][199/296]\tBatch 98.274 (122.780)\tData 97.831 (122.327)\tLoss 3.24449 (4.00926)\tAcc@1  24.22 ( 10.57)\tAcc@5  49.22 ( 27.61)\tLearningRate 0.09880 (0.06243)\n",
      "0:02:21.683367 elapsed for 8\n",
      "Epoch: [8][ 99/296]\tBatch 53.533 (118.953)\tData 53.086 (118.500)\tLoss 3.14531 (3.94489)\tAcc@1  25.78 ( 11.68)\tAcc@5  53.12 ( 29.44)\tLearningRate 0.09767 (0.06528)\n",
      "Epoch: [8][199/296]\tBatch 98.600 (117.293)\tData 98.147 (116.840)\tLoss 3.16959 (3.91226)\tAcc@1  25.39 ( 12.24)\tAcc@5  53.12 ( 30.35)\tLearningRate 0.09696 (0.06653)\n",
      "0:02:21.821348 elapsed for 9\n",
      "Epoch: [9][ 99/296]\tBatch 53.162 (114.262)\tData 52.719 (113.808)\tLoss 2.96142 (3.84824)\tAcc@1  33.59 ( 13.41)\tAcc@5  56.64 ( 32.11)\tLearningRate 0.09529 (0.06863)\n",
      "Epoch: [9][199/296]\tBatch 98.189 (112.922)\tData 97.741 (112.469)\tLoss 2.80006 (3.81557)\tAcc@1  33.59 ( 14.00)\tAcc@5  62.89 ( 33.00)\tLearningRate 0.09431 (0.06954)\n",
      "0:02:21.593895 elapsed for 10\n",
      "Epoch: [10][ 99/296]\tBatch 53.299 (110.459)\tData 52.858 (110.005)\tLoss 3.00704 (3.75480)\tAcc@1  28.91 ( 15.13)\tAcc@5  58.20 ( 34.62)\tLearningRate 0.09214 (0.07106)\n",
      "Epoch: [10][199/296]\tBatch 98.266 (109.369)\tData 97.816 (108.916)\tLoss 2.92680 (3.72430)\tAcc@1  33.59 ( 15.70)\tAcc@5  54.30 ( 35.42)\tLearningRate 0.09091 (0.07171)\n",
      "0:02:21.405149 elapsed for 11\n",
      "Epoch: [11][ 99/296]\tBatch 53.611 (107.333)\tData 53.167 (106.879)\tLoss 2.61316 (3.66560)\tAcc@1  37.11 ( 16.79)\tAcc@5  62.50 ( 36.94)\tLearningRate 0.08827 (0.07275)\n",
      "Epoch: [11][199/296]\tBatch 98.550 (106.435)\tData 98.104 (105.981)\tLoss 2.61900 (3.63670)\tAcc@1  35.16 ( 17.34)\tAcc@5  64.45 ( 37.69)\tLearningRate 0.08681 (0.07318)\n",
      "0:02:21.877485 elapsed for 12\n",
      "Epoch: [12][ 99/296]\tBatch 53.837 (104.744)\tData 53.397 (104.290)\tLoss 2.37658 (3.58092)\tAcc@1  44.14 ( 18.41)\tAcc@5  70.70 ( 39.10)\tLearningRate 0.08374 (0.07383)\n",
      "Epoch: [12][199/296]\tBatch 98.917 (103.992)\tData 98.473 (103.539)\tLoss 2.26396 (3.55352)\tAcc@1  41.80 ( 18.93)\tAcc@5  74.61 ( 39.80)\tLearningRate 0.08208 (0.07407)\n",
      "0:02:22.233893 elapsed for 13\n",
      "Epoch: [13][ 99/296]\tBatch 53.492 (102.548)\tData 53.039 (102.094)\tLoss 2.41702 (3.50092)\tAcc@1  37.50 ( 19.94)\tAcc@5  66.41 ( 41.11)\tLearningRate 0.07864 (0.07438)\n",
      "Epoch: [13][199/296]\tBatch 98.562 (101.898)\tData 98.102 (101.444)\tLoss 2.32139 (3.47502)\tAcc@1  44.14 ( 20.43)\tAcc@5  64.45 ( 41.75)\tLearningRate 0.07680 (0.07447)\n",
      "0:02:21.813461 elapsed for 14\n",
      "Epoch: [14][ 99/296]\tBatch 54.231 (100.663)\tData 53.782 (100.210)\tLoss 2.26396 (3.42527)\tAcc@1  43.75 ( 21.41)\tAcc@5  69.14 ( 42.98)\tLearningRate 0.07304 (0.07449)\n",
      "Epoch: [14][199/296]\tBatch 99.346 (100.119)\tData 98.902 (99.665)\tLoss 2.34402 (3.40045)\tAcc@1  44.14 ( 21.90)\tAcc@5  69.14 ( 43.58)\tLearningRate 0.07106 (0.07443)\n",
      "0:02:22.652817 elapsed for 15\n",
      "Epoch: [15][ 99/296]\tBatch 52.978 (99.025)\tData 52.521 (98.572)\tLoss 2.15767 (3.35329)\tAcc@1  47.66 ( 22.85)\tAcc@5  71.88 ( 44.72)\tLearningRate 0.06705 (0.07420)\n",
      "Epoch: [15][199/296]\tBatch 98.033 (98.523)\tData 97.584 (98.069)\tLoss 2.28833 (3.33011)\tAcc@1  42.19 ( 23.31)\tAcc@5  69.53 ( 45.27)\tLearningRate 0.06496 (0.07402)\n",
      "0:02:21.332367 elapsed for 16\n",
      "Epoch: [16][ 99/296]\tBatch 53.571 (97.547)\tData 53.127 (97.093)\tLoss 2.18539 (3.28525)\tAcc@1  47.27 ( 24.20)\tAcc@5  69.92 ( 46.34)\tLearningRate 0.06077 (0.07357)\n",
      "Epoch: [16][199/296]\tBatch 98.600 (97.117)\tData 98.159 (96.663)\tLoss 2.12508 (3.26321)\tAcc@1  50.39 ( 24.65)\tAcc@5  69.92 ( 46.86)\tLearningRate 0.05860 (0.07329)\n",
      "0:02:21.978600 elapsed for 17\n",
      "Epoch: [17][ 99/296]\tBatch 53.532 (96.264)\tData 53.082 (95.810)\tLoss 2.09096 (3.22146)\tAcc@1  49.22 ( 25.49)\tAcc@5  73.83 ( 47.84)\tLearningRate 0.05431 (0.07265)\n",
      "Epoch: [17][199/296]\tBatch 98.619 (95.882)\tData 98.169 (95.428)\tLoss 2.07598 (3.20045)\tAcc@1  51.95 ( 25.91)\tAcc@5  68.36 ( 48.33)\tLearningRate 0.05210 (0.07227)\n",
      "0:02:21.892073 elapsed for 18\n",
      "Epoch: [18][ 99/296]\tBatch 54.129 (95.135)\tData 53.672 (94.681)\tLoss 2.26189 (3.16062)\tAcc@1  43.36 ( 26.72)\tAcc@5  71.88 ( 49.25)\tLearningRate 0.04777 (0.07147)\n",
      "Epoch: [18][199/296]\tBatch 99.264 (94.805)\tData 98.806 (94.351)\tLoss 2.08221 (3.14048)\tAcc@1  47.66 ( 27.13)\tAcc@5  73.44 ( 49.71)\tLearningRate 0.04556 (0.07102)\n",
      "0:02:22.521607 elapsed for 19\n",
      "Epoch: [19][ 99/296]\tBatch 53.424 (94.128)\tData 52.975 (93.674)\tLoss 1.97649 (3.10236)\tAcc@1  51.95 ( 27.91)\tAcc@5  74.22 ( 50.59)\tLearningRate 0.04127 (0.07007)\n",
      "Epoch: [19][199/296]\tBatch 98.512 (93.820)\tData 98.056 (93.367)\tLoss 1.72076 (3.08293)\tAcc@1  59.38 ( 28.31)\tAcc@5  78.12 ( 51.03)\tLearningRate 0.03910 (0.06956)\n",
      "0:02:21.747413 elapsed for 20\n",
      "Epoch: [20][ 99/296]\tBatch 54.000 (93.210)\tData 53.548 (92.757)\tLoss 1.90908 (3.04622)\tAcc@1  52.34 ( 29.06)\tAcc@5  79.30 ( 51.85)\tLearningRate 0.03492 (0.06850)\n",
      "Epoch: [20][199/296]\tBatch 99.053 (92.942)\tData 98.593 (92.488)\tLoss 1.79483 (3.02828)\tAcc@1  53.91 ( 29.43)\tAcc@5  78.91 ( 52.25)\tLearningRate 0.03282 (0.06793)\n",
      "0:02:22.452366 elapsed for 21\n",
      "Epoch: [21][ 99/296]\tBatch 53.559 (92.385)\tData 53.100 (91.931)\tLoss 2.06366 (2.99271)\tAcc@1  51.17 ( 30.17)\tAcc@5  75.00 ( 53.04)\tLearningRate 0.02882 (0.06678)\n",
      "Epoch: [21][199/296]\tBatch 98.338 (92.132)\tData 97.895 (91.679)\tLoss 1.87446 (2.97514)\tAcc@1  53.52 ( 30.53)\tAcc@5  77.73 ( 53.43)\tLearningRate 0.02684 (0.06617)\n",
      "0:02:21.708833 elapsed for 22\n",
      "Epoch: [22][ 99/296]\tBatch 53.631 (91.615)\tData 53.163 (91.162)\tLoss 1.74585 (2.94048)\tAcc@1  55.08 ( 31.24)\tAcc@5  81.64 ( 54.18)\tLearningRate 0.02309 (0.06495)\n",
      "Epoch: [22][199/296]\tBatch 98.723 (91.389)\tData 98.266 (90.935)\tLoss 1.63925 (2.92315)\tAcc@1  59.38 ( 31.61)\tAcc@5  82.42 ( 54.56)\tLearningRate 0.02126 (0.06431)\n",
      "0:02:22.087319 elapsed for 23\n",
      "Epoch: [23][ 99/296]\tBatch 53.463 (90.919)\tData 53.008 (90.465)\tLoss 1.61467 (2.88918)\tAcc@1  60.16 ( 32.32)\tAcc@5  82.03 ( 55.29)\tLearningRate 0.01782 (0.06304)\n",
      "Epoch: [23][199/296]\tBatch 98.307 (90.708)\tData 97.866 (90.254)\tLoss 1.57590 (2.87202)\tAcc@1  59.77 ( 32.68)\tAcc@5  82.03 ( 55.65)\tLearningRate 0.01616 (0.06238)\n",
      "0:02:21.662033 elapsed for 24\n",
      "Epoch: [24][ 99/296]\tBatch 53.504 (90.271)\tData 53.051 (89.817)\tLoss 1.48157 (2.83890)\tAcc@1  61.33 ( 33.38)\tAcc@5  83.20 ( 56.35)\tLearningRate 0.01310 (0.06108)\n",
      "Epoch: [24][199/296]\tBatch 98.719 (90.080)\tData 98.256 (89.626)\tLoss 1.57066 (2.82222)\tAcc@1  58.59 ( 33.73)\tAcc@5  82.03 ( 56.70)\tLearningRate 0.01165 (0.06042)\n",
      "0:02:22.061141 elapsed for 25\n",
      "Epoch: [25][ 99/296]\tBatch 53.360 (89.680)\tData 52.902 (89.226)\tLoss 1.54430 (2.78915)\tAcc@1  60.94 ( 34.43)\tAcc@5  83.98 ( 57.38)\tLearningRate 0.00901 (0.05911)\n",
      "Epoch: [25][199/296]\tBatch 98.449 (89.502)\tData 97.992 (89.049)\tLoss 1.58273 (2.77234)\tAcc@1  57.42 ( 34.78)\tAcc@5  83.20 ( 57.73)\tLearningRate 0.00779 (0.05844)\n",
      "0:02:21.933535 elapsed for 26\n",
      "Epoch: [26][ 99/296]\tBatch 53.749 (89.134)\tData 53.286 (88.680)\tLoss 1.47529 (2.73939)\tAcc@1  62.11 ( 35.49)\tAcc@5  86.72 ( 58.40)\tLearningRate 0.00563 (0.05714)\n",
      "Epoch: [26][199/296]\tBatch 98.727 (88.973)\tData 98.258 (88.519)\tLoss 1.45439 (2.72249)\tAcc@1  64.06 ( 35.85)\tAcc@5  81.64 ( 58.74)\tLearningRate 0.00465 (0.05648)\n",
      "0:02:22.049722 elapsed for 27\n",
      "Epoch: [27][ 99/296]\tBatch 53.195 (88.627)\tData 52.752 (88.173)\tLoss 1.11371 (2.68946)\tAcc@1  71.09 ( 36.57)\tAcc@5  89.45 ( 59.39)\tLearningRate 0.00300 (0.05520)\n",
      "Epoch: [27][199/296]\tBatch 98.312 (88.472)\tData 97.867 (88.018)\tLoss 1.33778 (2.67255)\tAcc@1  67.58 ( 36.93)\tAcc@5  84.77 ( 59.73)\tLearningRate 0.00229 (0.05456)\n",
      "0:02:21.578646 elapsed for 28\n",
      "Epoch: [28][ 99/296]\tBatch 53.856 (88.153)\tData 53.410 (87.699)\tLoss 1.15674 (2.63986)\tAcc@1  69.53 ( 37.65)\tAcc@5  90.62 ( 60.37)\tLearningRate 0.00117 (0.05333)\n",
      "Epoch: [28][199/296]\tBatch 99.060 (88.018)\tData 98.605 (87.564)\tLoss 1.26026 (2.62331)\tAcc@1  64.84 ( 38.01)\tAcc@5  87.11 ( 60.69)\tLearningRate 0.00075 (0.05271)\n",
      "0:02:22.488867 elapsed for 29\n",
      "Epoch: [29][ 99/296]\tBatch 54.076 (87.732)\tData 53.616 (87.278)\tLoss 1.20176 (2.59186)\tAcc@1  71.09 ( 38.69)\tAcc@5  88.28 ( 61.29)\tLearningRate 0.00019 (0.05153)\n",
      "Epoch: [29][199/296]\tBatch 99.146 (87.608)\tData 98.695 (87.155)\tLoss 1.26962 (2.57612)\tAcc@1  68.75 ( 39.04)\tAcc@5  87.50 ( 61.60)\tLearningRate 0.00004 (0.05094)\n",
      "0:02:22.463342 elapsed for 30\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/darknet53_fp16_5e4_ep030.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8017029702970297"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_val(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_results = [classification_val(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7880709192522642"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024864015877712558"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7857005203314704,\n",
       " 0.7870495278473695,\n",
       " 0.7844478704952784,\n",
       " 0.7871458855270765,\n",
       " 0.7878203892850261,\n",
       " 0.7919637695124302,\n",
       " 0.7898439005588745,\n",
       " 0.785218731932935,\n",
       " 0.7910965503950664,\n",
       " 0.790422046637117]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (_, images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result = [val_retrieval(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.701416457891694"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0033551434134026586"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7089034496049336,\n",
       " 0.7012911929080747,\n",
       " 0.7007130468298324,\n",
       " 0.703025631142802,\n",
       " 0.6996531123530545,\n",
       " 0.7017729813066101,\n",
       " 0.6961842358835999,\n",
       " 0.6970514550009635,\n",
       " 0.7027365581036809,\n",
       " 0.7028329157833879]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
