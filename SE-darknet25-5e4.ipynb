{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, planes, ratio):\n",
    "\n",
    "      super(SEBlock, self).__init__()\n",
    "\n",
    "      self.se_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "      self.se_fc1 = nn.Linear(planes, planes // ratio)\n",
    "      self.relu = nn.ReLU(inplace=True)\n",
    "      self.se_fc2 = nn.Linear(planes // ratio, planes)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "      out = self.se_pool(x)\n",
    "      out = torch.flatten(out, 1)\n",
    "      out = self.se_fc1(out)\n",
    "      #print(out.shape)\n",
    "      out = F.relu(out)\n",
    "      out = self.se_fc2(out)\n",
    "      out = torch.sigmoid(out)\n",
    "      out = out.view(out.size(0), out.size(1), 1, 1)\n",
    "      #print(x.shape)\n",
    "      #print(out.shape)\n",
    "      out = torch.mul(out.expand_as(x), x)\n",
    "\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        self.se = SEBlock(self.inplanes, ratio=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(2, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(2, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(2, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((1,3,224,224))\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 150])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20755370"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)\n",
    "\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image_id, sample, target) where target is class_index of\n",
    "                the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        #print(path)\n",
    "        #print(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        image_id = path.split('/')[-1]\n",
    "\n",
    "        return image_id, sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "data_dir = 'train/train_data'\n",
    "\n",
    "dataset = CustomDataset(data_dir, transform=get_transform(random_crop=True))\n",
    "\n",
    "split_size = int(len(dataset) * 0.9)\n",
    "train_set, valid_set = data.random_split(dataset, [split_size, len(dataset) - split_size])\n",
    "tr_loader = data.DataLoader(dataset=train_set,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=valid_set,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,\n",
    "                            pin_memory=True,                             \n",
    "                            num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    _, inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=30, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 99/365]\tBatch 391.303 (209.857)\tData 390.992 (209.558)\tLoss 4.10173 (4.42981)\tAcc@1   4.30 (  5.40)\tAcc@5  23.83 ( 18.67)\tLearningRate 0.00449 (0.00417)\n",
      "Epoch: [0][199/365]\tBatch 732.104 (384.294)\tData 731.811 (383.997)\tLoss 3.78979 (4.18222)\tAcc@1  12.11 (  8.18)\tAcc@5  32.42 ( 25.03)\tLearningRate 0.00596 (0.00466)\n",
      "Epoch: [0][299/365]\tBatch 1243.391 (570.134)\tData 1243.086 (569.838)\tLoss 3.45359 (4.02665)\tAcc@1  20.31 ( 10.10)\tAcc@5  44.92 ( 29.30)\tLearningRate 0.00838 (0.00548)\n",
      "0:35:55.159932 elapsed for 1\n",
      "Epoch: [1][ 99/365]\tBatch 59.299 (634.855)\tData 58.999 (634.557)\tLoss 3.24490 (3.80336)\tAcc@1  19.92 ( 13.17)\tAcc@5  50.39 ( 35.51)\tLearningRate 0.01430 (0.00750)\n",
      "Epoch: [1][199/365]\tBatch 104.743 (536.921)\tData 104.412 (536.620)\tLoss 2.94836 (3.68188)\tAcc@1  25.39 ( 15.07)\tAcc@5  56.64 ( 38.81)\tLearningRate 0.01893 (0.00910)\n",
      "Epoch: [1][299/365]\tBatch 154.480 (475.752)\tData 154.176 (475.450)\tLoss 2.81885 (3.56600)\tAcc@1  29.69 ( 16.97)\tAcc@5  60.94 ( 41.92)\tLearningRate 0.02425 (0.01098)\n",
      "0:03:04.331047 elapsed for 2\n",
      "Epoch: [2][ 99/365]\tBatch 58.637 (398.712)\tData 58.343 (398.409)\tLoss 2.86408 (3.39138)\tAcc@1  33.20 ( 20.06)\tAcc@5  62.11 ( 46.42)\tLearningRate 0.03421 (0.01459)\n",
      "Epoch: [2][199/365]\tBatch 109.795 (364.779)\tData 109.501 (364.475)\tLoss 2.29122 (3.29986)\tAcc@1  38.67 ( 21.75)\tAcc@5  71.88 ( 48.74)\tLearningRate 0.04077 (0.01705)\n",
      "Epoch: [2][299/365]\tBatch 157.272 (342.252)\tData 156.965 (341.947)\tLoss 2.24512 (3.21387)\tAcc@1  41.41 ( 23.32)\tAcc@5  76.56 ( 50.84)\tLearningRate 0.04756 (0.01969)\n",
      "0:03:07.013508 elapsed for 3\n",
      "Epoch: [3][ 99/365]\tBatch 58.438 (307.288)\tData 58.123 (306.983)\tLoss 2.12028 (3.08141)\tAcc@1  41.02 ( 25.85)\tAcc@5  77.73 ( 53.97)\tLearningRate 0.05890 (0.02433)\n",
      "Epoch: [3][199/365]\tBatch 109.854 (290.045)\tData 109.543 (289.740)\tLoss 1.95314 (3.00957)\tAcc@1  48.83 ( 27.25)\tAcc@5  77.73 ( 55.61)\tLearningRate 0.06562 (0.02726)\n",
      "Epoch: [3][299/365]\tBatch 160.287 (278.930)\tData 159.900 (278.624)\tLoss 1.99625 (2.94255)\tAcc@1  48.83 ( 28.59)\tAcc@5  76.95 ( 57.14)\tLearningRate 0.07207 (0.03024)\n",
      "0:03:07.770915 elapsed for 4\n",
      "Epoch: [4][ 99/365]\tBatch 57.144 (258.809)\tData 56.840 (258.503)\tLoss 1.79755 (2.84036)\tAcc@1  54.30 ( 30.65)\tAcc@5  80.47 ( 59.43)\tLearningRate 0.08173 (0.03519)\n",
      "Epoch: [4][199/365]\tBatch 108.361 (248.151)\tData 108.064 (247.844)\tLoss 1.78003 (2.78406)\tAcc@1  52.34 ( 31.82)\tAcc@5  83.20 ( 60.66)\tLearningRate 0.08682 (0.03816)\n",
      "Epoch: [4][299/365]\tBatch 154.781 (241.549)\tData 154.377 (241.242)\tLoss 1.84680 (2.73069)\tAcc@1  47.27 ( 32.94)\tAcc@5  80.08 ( 61.80)\tLearningRate 0.09119 (0.04105)\n",
      "0:03:02.561999 elapsed for 5\n",
      "Epoch: [5][ 99/365]\tBatch 58.752 (228.288)\tData 58.459 (227.981)\tLoss 1.55694 (2.64821)\tAcc@1  57.42 ( 34.70)\tAcc@5  84.77 ( 63.52)\tLearningRate 0.09660 (0.04560)\n",
      "Epoch: [5][199/365]\tBatch 109.101 (221.120)\tData 108.798 (220.813)\tLoss 1.69516 (2.60259)\tAcc@1  53.91 ( 35.67)\tAcc@5  82.81 ( 64.47)\tLearningRate 0.09868 (0.04817)\n",
      "Epoch: [5][299/365]\tBatch 156.790 (216.971)\tData 156.474 (216.664)\tLoss 1.59381 (2.55914)\tAcc@1  60.55 ( 36.59)\tAcc@5  82.03 ( 65.37)\tLearningRate 0.09980 (0.05058)\n",
      "0:03:04.571414 elapsed for 6\n",
      "Epoch: [6][ 99/365]\tBatch 57.814 (207.675)\tData 57.466 (207.367)\tLoss 1.60949 (2.49207)\tAcc@1  53.12 ( 38.03)\tAcc@5  85.94 ( 66.73)\tLearningRate 0.09997 (0.05414)\n",
      "Epoch: [6][199/365]\tBatch 106.382 (202.419)\tData 106.068 (202.111)\tLoss 1.58238 (2.45436)\tAcc@1  58.98 ( 38.87)\tAcc@5  84.38 ( 67.48)\tLearningRate 0.09987 (0.05605)\n",
      "Epoch: [6][299/365]\tBatch 154.182 (199.543)\tData 153.886 (199.235)\tLoss 1.66521 (2.41865)\tAcc@1  55.86 ( 39.65)\tAcc@5  84.77 ( 68.19)\tLearningRate 0.09971 (0.05781)\n",
      "0:03:03.360601 elapsed for 7\n",
      "Epoch: [7][ 99/365]\tBatch 59.993 (192.594)\tData 59.691 (192.286)\tLoss 1.35388 (2.36278)\tAcc@1  63.67 ( 40.90)\tAcc@5  87.50 ( 69.27)\tLearningRate 0.09930 (0.06040)\n",
      "Epoch: [7][199/365]\tBatch 108.905 (188.622)\tData 108.596 (188.314)\tLoss 1.47239 (2.33139)\tAcc@1  58.98 ( 41.60)\tAcc@5  86.72 ( 69.89)\tLearningRate 0.09897 (0.06181)\n",
      "Epoch: [7][299/365]\tBatch 156.017 (186.616)\tData 155.705 (186.308)\tLoss 1.70129 (2.30136)\tAcc@1  54.30 ( 42.27)\tAcc@5  79.69 ( 70.46)\tLearningRate 0.09858 (0.06310)\n",
      "0:03:03.750678 elapsed for 8\n",
      "Epoch: [8][ 99/365]\tBatch 59.371 (181.218)\tData 59.075 (180.910)\tLoss 1.40429 (2.25387)\tAcc@1  60.55 ( 43.34)\tAcc@5  89.45 ( 71.36)\tLearningRate 0.09780 (0.06502)\n",
      "Epoch: [8][199/365]\tBatch 108.810 (178.077)\tData 108.513 (177.768)\tLoss 1.51345 (2.22753)\tAcc@1  62.11 ( 43.93)\tAcc@5  84.77 ( 71.86)\tLearningRate 0.09724 (0.06606)\n",
      "Epoch: [8][299/365]\tBatch 151.885 (176.613)\tData 151.591 (176.304)\tLoss 1.44388 (2.20175)\tAcc@1  63.28 ( 44.51)\tAcc@5  87.50 ( 72.34)\tLearningRate 0.09662 (0.06702)\n",
      "0:03:03.246854 elapsed for 9\n",
      "Epoch: [9][ 99/365]\tBatch 57.607 (172.221)\tData 57.283 (171.912)\tLoss 1.38719 (2.16084)\tAcc@1  63.67 ( 45.43)\tAcc@5  85.55 ( 73.09)\tLearningRate 0.09547 (0.06844)\n",
      "Epoch: [9][199/365]\tBatch 107.351 (169.651)\tData 107.055 (169.343)\tLoss 1.53070 (2.13852)\tAcc@1  58.20 ( 45.93)\tAcc@5  86.33 ( 73.51)\tLearningRate 0.09470 (0.06920)\n",
      "Epoch: [9][299/365]\tBatch 154.623 (168.588)\tData 154.255 (168.279)\tLoss 1.38136 (2.11652)\tAcc@1  62.11 ( 46.42)\tAcc@5  88.28 ( 73.92)\tLearningRate 0.09386 (0.06990)\n",
      "0:03:04.192165 elapsed for 10\n",
      "Epoch: [10][ 99/365]\tBatch 56.168 (165.020)\tData 55.875 (164.711)\tLoss 1.30841 (2.08144)\tAcc@1  65.23 ( 47.22)\tAcc@5  88.67 ( 74.56)\tLearningRate 0.09237 (0.07092)\n",
      "Epoch: [10][199/365]\tBatch 104.127 (162.838)\tData 103.823 (162.529)\tLoss 1.45540 (2.06147)\tAcc@1  64.45 ( 47.68)\tAcc@5  86.72 ( 74.92)\tLearningRate 0.09139 (0.07147)\n",
      "Epoch: [10][299/365]\tBatch 155.191 (162.000)\tData 154.888 (161.690)\tLoss 1.42699 (2.04246)\tAcc@1  57.81 ( 48.11)\tAcc@5  87.11 ( 75.26)\tLearningRate 0.09036 (0.07196)\n",
      "0:03:02.059683 elapsed for 11\n",
      "Epoch: [11][ 99/365]\tBatch 57.175 (158.977)\tData 56.879 (158.668)\tLoss 1.20743 (2.01174)\tAcc@1  70.31 ( 48.81)\tAcc@5  91.80 ( 75.80)\tLearningRate 0.08854 (0.07266)\n",
      "Epoch: [11][199/365]\tBatch 106.092 (157.161)\tData 105.799 (156.851)\tLoss 1.28772 (1.99434)\tAcc@1  68.36 ( 49.20)\tAcc@5  86.33 ( 76.11)\tLearningRate 0.08737 (0.07302)\n",
      "Epoch: [11][299/365]\tBatch 152.934 (156.535)\tData 152.578 (156.225)\tLoss 1.27989 (1.97784)\tAcc@1  62.50 ( 49.58)\tAcc@5  91.02 ( 76.41)\tLearningRate 0.08616 (0.07334)\n",
      "0:03:02.653007 elapsed for 12\n",
      "Epoch: [12][ 99/365]\tBatch 59.847 (153.974)\tData 59.552 (153.664)\tLoss 1.06526 (1.95085)\tAcc@1  70.31 ( 50.19)\tAcc@5  93.75 ( 76.90)\tLearningRate 0.08405 (0.07377)\n",
      "Epoch: [12][199/365]\tBatch 106.014 (152.429)\tData 105.641 (152.119)\tLoss 1.26628 (1.93519)\tAcc@1  65.23 ( 50.55)\tAcc@5  90.62 ( 77.17)\tLearningRate 0.08272 (0.07398)\n",
      "Epoch: [12][299/365]\tBatch 152.842 (151.943)\tData 152.538 (151.633)\tLoss 1.22554 (1.92033)\tAcc@1  68.75 ( 50.89)\tAcc@5  88.28 ( 77.44)\tLearningRate 0.08134 (0.07416)\n",
      "0:03:02.438451 elapsed for 13\n",
      "Epoch: [13][ 99/365]\tBatch 58.899 (149.744)\tData 58.583 (149.435)\tLoss 1.35815 (1.89679)\tAcc@1  65.23 ( 51.43)\tAcc@5  88.67 ( 77.85)\tLearningRate 0.07898 (0.07436)\n",
      "Epoch: [13][199/365]\tBatch 106.126 (148.387)\tData 105.745 (148.077)\tLoss 1.16667 (1.88257)\tAcc@1  69.92 ( 51.77)\tAcc@5  91.80 ( 78.10)\tLearningRate 0.07750 (0.07444)\n",
      "Epoch: [13][299/365]\tBatch 154.931 (148.041)\tData 154.622 (147.731)\tLoss 1.25449 (1.86909)\tAcc@1  64.84 ( 52.09)\tAcc@5  89.84 ( 78.33)\tLearningRate 0.07599 (0.07448)\n",
      "0:03:02.532297 elapsed for 14\n",
      "Epoch: [14][ 99/365]\tBatch 57.334 (146.117)\tData 57.023 (145.807)\tLoss 1.37553 (1.84731)\tAcc@1  66.41 ( 52.59)\tAcc@5  86.72 ( 78.71)\tLearningRate 0.07342 (0.07449)\n",
      "Epoch: [14][199/365]\tBatch 105.334 (144.878)\tData 105.039 (144.568)\tLoss 1.20728 (1.83459)\tAcc@1  67.58 ( 52.88)\tAcc@5  87.89 ( 78.92)\tLearningRate 0.07182 (0.07446)\n",
      "Epoch: [14][299/365]\tBatch 155.530 (144.600)\tData 155.187 (144.290)\tLoss 1.19997 (1.82224)\tAcc@1  62.50 ( 53.16)\tAcc@5  92.58 ( 79.14)\tLearningRate 0.07019 (0.07439)\n",
      "0:03:04.339155 elapsed for 15\n",
      "Epoch: [15][ 99/365]\tBatch 57.983 (142.916)\tData 57.675 (142.607)\tLoss 1.15523 (1.80245)\tAcc@1  66.41 ( 53.62)\tAcc@5  89.84 ( 79.48)\tLearningRate 0.06745 (0.07423)\n",
      "Epoch: [15][199/365]\tBatch 108.835 (141.875)\tData 108.515 (141.565)\tLoss 1.22249 (1.79097)\tAcc@1  63.28 ( 53.90)\tAcc@5  87.11 ( 79.67)\tLearningRate 0.06576 (0.07409)\n",
      "Epoch: [15][299/365]\tBatch 157.792 (141.736)\tData 157.496 (141.426)\tLoss 1.21756 (1.77995)\tAcc@1  66.41 ( 54.16)\tAcc@5  93.36 ( 79.86)\tLearningRate 0.06405 (0.07393)\n",
      "0:03:08.519721 elapsed for 16\n",
      "Epoch: [16][ 99/365]\tBatch 57.123 (140.277)\tData 56.817 (139.967)\tLoss 1.34378 (1.76150)\tAcc@1  64.45 ( 54.59)\tAcc@5  87.11 ( 80.17)\tLearningRate 0.06118 (0.07362)\n",
      "Epoch: [16][199/365]\tBatch 107.191 (139.320)\tData 106.880 (139.009)\tLoss 0.99789 (1.75104)\tAcc@1  72.66 ( 54.83)\tAcc@5  92.58 ( 80.34)\tLearningRate 0.05943 (0.07340)\n",
      "Epoch: [16][299/365]\tBatch 154.455 (139.182)\tData 154.132 (138.872)\tLoss 1.16491 (1.74083)\tAcc@1  67.97 ( 55.07)\tAcc@5  90.23 ( 80.52)\tLearningRate 0.05766 (0.07316)\n",
      "0:03:04.713090 elapsed for 17\n",
      "Epoch: [17][ 99/365]\tBatch 59.210 (137.845)\tData 58.914 (137.534)\tLoss 0.93195 (1.72395)\tAcc@1  73.83 ( 55.47)\tAcc@5  92.58 ( 80.80)\tLearningRate 0.05473 (0.07271)\n",
      "Epoch: [17][199/365]\tBatch 109.316 (136.999)\tData 109.021 (136.689)\tLoss 1.05839 (1.71405)\tAcc@1  71.09 ( 55.70)\tAcc@5  91.80 ( 80.97)\tLearningRate 0.05294 (0.07242)\n",
      "Epoch: [17][299/365]\tBatch 157.194 (136.955)\tData 156.887 (136.644)\tLoss 1.14684 (1.70457)\tAcc@1  67.58 ( 55.92)\tAcc@5  89.06 ( 81.12)\tLearningRate 0.05115 (0.07210)\n",
      "0:03:05.435474 elapsed for 18\n",
      "Epoch: [18][ 99/365]\tBatch 56.253 (135.752)\tData 55.940 (135.441)\tLoss 1.16867 (1.68892)\tAcc@1  73.05 ( 56.29)\tAcc@5  87.11 ( 81.38)\tLearningRate 0.04819 (0.07155)\n",
      "Epoch: [18][199/365]\tBatch 105.855 (134.961)\tData 105.561 (134.650)\tLoss 1.08636 (1.67944)\tAcc@1  69.14 ( 56.51)\tAcc@5  90.23 ( 81.53)\tLearningRate 0.04640 (0.07119)\n",
      "Epoch: [18][299/365]\tBatch 157.463 (134.932)\tData 157.167 (134.622)\tLoss 1.08203 (1.67050)\tAcc@1  68.75 ( 56.73)\tAcc@5  90.62 ( 81.68)\tLearningRate 0.04461 (0.07082)\n",
      "0:03:05.195535 elapsed for 19\n",
      "Epoch: [19][ 99/365]\tBatch 56.783 (133.826)\tData 56.412 (133.516)\tLoss 0.98036 (1.65549)\tAcc@1  74.61 ( 57.09)\tAcc@5  92.97 ( 81.92)\tLearningRate 0.04168 (0.07017)\n",
      "Epoch: [19][199/365]\tBatch 109.566 (133.116)\tData 109.272 (132.806)\tLoss 0.98275 (1.64654)\tAcc@1  72.66 ( 57.30)\tAcc@5  92.97 ( 82.06)\tLearningRate 0.03992 (0.06976)\n",
      "Epoch: [19][299/365]\tBatch 155.603 (133.111)\tData 155.307 (132.800)\tLoss 1.06818 (1.63776)\tAcc@1  74.22 ( 57.51)\tAcc@5  90.62 ( 82.20)\tLearningRate 0.03817 (0.06933)\n",
      "0:03:05.114102 elapsed for 20\n",
      "Epoch: [20][ 99/365]\tBatch 58.488 (132.099)\tData 58.193 (131.788)\tLoss 0.74673 (1.62349)\tAcc@1  77.73 ( 57.84)\tAcc@5  95.70 ( 82.42)\tLearningRate 0.03532 (0.06860)\n",
      "Epoch: [20][199/365]\tBatch 107.440 (131.436)\tData 107.139 (131.126)\tLoss 0.90192 (1.61517)\tAcc@1  76.17 ( 58.04)\tAcc@5  93.36 ( 82.55)\tLearningRate 0.03361 (0.06815)\n",
      "Epoch: [20][299/365]\tBatch 156.751 (131.448)\tData 156.455 (131.138)\tLoss 1.02132 (1.60687)\tAcc@1  77.73 ( 58.24)\tAcc@5  91.80 ( 82.68)\tLearningRate 0.03193 (0.06768)\n",
      "0:03:05.881918 elapsed for 21\n",
      "Epoch: [21][ 99/365]\tBatch 55.649 (130.515)\tData 55.326 (130.204)\tLoss 0.75116 (1.59277)\tAcc@1  80.08 ( 58.58)\tAcc@5  96.48 ( 82.90)\tLearningRate 0.02921 (0.06689)\n",
      "Epoch: [21][199/365]\tBatch 109.258 (129.899)\tData 108.963 (129.588)\tLoss 0.84118 (1.58447)\tAcc@1  75.39 ( 58.78)\tAcc@5  94.92 ( 83.03)\tLearningRate 0.02759 (0.06640)\n",
      "Epoch: [21][299/365]\tBatch 157.062 (129.939)\tData 156.762 (129.629)\tLoss 1.16778 (1.57657)\tAcc@1  67.97 ( 58.97)\tAcc@5  88.28 ( 83.15)\tLearningRate 0.02600 (0.06591)\n",
      "0:03:05.448314 elapsed for 22\n",
      "Epoch: [22][ 99/365]\tBatch 59.425 (129.097)\tData 59.053 (128.787)\tLoss 0.78250 (1.56296)\tAcc@1  79.30 ( 59.30)\tAcc@5  94.53 ( 83.35)\tLearningRate 0.02345 (0.06507)\n",
      "Epoch: [22][199/365]\tBatch 108.604 (128.552)\tData 108.310 (128.241)\tLoss 1.08149 (1.55481)\tAcc@1  70.70 ( 59.49)\tAcc@5  89.84 ( 83.47)\tLearningRate 0.02195 (0.06456)\n",
      "Epoch: [22][299/365]\tBatch 155.205 (128.603)\tData 154.905 (128.293)\tLoss 0.81246 (1.54691)\tAcc@1  77.34 ( 59.69)\tAcc@5  94.53 ( 83.59)\tLearningRate 0.02048 (0.06404)\n",
      "0:03:04.909779 elapsed for 23\n",
      "Epoch: [23][ 99/365]\tBatch 58.294 (127.823)\tData 57.998 (127.513)\tLoss 0.85273 (1.53376)\tAcc@1  76.17 ( 60.01)\tAcc@5  94.14 ( 83.79)\tLearningRate 0.01815 (0.06317)\n",
      "Epoch: [23][199/365]\tBatch 110.170 (127.320)\tData 109.862 (127.010)\tLoss 0.83404 (1.52570)\tAcc@1  78.91 ( 60.20)\tAcc@5  93.36 ( 83.91)\tLearningRate 0.01678 (0.06263)\n",
      "Epoch: [23][299/365]\tBatch 157.773 (127.407)\tData 157.432 (127.097)\tLoss 0.79178 (1.51769)\tAcc@1  79.30 ( 60.40)\tAcc@5  93.75 ( 84.02)\tLearningRate 0.01547 (0.06210)\n",
      "0:03:05.032270 elapsed for 24\n",
      "Epoch: [24][ 99/365]\tBatch 59.219 (126.679)\tData 58.923 (126.368)\tLoss 0.74553 (1.50461)\tAcc@1  81.25 ( 60.73)\tAcc@5  94.14 ( 84.21)\tLearningRate 0.01339 (0.06121)\n",
      "Epoch: [24][199/365]\tBatch 109.387 (126.198)\tData 109.067 (125.887)\tLoss 0.79836 (1.49654)\tAcc@1  76.17 ( 60.93)\tAcc@5  95.70 ( 84.33)\tLearningRate 0.01219 (0.06067)\n",
      "Epoch: [24][299/365]\tBatch 159.400 (126.282)\tData 159.071 (125.972)\tLoss 0.81102 (1.48870)\tAcc@1  78.12 ( 61.12)\tAcc@5  95.70 ( 84.44)\tLearningRate 0.01104 (0.06013)\n",
      "0:03:06.257878 elapsed for 25\n",
      "Epoch: [25][ 99/365]\tBatch 59.897 (125.619)\tData 59.587 (125.308)\tLoss 0.60929 (1.47530)\tAcc@1  84.38 ( 61.46)\tAcc@5  96.48 ( 84.62)\tLearningRate 0.00926 (0.05924)\n",
      "Epoch: [25][199/365]\tBatch 108.961 (125.170)\tData 108.637 (124.860)\tLoss 0.81035 (1.46725)\tAcc@1  79.69 ( 61.66)\tAcc@5  92.97 ( 84.73)\tLearningRate 0.00824 (0.05869)\n",
      "Epoch: [25][299/365]\tBatch 159.645 (125.272)\tData 159.336 (124.961)\tLoss 0.78780 (1.45940)\tAcc@1  80.47 ( 61.85)\tAcc@5  94.14 ( 84.84)\tLearningRate 0.00728 (0.05815)\n",
      "0:03:06.865925 elapsed for 26\n",
      "Epoch: [26][ 99/365]\tBatch 61.299 (124.663)\tData 60.993 (124.353)\tLoss 0.63770 (1.44604)\tAcc@1  83.59 ( 62.19)\tAcc@5  95.70 ( 85.02)\tLearningRate 0.00582 (0.05727)\n",
      "Epoch: [26][199/365]\tBatch 112.731 (124.269)\tData 112.397 (123.958)\tLoss 0.77848 (1.43784)\tAcc@1  80.08 ( 62.40)\tAcc@5  91.80 ( 85.13)\tLearningRate 0.00501 (0.05673)\n",
      "Epoch: [26][299/365]\tBatch 159.043 (124.387)\tData 158.731 (124.076)\tLoss 0.62379 (1.42970)\tAcc@1  85.16 ( 62.60)\tAcc@5  94.53 ( 85.24)\tLearningRate 0.00426 (0.05620)\n",
      "0:03:07.955162 elapsed for 27\n",
      "Epoch: [27][ 99/365]\tBatch 58.559 (123.822)\tData 58.263 (123.512)\tLoss 0.60394 (1.41639)\tAcc@1  84.38 ( 62.94)\tAcc@5  95.31 ( 85.41)\tLearningRate 0.00314 (0.05533)\n",
      "Epoch: [27][199/365]\tBatch 111.355 (123.449)\tData 111.052 (123.138)\tLoss 0.67973 (1.40829)\tAcc@1  81.25 ( 63.15)\tAcc@5  94.92 ( 85.52)\tLearningRate 0.00255 (0.05481)\n",
      "Epoch: [27][299/365]\tBatch 162.307 (123.575)\tData 162.014 (123.264)\tLoss 0.60850 (1.40022)\tAcc@1  84.38 ( 63.36)\tAcc@5  96.09 ( 85.62)\tLearningRate 0.00201 (0.05429)\n",
      "0:03:08.732325 elapsed for 28\n",
      "Epoch: [28][ 99/365]\tBatch 57.631 (123.039)\tData 57.331 (122.728)\tLoss 0.65624 (1.38687)\tAcc@1  83.20 ( 63.69)\tAcc@5  95.70 ( 85.79)\tLearningRate 0.00127 (0.05345)\n",
      "Epoch: [28][199/365]\tBatch 113.416 (122.679)\tData 113.017 (122.368)\tLoss 0.49824 (1.37880)\tAcc@1  87.11 ( 63.90)\tAcc@5  96.09 ( 85.90)\tLearningRate 0.00090 (0.05294)\n",
      "Epoch: [28][299/365]\tBatch 158.475 (122.794)\tData 158.179 (122.484)\tLoss 0.54722 (1.37087)\tAcc@1  85.16 ( 64.10)\tAcc@5  95.70 ( 86.00)\tLearningRate 0.00059 (0.05245)\n",
      "0:03:07.284338 elapsed for 29\n",
      "Epoch: [29][ 99/365]\tBatch 57.386 (122.269)\tData 57.064 (121.958)\tLoss 0.45203 (1.35800)\tAcc@1  89.06 ( 64.44)\tAcc@5  98.05 ( 86.16)\tLearningRate 0.00022 (0.05164)\n",
      "Epoch: [29][199/365]\tBatch 106.968 (121.895)\tData 106.674 (121.584)\tLoss 0.57027 (1.35022)\tAcc@1  87.50 ( 64.64)\tAcc@5  96.09 ( 86.26)\tLearningRate 0.00009 (0.05117)\n",
      "Epoch: [29][299/365]\tBatch 155.002 (121.987)\tData 154.691 (121.676)\tLoss 0.51753 (1.34266)\tAcc@1  86.72 ( 64.83)\tAcc@5  96.88 ( 86.35)\tLearningRate 0.00001 (0.05070)\n",
      "0:03:03.218833 elapsed for 30\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (_, inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/darknet25_fp16_5e4_ep030.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            _, images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_results = [classification_val(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7925515513586434"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7935054923877433,\n",
       " 0.7961071497398343,\n",
       " 0.7915783387936018,\n",
       " 0.7956253613412989,\n",
       " 0.7909038350356523,\n",
       " 0.793023703989208,\n",
       " 0.7915783387936018,\n",
       " 0.7916746964733089,\n",
       " 0.7905184043168241,\n",
       " 0.7910001927153594]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7828868760840239"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002308887647354633"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7867604548082482,\n",
       " 0.7841587974561572,\n",
       " 0.783773366737329,\n",
       " 0.7795336288302178,\n",
       " 0.7843515128155714,\n",
       " 0.7842551551358643,\n",
       " 0.7832915783387936,\n",
       " 0.7815571401040663,\n",
       " 0.7826170745808441,\n",
       " 0.778570052033147]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (_, images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result = [val_retrieval(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7019175178261708"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.694835228367701,\n",
       " 0.702929273463095,\n",
       " 0.7053382154557718,\n",
       " 0.6998458277124687,\n",
       " 0.6999421853921758,\n",
       " 0.7022547697051456,\n",
       " 0.7060127192137213,\n",
       " 0.7028329157833879,\n",
       " 0.7006166891501253,\n",
       " 0.7045673540181152]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6494603969936404"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00523044716416355"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6546540759298516,\n",
       " 0.6394295625361341,\n",
       " 0.6442474465214878,\n",
       " 0.6489689728271343,\n",
       " 0.6478126806706495,\n",
       " 0.6587974561572557,\n",
       " 0.6544613605704375,\n",
       " 0.6481981113894777,\n",
       " 0.6481017537097706,\n",
       " 0.6499325496242051]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
