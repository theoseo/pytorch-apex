{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from PIL import ImageFile\n",
    "\n",
    "from resnet import resnet50\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        #mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        #std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n",
    "        [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        #transform.append(transforms.RandomRotation(30))\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "        transform.append(transforms.ColorJitter(hue=.05, saturation=.05),)\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "data_dir = './data/food101/'\n",
    "train_data = datasets.ImageFolder(data_dir + 'train', transform=get_transform(random_crop=True))\n",
    "test_data = datasets.ImageFolder(data_dir + 'test', transform=get_transform(random_crop=False))\n",
    "tr_loader = data.DataLoader(dataset=train_data,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=test_data,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,\n",
    "                            pin_memory=True,                             \n",
    "                            num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23714981"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "test 0:00:35.127099\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "test_time = datetime.datetime.now()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()\n",
    "test_end = datetime.datetime.now() - test_time\n",
    "print('test {}'.format(test_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=30, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 99/296]\tBatch 64.714 (37.240)\tData 64.367 (36.877)\tLoss 4.59119 (4.68184)\tAcc@1   0.78 (  1.28)\tAcc@5   7.03 (  6.18)\tLearningRate 0.00475 (0.00425)\n",
      "Epoch: [0][199/296]\tBatch 118.518 (64.541)\tData 118.163 (64.180)\tLoss 4.60490 (4.62428)\tAcc@1   3.91 (  2.02)\tAcc@5  12.89 (  8.17)\tLearningRate 0.00698 (0.00500)\n",
      "0:02:46.530840 elapsed for 1\n",
      "Epoch: [1][ 99/296]\tBatch 64.298 (76.590)\tData 63.952 (76.228)\tLoss 4.35478 (4.51634)\tAcc@1   6.25 (  3.44)\tAcc@5  17.58 ( 12.26)\tLearningRate 0.01531 (0.00785)\n",
      "Epoch: [1][199/296]\tBatch 118.911 (79.553)\tData 118.563 (79.192)\tLoss 4.17624 (4.46045)\tAcc@1   7.81 (  4.13)\tAcc@5  21.88 ( 14.16)\tLearningRate 0.02134 (0.00995)\n",
      "0:02:47.295487 elapsed for 2\n",
      "Epoch: [2][ 99/296]\tBatch 64.055 (82.326)\tData 63.593 (81.965)\tLoss 4.03603 (4.34953)\tAcc@1   9.38 (  5.60)\tAcc@5  26.56 ( 17.71)\tLearningRate 0.03572 (0.01515)\n",
      "Epoch: [2][199/296]\tBatch 118.972 (83.450)\tData 118.606 (83.089)\tLoss 3.76656 (4.29750)\tAcc@1  16.41 (  6.30)\tAcc@5  34.38 ( 19.30)\tLearningRate 0.04393 (0.01827)\n",
      "0:02:47.463773 elapsed for 3\n",
      "Epoch: [3][ 99/296]\tBatch 63.744 (84.625)\tData 63.333 (84.264)\tLoss 3.61230 (4.19777)\tAcc@1  18.75 (  7.64)\tAcc@5  40.23 ( 22.28)\tLearningRate 0.06049 (0.02501)\n",
      "Epoch: [3][199/296]\tBatch 116.297 (85.085)\tData 115.945 (84.724)\tLoss 3.51116 (4.14712)\tAcc@1  18.36 (  8.35)\tAcc@5  44.53 ( 23.77)\tLearningRate 0.06868 (0.02866)\n",
      "0:02:46.244549 elapsed for 4\n",
      "Epoch: [4][ 99/296]\tBatch 63.381 (85.591)\tData 62.966 (85.229)\tLoss 3.36980 (4.05198)\tAcc@1  21.88 (  9.76)\tAcc@5  47.27 ( 26.56)\tLearningRate 0.08299 (0.03590)\n",
      "Epoch: [4][199/296]\tBatch 117.916 (85.950)\tData 117.546 (85.588)\tLoss 3.32102 (4.00289)\tAcc@1  21.88 ( 10.52)\tAcc@5  48.44 ( 27.98)\tLearningRate 0.08896 (0.03953)\n",
      "0:02:47.735024 elapsed for 5\n",
      "Epoch: [5][ 99/296]\tBatch 63.712 (86.326)\tData 63.338 (85.964)\tLoss 3.18639 (3.90714)\tAcc@1  24.22 ( 12.12)\tAcc@5  54.69 ( 30.71)\tLearningRate 0.09717 (0.04622)\n",
      "Epoch: [5][199/296]\tBatch 117.752 (86.593)\tData 117.413 (86.232)\tLoss 3.17356 (3.85876)\tAcc@1  24.22 ( 12.96)\tAcc@5  53.12 ( 32.08)\tLearningRate 0.09932 (0.04933)\n",
      "0:02:46.680417 elapsed for 6\n",
      "Epoch: [6][ 99/296]\tBatch 63.676 (86.863)\tData 63.327 (86.501)\tLoss 2.78130 (3.76428)\tAcc@1  35.16 ( 14.63)\tAcc@5  57.42 ( 34.66)\tLearningRate 0.09995 (0.05461)\n",
      "Epoch: [6][199/296]\tBatch 118.145 (87.065)\tData 117.790 (86.704)\tLoss 2.87425 (3.71730)\tAcc@1  32.03 ( 15.49)\tAcc@5  53.91 ( 35.91)\tLearningRate 0.09980 (0.05690)\n",
      "0:02:47.069006 elapsed for 7\n",
      "Epoch: [7][ 99/296]\tBatch 64.370 (87.255)\tData 64.014 (86.893)\tLoss 2.35553 (3.62985)\tAcc@1  40.23 ( 17.13)\tAcc@5  70.70 ( 38.24)\tLearningRate 0.09923 (0.06075)\n",
      "Epoch: [7][199/296]\tBatch 118.006 (87.422)\tData 117.660 (87.061)\tLoss 2.68756 (3.58744)\tAcc@1  35.16 ( 17.93)\tAcc@5  65.23 ( 39.33)\tLearningRate 0.09880 (0.06243)\n",
      "0:02:46.602128 elapsed for 8\n",
      "Epoch: [8][ 99/296]\tBatch 64.919 (87.555)\tData 64.572 (87.194)\tLoss 2.45728 (3.50684)\tAcc@1  41.41 ( 19.46)\tAcc@5  67.58 ( 41.39)\tLearningRate 0.09767 (0.06528)\n",
      "Epoch: [8][199/296]\tBatch 118.637 (87.731)\tData 118.294 (87.370)\tLoss 2.46480 (3.46714)\tAcc@1  41.02 ( 20.22)\tAcc@5  68.75 ( 42.37)\tLearningRate 0.09696 (0.06653)\n",
      "0:02:46.954303 elapsed for 9\n",
      "Epoch: [9][ 99/296]\tBatch 64.513 (87.844)\tData 64.165 (87.483)\tLoss 2.33408 (3.39289)\tAcc@1  43.36 ( 21.66)\tAcc@5  71.09 ( 44.22)\tLearningRate 0.09529 (0.06863)\n",
      "Epoch: [9][199/296]\tBatch 117.978 (87.954)\tData 117.618 (87.593)\tLoss 2.29204 (3.35618)\tAcc@1  42.19 ( 22.36)\tAcc@5  72.27 ( 45.11)\tLearningRate 0.09431 (0.06954)\n",
      "0:02:46.798285 elapsed for 10\n",
      "Epoch: [10][ 99/296]\tBatch 63.917 (87.995)\tData 63.563 (87.634)\tLoss 2.09580 (3.29100)\tAcc@1  47.66 ( 23.65)\tAcc@5  75.39 ( 46.70)\tLearningRate 0.09214 (0.07106)\n",
      "Epoch: [10][199/296]\tBatch 118.067 (88.061)\tData 117.700 (87.700)\tLoss 2.17283 (3.25819)\tAcc@1  45.31 ( 24.29)\tAcc@5  74.61 ( 47.48)\tLearningRate 0.09091 (0.07171)\n",
      "0:02:46.597334 elapsed for 11\n",
      "Epoch: [11][ 99/296]\tBatch 63.790 (88.105)\tData 63.365 (87.743)\tLoss 1.99907 (3.19598)\tAcc@1  50.00 ( 25.54)\tAcc@5  76.95 ( 48.95)\tLearningRate 0.08827 (0.07275)\n",
      "Epoch: [11][199/296]\tBatch 117.942 (88.176)\tData 117.590 (87.815)\tLoss 2.19454 (3.16589)\tAcc@1  44.53 ( 26.15)\tAcc@5  71.88 ( 49.65)\tLearningRate 0.08681 (0.07318)\n",
      "0:02:45.758922 elapsed for 12\n",
      "Epoch: [12][ 99/296]\tBatch 64.271 (88.204)\tData 63.915 (87.843)\tLoss 1.96931 (3.10913)\tAcc@1  50.78 ( 27.30)\tAcc@5  76.95 ( 50.96)\tLearningRate 0.08374 (0.07383)\n",
      "Epoch: [12][199/296]\tBatch 116.834 (88.259)\tData 116.490 (87.898)\tLoss 2.05898 (3.08159)\tAcc@1  47.66 ( 27.86)\tAcc@5  75.39 ( 51.60)\tLearningRate 0.08208 (0.07407)\n",
      "0:02:45.284315 elapsed for 13\n",
      "Epoch: [13][ 99/296]\tBatch 62.980 (88.242)\tData 62.595 (87.880)\tLoss 1.97505 (3.02898)\tAcc@1  48.05 ( 28.94)\tAcc@5  74.61 ( 52.80)\tLearningRate 0.07864 (0.07438)\n",
      "Epoch: [13][199/296]\tBatch 117.562 (88.290)\tData 117.223 (87.928)\tLoss 2.01303 (3.00379)\tAcc@1  48.83 ( 29.46)\tAcc@5  73.44 ( 53.37)\tLearningRate 0.07680 (0.07447)\n",
      "0:02:46.352955 elapsed for 14\n",
      "Epoch: [14][ 99/296]\tBatch 64.105 (88.303)\tData 63.741 (87.942)\tLoss 1.86748 (2.95510)\tAcc@1  52.73 ( 30.45)\tAcc@5  76.56 ( 54.45)\tLearningRate 0.07304 (0.07449)\n",
      "Epoch: [14][199/296]\tBatch 117.505 (88.352)\tData 117.143 (87.990)\tLoss 1.93045 (2.93186)\tAcc@1  50.78 ( 30.94)\tAcc@5  78.12 ( 54.97)\tLearningRate 0.07106 (0.07443)\n",
      "0:02:46.212765 elapsed for 15\n",
      "Epoch: [15][ 99/296]\tBatch 63.706 (88.369)\tData 63.328 (88.007)\tLoss 1.97096 (2.88699)\tAcc@1  51.56 ( 31.87)\tAcc@5  76.17 ( 55.97)\tLearningRate 0.06705 (0.07420)\n",
      "Epoch: [15][199/296]\tBatch 117.662 (88.421)\tData 117.308 (88.059)\tLoss 1.92483 (2.86489)\tAcc@1  50.00 ( 32.32)\tAcc@5  78.52 ( 56.45)\tLearningRate 0.06496 (0.07402)\n",
      "0:02:46.590725 elapsed for 16\n",
      "Epoch: [16][ 99/296]\tBatch 64.850 (88.451)\tData 64.506 (88.089)\tLoss 1.85553 (2.82312)\tAcc@1  53.91 ( 33.19)\tAcc@5  74.61 ( 57.35)\tLearningRate 0.06077 (0.07357)\n",
      "Epoch: [16][199/296]\tBatch 120.215 (88.520)\tData 119.853 (88.158)\tLoss 1.58591 (2.80247)\tAcc@1  57.81 ( 33.62)\tAcc@5  83.59 ( 57.80)\tLearningRate 0.05860 (0.07329)\n",
      "0:02:48.160619 elapsed for 17\n",
      "Epoch: [17][ 99/296]\tBatch 62.796 (88.548)\tData 62.434 (88.186)\tLoss 1.86313 (2.76323)\tAcc@1  50.78 ( 34.44)\tAcc@5  79.69 ( 58.64)\tLearningRate 0.05431 (0.07265)\n",
      "Epoch: [17][199/296]\tBatch 116.728 (88.562)\tData 116.358 (88.200)\tLoss 1.73434 (2.74398)\tAcc@1  54.30 ( 34.85)\tAcc@5  80.08 ( 59.06)\tLearningRate 0.05210 (0.07227)\n",
      "0:02:46.052304 elapsed for 18\n",
      "Epoch: [18][ 99/296]\tBatch 64.352 (88.552)\tData 63.996 (88.190)\tLoss 1.62463 (2.70673)\tAcc@1  59.77 ( 35.64)\tAcc@5  81.64 ( 59.84)\tLearningRate 0.04777 (0.07147)\n",
      "Epoch: [18][199/296]\tBatch 119.116 (88.600)\tData 118.749 (88.238)\tLoss 1.51953 (2.68861)\tAcc@1  56.64 ( 36.02)\tAcc@5  83.59 ( 60.22)\tLearningRate 0.04556 (0.07102)\n",
      "0:02:47.289290 elapsed for 19\n",
      "Epoch: [19][ 99/296]\tBatch 63.679 (88.615)\tData 63.273 (88.253)\tLoss 1.73110 (2.65341)\tAcc@1  59.38 ( 36.77)\tAcc@5  80.47 ( 60.96)\tLearningRate 0.04127 (0.07007)\n",
      "Epoch: [19][199/296]\tBatch 116.699 (88.638)\tData 116.351 (88.276)\tLoss 1.70803 (2.63630)\tAcc@1  56.64 ( 37.14)\tAcc@5  80.08 ( 61.32)\tLearningRate 0.03910 (0.06956)\n",
      "0:02:46.715581 elapsed for 20\n",
      "Epoch: [20][ 99/296]\tBatch 63.086 (88.635)\tData 62.715 (88.273)\tLoss 1.51494 (2.60241)\tAcc@1  59.38 ( 37.87)\tAcc@5  83.59 ( 62.02)\tLearningRate 0.03492 (0.06850)\n",
      "Epoch: [20][199/296]\tBatch 117.647 (88.661)\tData 117.307 (88.299)\tLoss 1.53159 (2.58564)\tAcc@1  60.55 ( 38.22)\tAcc@5  84.38 ( 62.36)\tLearningRate 0.03282 (0.06793)\n",
      "0:02:47.277558 elapsed for 21\n",
      "Epoch: [21][ 99/296]\tBatch 62.093 (88.659)\tData 61.690 (88.297)\tLoss 1.75666 (2.55288)\tAcc@1  54.69 ( 38.93)\tAcc@5  81.25 ( 63.02)\tLearningRate 0.02882 (0.06678)\n",
      "Epoch: [21][199/296]\tBatch 116.461 (88.665)\tData 116.093 (88.303)\tLoss 1.50163 (2.53646)\tAcc@1  63.67 ( 39.28)\tAcc@5  85.55 ( 63.35)\tLearningRate 0.02684 (0.06617)\n",
      "0:02:45.660242 elapsed for 22\n",
      "Epoch: [22][ 99/296]\tBatch 64.892 (88.653)\tData 64.533 (88.291)\tLoss 1.36698 (2.50455)\tAcc@1  64.84 ( 39.97)\tAcc@5  85.16 ( 63.98)\tLearningRate 0.02309 (0.06495)\n",
      "Epoch: [22][199/296]\tBatch 120.491 (88.712)\tData 120.153 (88.350)\tLoss 1.49688 (2.48861)\tAcc@1  63.28 ( 40.32)\tAcc@5  81.64 ( 64.29)\tLearningRate 0.02126 (0.06431)\n",
      "0:02:48.668764 elapsed for 23\n",
      "Epoch: [23][ 99/296]\tBatch 62.791 (88.741)\tData 62.420 (88.378)\tLoss 1.32237 (2.45759)\tAcc@1  67.19 ( 40.99)\tAcc@5  87.11 ( 64.91)\tLearningRate 0.01782 (0.06304)\n",
      "Epoch: [23][199/296]\tBatch 118.378 (88.764)\tData 118.036 (88.402)\tLoss 1.25658 (2.44181)\tAcc@1  64.45 ( 41.34)\tAcc@5  89.06 ( 65.21)\tLearningRate 0.01616 (0.06238)\n",
      "0:02:47.230192 elapsed for 24\n",
      "Epoch: [24][ 99/296]\tBatch 64.837 (88.776)\tData 64.497 (88.414)\tLoss 1.28985 (2.41067)\tAcc@1  65.62 ( 42.02)\tAcc@5  90.23 ( 65.81)\tLearningRate 0.01310 (0.06108)\n",
      "Epoch: [24][199/296]\tBatch 118.980 (88.814)\tData 118.524 (88.452)\tLoss 1.12831 (2.39521)\tAcc@1  73.44 ( 42.36)\tAcc@5  88.67 ( 66.11)\tLearningRate 0.01165 (0.06042)\n",
      "0:02:47.764326 elapsed for 25\n",
      "Epoch: [25][ 99/296]\tBatch 64.135 (88.833)\tData 63.778 (88.471)\tLoss 1.11889 (2.36436)\tAcc@1  71.09 ( 43.05)\tAcc@5  90.62 ( 66.68)\tLearningRate 0.00901 (0.05911)\n",
      "Epoch: [25][199/296]\tBatch 119.737 (88.865)\tData 119.297 (88.503)\tLoss 1.25655 (2.34886)\tAcc@1  66.02 ( 43.39)\tAcc@5  87.89 ( 66.97)\tLearningRate 0.00779 (0.05844)\n",
      "0:02:48.754534 elapsed for 26\n",
      "Epoch: [26][ 99/296]\tBatch 63.942 (88.885)\tData 63.578 (88.523)\tLoss 1.01189 (2.31775)\tAcc@1  75.39 ( 44.09)\tAcc@5  90.62 ( 67.54)\tLearningRate 0.00563 (0.05714)\n",
      "Epoch: [26][199/296]\tBatch 117.002 (88.900)\tData 116.648 (88.537)\tLoss 1.12528 (2.30194)\tAcc@1  72.27 ( 44.44)\tAcc@5  90.62 ( 67.83)\tLearningRate 0.00465 (0.05648)\n",
      "0:02:45.208685 elapsed for 27\n",
      "Epoch: [27][ 99/296]\tBatch 63.313 (88.876)\tData 62.962 (88.514)\tLoss 0.96221 (2.27129)\tAcc@1  75.78 ( 45.14)\tAcc@5  92.58 ( 68.38)\tLearningRate 0.00300 (0.05520)\n",
      "Epoch: [27][199/296]\tBatch 117.641 (88.892)\tData 117.277 (88.530)\tLoss 0.96750 (2.25563)\tAcc@1  73.83 ( 45.49)\tAcc@5  91.41 ( 68.65)\tLearningRate 0.00229 (0.05456)\n",
      "0:02:46.381609 elapsed for 28\n",
      "Epoch: [28][ 99/296]\tBatch 64.290 (88.891)\tData 63.939 (88.528)\tLoss 0.77535 (2.22525)\tAcc@1  81.64 ( 46.18)\tAcc@5  93.36 ( 69.19)\tLearningRate 0.00117 (0.05333)\n",
      "Epoch: [28][199/296]\tBatch 117.675 (88.916)\tData 117.331 (88.553)\tLoss 0.86459 (2.20977)\tAcc@1  79.69 ( 46.54)\tAcc@5  92.19 ( 69.45)\tLearningRate 0.00075 (0.05271)\n",
      "0:02:46.462971 elapsed for 29\n",
      "Epoch: [29][ 99/296]\tBatch 64.168 (88.908)\tData 63.792 (88.545)\tLoss 1.03477 (2.18061)\tAcc@1  74.61 ( 47.21)\tAcc@5  89.45 ( 69.96)\tLearningRate 0.00019 (0.05153)\n",
      "Epoch: [29][199/296]\tBatch 117.040 (88.928)\tData 116.668 (88.565)\tLoss 0.74324 (2.16587)\tAcc@1  82.03 ( 47.55)\tAcc@5  94.92 ( 70.21)\tLearningRate 0.00004 (0.05094)\n",
      "0:02:46.826063 elapsed for 30\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/food_resnet50_fp16_sconv_ep030.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8085940594059406"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_val(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_results = [classification_val(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7856330699556754"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7881094623241472,\n",
       " 0.7854114472923492,\n",
       " 0.7869531701676623,\n",
       " 0.7803044902678743,\n",
       " 0.7797263441896319,\n",
       " 0.7850260165735209,\n",
       " 0.7908074773559453,\n",
       " 0.7862786664097129,\n",
       " 0.7879167469647331,\n",
       " 0.7857968780111775]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7725380612834843"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.425454679560811e-06"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025348480584762496"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7239603960396039"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_retrieval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result = [val_retrieval(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6885912507226826"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.684428598959337,\n",
       " 0.6872229716708421,\n",
       " 0.693582578531509,\n",
       " 0.6871266139911351,\n",
       " 0.6907882058200039,\n",
       " 0.6866448255925998,\n",
       " 0.6872229716708421,\n",
       " 0.690980921179418,\n",
       " 0.6823087300057814,\n",
       " 0.6956060898053575]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6178358065137791"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0038908581994208"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6151474272499519,\n",
       " 0.6104259009443053,\n",
       " 0.615436500289073,\n",
       " 0.6237232607438813,\n",
       " 0.6200616689150126,\n",
       " 0.6146656388514165,\n",
       " 0.6214106764309115,\n",
       " 0.6160146463673155,\n",
       " 0.6200616689150126,\n",
       " 0.6214106764309115]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
