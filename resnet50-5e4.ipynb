{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from PIL import ImageFile\n",
    "\n",
    "from resnet import resnet50\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)\n",
    "\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image_id, sample, target) where target is class_index of\n",
    "                the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        #print(path)\n",
    "        #print(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        image_id = path.split('/')[-1]\n",
    "\n",
    "        return image_id, sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "data_dir = 'train/train_data'\n",
    "\n",
    "dataset = CustomDataset(data_dir, transform=get_transform(random_crop=True))\n",
    "\n",
    "split_size = int(len(dataset) * 0.9)\n",
    "train_set, valid_set = data.random_split(dataset, [split_size, len(dataset) - split_size])\n",
    "tr_loader = data.DataLoader(dataset=train_set,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=valid_set,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,\n",
    "                            pin_memory=True,                             \n",
    "                            num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23815382"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "test 0:03:52.709179\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "test_time = datetime.datetime.now()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    _, inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()\n",
    "test_end = datetime.datetime.now() - test_time\n",
    "print('test {}'.format(test_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=30, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 99/365]\tBatch 396.348 (209.156)\tData 396.012 (208.814)\tLoss 4.68770 (4.83100)\tAcc@1   1.95 (  1.34)\tAcc@5   6.64 (  6.19)\tLearningRate 0.00449 (0.00417)\n",
      "Epoch: [0][199/365]\tBatch 738.460 (383.965)\tData 738.123 (383.625)\tLoss 4.40212 (4.68312)\tAcc@1   4.30 (  2.39)\tAcc@5  15.62 (  9.81)\tLearningRate 0.00596 (0.00466)\n",
      "Epoch: [0][299/365]\tBatch 1073.180 (558.496)\tData 1072.844 (558.157)\tLoss 4.13271 (4.54419)\tAcc@1  11.72 (  3.67)\tAcc@5  25.00 ( 13.57)\tLearningRate 0.00838 (0.00548)\n",
      "0:21:11.971404 elapsed for 1\n",
      "Epoch: [1][ 99/365]\tBatch 58.688 (534.289)\tData 58.344 (533.944)\tLoss 3.80376 (4.34835)\tAcc@1  12.11 (  5.93)\tAcc@5  37.50 ( 19.36)\tLearningRate 0.01430 (0.00750)\n",
      "Epoch: [1][199/365]\tBatch 107.588 (454.300)\tData 107.200 (453.951)\tLoss 3.62843 (4.24905)\tAcc@1  12.50 (  7.20)\tAcc@5  42.97 ( 22.37)\tLearningRate 0.01893 (0.00910)\n",
      "Epoch: [1][299/365]\tBatch 156.355 (405.757)\tData 155.996 (405.406)\tLoss 3.55857 (4.15325)\tAcc@1  19.92 (  8.49)\tAcc@5  45.31 ( 25.25)\tLearningRate 0.02425 (0.01098)\n",
      "0:03:04.903070 elapsed for 2\n",
      "Epoch: [2][ 99/365]\tBatch 59.014 (342.712)\tData 58.677 (342.360)\tLoss 3.26179 (4.00824)\tAcc@1  22.66 ( 10.52)\tAcc@5  47.27 ( 29.48)\tLearningRate 0.03421 (0.01459)\n",
      "Epoch: [2][199/365]\tBatch 108.253 (314.891)\tData 107.915 (314.538)\tLoss 3.21250 (3.92155)\tAcc@1  23.44 ( 11.81)\tAcc@5  57.03 ( 31.91)\tLearningRate 0.04077 (0.01705)\n",
      "Epoch: [2][299/365]\tBatch 156.990 (297.212)\tData 156.653 (296.857)\tLoss 2.94527 (3.84050)\tAcc@1  25.78 ( 13.07)\tAcc@5  60.16 ( 34.18)\tLearningRate 0.04756 (0.01969)\n",
      "0:03:06.089624 elapsed for 3\n",
      "Epoch: [3][ 99/365]\tBatch 59.904 (268.498)\tData 59.566 (268.143)\tLoss 2.53566 (3.71084)\tAcc@1  35.16 ( 15.12)\tAcc@5  66.02 ( 37.71)\tLearningRate 0.05890 (0.02433)\n",
      "Epoch: [3][199/365]\tBatch 106.614 (254.101)\tData 106.256 (253.746)\tLoss 2.78648 (3.63718)\tAcc@1  28.91 ( 16.33)\tAcc@5  62.11 ( 39.68)\tLearningRate 0.06562 (0.02726)\n",
      "Epoch: [3][299/365]\tBatch 155.706 (245.354)\tData 155.281 (244.997)\tLoss 2.65984 (3.56660)\tAcc@1  30.86 ( 17.52)\tAcc@5  63.67 ( 41.51)\tLearningRate 0.07207 (0.03024)\n",
      "0:03:05.831580 elapsed for 4\n",
      "Epoch: [4][ 99/365]\tBatch 58.099 (228.660)\tData 57.733 (228.304)\tLoss 2.43102 (3.45346)\tAcc@1  36.33 ( 19.51)\tAcc@5  68.75 ( 44.40)\tLearningRate 0.08173 (0.03519)\n",
      "Epoch: [4][199/365]\tBatch 108.975 (219.932)\tData 108.571 (219.575)\tLoss 2.35274 (3.39013)\tAcc@1  43.36 ( 20.65)\tAcc@5  69.14 ( 45.98)\tLearningRate 0.08682 (0.03816)\n",
      "Epoch: [4][299/365]\tBatch 156.694 (214.966)\tData 156.255 (214.608)\tLoss 2.25878 (3.32748)\tAcc@1  43.36 ( 21.81)\tAcc@5  71.88 ( 47.49)\tLearningRate 0.09119 (0.04105)\n",
      "0:03:07.717451 elapsed for 5\n",
      "Epoch: [5][ 99/365]\tBatch 59.393 (204.116)\tData 59.050 (203.758)\tLoss 1.97586 (3.23172)\tAcc@1  50.78 ( 23.62)\tAcc@5  78.12 ( 49.82)\tLearningRate 0.09660 (0.04560)\n",
      "Epoch: [5][199/365]\tBatch 109.037 (198.211)\tData 108.642 (197.852)\tLoss 2.25673 (3.17654)\tAcc@1  40.62 ( 24.71)\tAcc@5  73.83 ( 51.13)\tLearningRate 0.09868 (0.04817)\n",
      "Epoch: [5][299/365]\tBatch 158.804 (195.197)\tData 158.399 (194.838)\tLoss 2.18803 (3.12615)\tAcc@1  44.14 ( 25.67)\tAcc@5  74.22 ( 52.31)\tLearningRate 0.09980 (0.05058)\n",
      "0:03:06.561213 elapsed for 6\n",
      "Epoch: [6][ 99/365]\tBatch 58.898 (187.539)\tData 58.560 (187.180)\tLoss 1.88170 (3.04458)\tAcc@1  46.88 ( 27.26)\tAcc@5  80.47 ( 54.19)\tLearningRate 0.09997 (0.05414)\n",
      "Epoch: [6][199/365]\tBatch 107.939 (183.214)\tData 107.600 (182.855)\tLoss 1.76863 (2.99708)\tAcc@1  54.69 ( 28.18)\tAcc@5  85.16 ( 55.27)\tLearningRate 0.09987 (0.05605)\n",
      "Epoch: [6][299/365]\tBatch 156.721 (181.191)\tData 156.336 (180.832)\tLoss 1.83908 (2.95225)\tAcc@1  51.95 ( 29.07)\tAcc@5  81.25 ( 56.28)\tLearningRate 0.09971 (0.05781)\n",
      "0:03:06.004929 elapsed for 7\n",
      "Epoch: [7][ 99/365]\tBatch 58.463 (175.409)\tData 58.079 (175.050)\tLoss 1.80237 (2.88175)\tAcc@1  52.73 ( 30.49)\tAcc@5  82.42 ( 57.82)\tLearningRate 0.09930 (0.06040)\n",
      "Epoch: [7][199/365]\tBatch 107.919 (172.070)\tData 107.524 (171.710)\tLoss 1.80397 (2.84186)\tAcc@1  54.30 ( 31.30)\tAcc@5  81.64 ( 58.70)\tLearningRate 0.09897 (0.06181)\n",
      "Epoch: [7][299/365]\tBatch 157.887 (170.705)\tData 157.512 (170.345)\tLoss 1.76070 (2.80409)\tAcc@1  51.17 ( 32.07)\tAcc@5  83.20 ( 59.52)\tLearningRate 0.09858 (0.06310)\n",
      "0:03:06.010063 elapsed for 8\n",
      "Epoch: [8][ 99/365]\tBatch 58.657 (166.220)\tData 58.285 (165.860)\tLoss 1.61455 (2.74440)\tAcc@1  57.42 ( 33.29)\tAcc@5  83.59 ( 60.81)\tLearningRate 0.09780 (0.06502)\n",
      "Epoch: [8][199/365]\tBatch 105.600 (163.528)\tData 105.185 (163.168)\tLoss 1.95915 (2.70979)\tAcc@1  47.27 ( 34.01)\tAcc@5  79.69 ( 61.55)\tLearningRate 0.09724 (0.06606)\n",
      "Epoch: [8][299/365]\tBatch 154.321 (162.519)\tData 153.939 (162.158)\tLoss 1.78896 (2.67758)\tAcc@1  50.39 ( 34.68)\tAcc@5  81.25 ( 62.23)\tLearningRate 0.09662 (0.06702)\n",
      "0:03:05.054876 elapsed for 9\n",
      "Epoch: [9][ 99/365]\tBatch 56.997 (158.875)\tData 56.660 (158.514)\tLoss 1.52282 (2.62681)\tAcc@1  60.16 ( 35.75)\tAcc@5  87.89 ( 63.29)\tLearningRate 0.09547 (0.06844)\n",
      "Epoch: [9][199/365]\tBatch 108.645 (156.674)\tData 108.306 (156.314)\tLoss 1.66494 (2.59676)\tAcc@1  53.91 ( 36.37)\tAcc@5  83.59 ( 63.92)\tLearningRate 0.09470 (0.06920)\n",
      "Epoch: [9][299/365]\tBatch 157.316 (156.057)\tData 156.914 (155.697)\tLoss 1.66897 (2.56888)\tAcc@1  55.47 ( 36.96)\tAcc@5  80.86 ( 64.50)\tLearningRate 0.09386 (0.06990)\n",
      "0:03:05.645220 elapsed for 10\n",
      "Epoch: [10][ 99/365]\tBatch 57.869 (153.062)\tData 57.512 (152.702)\tLoss 1.63822 (2.52371)\tAcc@1  54.30 ( 37.91)\tAcc@5  85.55 ( 65.42)\tLearningRate 0.09237 (0.07092)\n",
      "Epoch: [10][199/365]\tBatch 108.028 (151.247)\tData 107.677 (150.887)\tLoss 1.46663 (2.49820)\tAcc@1  64.06 ( 38.46)\tAcc@5  85.16 ( 65.93)\tLearningRate 0.09139 (0.07147)\n",
      "Epoch: [10][299/365]\tBatch 156.468 (150.769)\tData 156.028 (150.408)\tLoss 1.72380 (2.47314)\tAcc@1  57.42 ( 38.99)\tAcc@5  82.81 ( 66.44)\tLearningRate 0.09036 (0.07196)\n",
      "0:03:05.835171 elapsed for 11\n",
      "Epoch: [11][ 99/365]\tBatch 58.354 (148.252)\tData 57.979 (147.891)\tLoss 1.37008 (2.43336)\tAcc@1  64.06 ( 39.84)\tAcc@5  87.50 ( 67.24)\tLearningRate 0.08854 (0.07266)\n",
      "Epoch: [11][199/365]\tBatch 108.045 (146.722)\tData 107.664 (146.361)\tLoss 1.51123 (2.41124)\tAcc@1  59.77 ( 40.31)\tAcc@5  85.94 ( 67.70)\tLearningRate 0.08737 (0.07302)\n",
      "Epoch: [11][299/365]\tBatch 155.925 (146.368)\tData 155.541 (146.007)\tLoss 1.39740 (2.38935)\tAcc@1  62.89 ( 40.78)\tAcc@5  87.50 ( 68.14)\tLearningRate 0.08616 (0.07334)\n",
      "0:03:04.988792 elapsed for 12\n",
      "Epoch: [12][ 99/365]\tBatch 59.127 (144.205)\tData 58.788 (143.844)\tLoss 1.34443 (2.35429)\tAcc@1  66.80 ( 41.54)\tAcc@5  87.11 ( 68.83)\tLearningRate 0.08405 (0.07377)\n",
      "Epoch: [12][199/365]\tBatch 107.276 (142.871)\tData 106.924 (142.511)\tLoss 1.59322 (2.33407)\tAcc@1  57.81 ( 41.97)\tAcc@5  82.81 ( 69.23)\tLearningRate 0.08272 (0.07398)\n",
      "Epoch: [12][299/365]\tBatch 155.668 (142.641)\tData 155.324 (142.280)\tLoss 1.34656 (2.31437)\tAcc@1  65.23 ( 42.40)\tAcc@5  89.84 ( 69.61)\tLearningRate 0.08134 (0.07416)\n",
      "0:03:04.404577 elapsed for 13\n",
      "Epoch: [13][ 99/365]\tBatch 57.485 (140.757)\tData 57.146 (140.396)\tLoss 1.28059 (2.28297)\tAcc@1  66.02 ( 43.09)\tAcc@5  87.89 ( 70.22)\tLearningRate 0.07898 (0.07436)\n",
      "Epoch: [13][199/365]\tBatch 107.136 (139.588)\tData 106.798 (139.227)\tLoss 1.34138 (2.26499)\tAcc@1  64.06 ( 43.48)\tAcc@5  87.89 ( 70.57)\tLearningRate 0.07750 (0.07444)\n",
      "Epoch: [13][299/365]\tBatch 157.678 (139.458)\tData 157.307 (139.097)\tLoss 1.29749 (2.24733)\tAcc@1  66.02 ( 43.86)\tAcc@5  88.67 ( 70.92)\tLearningRate 0.07599 (0.07448)\n",
      "0:03:06.439243 elapsed for 14\n",
      "Epoch: [14][ 99/365]\tBatch 57.363 (137.839)\tData 57.016 (137.478)\tLoss 1.18972 (2.21940)\tAcc@1  66.41 ( 44.47)\tAcc@5  91.41 ( 71.45)\tLearningRate 0.07342 (0.07449)\n",
      "Epoch: [14][199/365]\tBatch 108.055 (136.800)\tData 107.710 (136.439)\tLoss 1.35056 (2.20259)\tAcc@1  63.28 ( 44.83)\tAcc@5  87.50 ( 71.78)\tLearningRate 0.07182 (0.07446)\n",
      "Epoch: [14][299/365]\tBatch 158.242 (136.740)\tData 157.835 (136.379)\tLoss 1.44242 (2.18725)\tAcc@1  60.94 ( 45.18)\tAcc@5  86.33 ( 72.08)\tLearningRate 0.07019 (0.07439)\n",
      "0:03:05.447460 elapsed for 15\n",
      "Epoch: [15][ 99/365]\tBatch 58.655 (135.312)\tData 58.314 (134.951)\tLoss 1.35608 (2.16162)\tAcc@1  63.28 ( 45.74)\tAcc@5  89.06 ( 72.56)\tLearningRate 0.06745 (0.07423)\n",
      "Epoch: [15][199/365]\tBatch 106.215 (134.395)\tData 105.876 (134.034)\tLoss 1.19331 (2.14616)\tAcc@1  67.97 ( 46.09)\tAcc@5  91.80 ( 72.86)\tLearningRate 0.06576 (0.07409)\n",
      "Epoch: [15][299/365]\tBatch 154.903 (134.315)\tData 154.544 (133.953)\tLoss 1.46809 (2.13167)\tAcc@1  61.33 ( 46.40)\tAcc@5  89.45 ( 73.13)\tLearningRate 0.06405 (0.07393)\n",
      "0:03:05.455865 elapsed for 16\n",
      "Epoch: [16][ 99/365]\tBatch 57.014 (133.012)\tData 56.676 (132.650)\tLoss 1.28100 (2.10757)\tAcc@1  66.80 ( 46.93)\tAcc@5  90.23 ( 73.58)\tLearningRate 0.06118 (0.07362)\n",
      "Epoch: [16][199/365]\tBatch 108.609 (132.184)\tData 108.270 (131.823)\tLoss 1.18226 (2.09354)\tAcc@1  68.36 ( 47.25)\tAcc@5  91.02 ( 73.84)\tLearningRate 0.05943 (0.07340)\n",
      "Epoch: [16][299/365]\tBatch 155.287 (132.182)\tData 154.947 (131.820)\tLoss 1.28001 (2.08032)\tAcc@1  67.19 ( 47.55)\tAcc@5  89.45 ( 74.09)\tLearningRate 0.05766 (0.07316)\n",
      "0:03:06.138655 elapsed for 17\n",
      "Epoch: [17][ 99/365]\tBatch 60.036 (131.041)\tData 59.697 (130.679)\tLoss 1.03248 (2.05838)\tAcc@1  74.61 ( 48.03)\tAcc@5  92.19 ( 74.49)\tLearningRate 0.05473 (0.07271)\n",
      "Epoch: [17][199/365]\tBatch 109.168 (130.311)\tData 108.829 (129.950)\tLoss 1.29857 (2.04556)\tAcc@1  64.06 ( 48.32)\tAcc@5  88.67 ( 74.73)\tLearningRate 0.05294 (0.07242)\n",
      "Epoch: [17][299/365]\tBatch 156.630 (130.356)\tData 156.281 (129.994)\tLoss 1.23142 (2.03287)\tAcc@1  67.58 ( 48.61)\tAcc@5  89.06 ( 74.96)\tLearningRate 0.05115 (0.07210)\n",
      "0:03:07.459470 elapsed for 18\n",
      "Epoch: [18][ 99/365]\tBatch 57.911 (129.306)\tData 57.526 (128.944)\tLoss 1.13690 (2.01210)\tAcc@1  70.31 ( 49.08)\tAcc@5  89.84 ( 75.34)\tLearningRate 0.04819 (0.07155)\n",
      "Epoch: [18][199/365]\tBatch 107.129 (128.610)\tData 106.766 (128.248)\tLoss 1.21566 (2.00000)\tAcc@1  65.23 ( 49.35)\tAcc@5  90.23 ( 75.56)\tLearningRate 0.04640 (0.07119)\n",
      "Epoch: [18][299/365]\tBatch 157.204 (128.654)\tData 156.753 (128.292)\tLoss 1.20399 (1.98791)\tAcc@1  71.09 ( 49.62)\tAcc@5  87.89 ( 75.77)\tLearningRate 0.04461 (0.07082)\n",
      "0:03:06.255807 elapsed for 19\n",
      "Epoch: [19][ 99/365]\tBatch 58.679 (127.712)\tData 58.341 (127.350)\tLoss 1.24469 (1.96799)\tAcc@1  64.06 ( 50.07)\tAcc@5  90.23 ( 76.13)\tLearningRate 0.04168 (0.07017)\n",
      "Epoch: [19][199/365]\tBatch 107.149 (127.095)\tData 106.805 (126.733)\tLoss 1.25964 (1.95649)\tAcc@1  64.06 ( 50.33)\tAcc@5  89.84 ( 76.33)\tLearningRate 0.03992 (0.06976)\n",
      "Epoch: [19][299/365]\tBatch 155.461 (127.153)\tData 155.121 (126.791)\tLoss 1.16647 (1.94519)\tAcc@1  67.19 ( 50.59)\tAcc@5  90.62 ( 76.53)\tLearningRate 0.03817 (0.06933)\n",
      "0:03:05.216062 elapsed for 20\n",
      "Epoch: [20][ 99/365]\tBatch 59.561 (126.264)\tData 59.207 (125.902)\tLoss 1.11349 (1.92695)\tAcc@1  69.14 ( 51.00)\tAcc@5  91.80 ( 76.85)\tLearningRate 0.03532 (0.06860)\n",
      "Epoch: [20][199/365]\tBatch 108.094 (125.693)\tData 107.733 (125.331)\tLoss 1.22619 (1.91579)\tAcc@1  68.36 ( 51.26)\tAcc@5  89.84 ( 77.05)\tLearningRate 0.03361 (0.06815)\n",
      "Epoch: [20][299/365]\tBatch 156.531 (125.777)\tData 156.193 (125.415)\tLoss 1.03580 (1.90481)\tAcc@1  71.48 ( 51.51)\tAcc@5  91.80 ( 77.24)\tLearningRate 0.03193 (0.06768)\n",
      "0:03:06.618025 elapsed for 21\n",
      "Epoch: [21][ 99/365]\tBatch 58.271 (124.973)\tData 57.879 (124.611)\tLoss 1.12001 (1.88677)\tAcc@1  69.92 ( 51.93)\tAcc@5  90.62 ( 77.55)\tLearningRate 0.02921 (0.06689)\n",
      "Epoch: [21][199/365]\tBatch 110.011 (124.465)\tData 109.672 (124.103)\tLoss 0.98827 (1.87617)\tAcc@1  72.27 ( 52.17)\tAcc@5  94.53 ( 77.73)\tLearningRate 0.02759 (0.06640)\n",
      "Epoch: [21][299/365]\tBatch 157.861 (124.572)\tData 157.523 (124.210)\tLoss 0.92090 (1.86565)\tAcc@1  73.05 ( 52.41)\tAcc@5  93.36 ( 77.91)\tLearningRate 0.02600 (0.06591)\n",
      "0:03:06.685233 elapsed for 22\n",
      "Epoch: [22][ 99/365]\tBatch 60.161 (123.865)\tData 59.797 (123.503)\tLoss 1.01781 (1.84835)\tAcc@1  72.27 ( 52.82)\tAcc@5  92.58 ( 78.20)\tLearningRate 0.02345 (0.06507)\n",
      "Epoch: [22][199/365]\tBatch 107.885 (123.384)\tData 107.441 (123.023)\tLoss 0.94752 (1.83771)\tAcc@1  72.66 ( 53.06)\tAcc@5  93.36 ( 78.38)\tLearningRate 0.02195 (0.06456)\n",
      "Epoch: [22][299/365]\tBatch 155.824 (123.490)\tData 155.485 (123.129)\tLoss 1.09960 (1.82739)\tAcc@1  69.92 ( 53.31)\tAcc@5  91.80 ( 78.55)\tLearningRate 0.02048 (0.06404)\n",
      "0:03:06.184802 elapsed for 23\n",
      "Epoch: [23][ 99/365]\tBatch 59.908 (122.815)\tData 59.540 (122.453)\tLoss 0.97582 (1.81044)\tAcc@1  73.83 ( 53.70)\tAcc@5  90.62 ( 78.83)\tLearningRate 0.01815 (0.06317)\n",
      "Epoch: [23][199/365]\tBatch 108.220 (122.367)\tData 107.880 (122.006)\tLoss 0.93233 (1.79998)\tAcc@1  74.22 ( 53.95)\tAcc@5  92.19 ( 79.00)\tLearningRate 0.01678 (0.06263)\n",
      "Epoch: [23][299/365]\tBatch 158.756 (122.504)\tData 158.384 (122.142)\tLoss 0.80478 (1.78990)\tAcc@1  76.56 ( 54.18)\tAcc@5  96.09 ( 79.16)\tLearningRate 0.01547 (0.06210)\n",
      "0:03:07.234944 elapsed for 24\n",
      "Epoch: [24][ 99/365]\tBatch 57.601 (121.877)\tData 57.213 (121.515)\tLoss 0.84943 (1.77288)\tAcc@1  77.73 ( 54.58)\tAcc@5  94.14 ( 79.43)\tLearningRate 0.01339 (0.06121)\n",
      "Epoch: [24][199/365]\tBatch 108.221 (121.447)\tData 107.866 (121.085)\tLoss 0.82741 (1.76260)\tAcc@1  78.12 ( 54.83)\tAcc@5  95.31 ( 79.59)\tLearningRate 0.01219 (0.06067)\n",
      "Epoch: [24][299/365]\tBatch 157.449 (121.574)\tData 157.101 (121.212)\tLoss 0.93612 (1.75256)\tAcc@1  74.61 ( 55.06)\tAcc@5  92.97 ( 79.75)\tLearningRate 0.01104 (0.06013)\n",
      "0:03:06.841657 elapsed for 25\n",
      "Epoch: [25][ 99/365]\tBatch 59.096 (121.003)\tData 58.715 (120.641)\tLoss 0.64024 (1.73586)\tAcc@1  80.08 ( 55.46)\tAcc@5  96.48 ( 80.01)\tLearningRate 0.00926 (0.05924)\n",
      "Epoch: [25][199/365]\tBatch 106.963 (120.595)\tData 106.615 (120.233)\tLoss 0.79676 (1.72570)\tAcc@1  76.17 ( 55.71)\tAcc@5  94.53 ( 80.16)\tLearningRate 0.00824 (0.05869)\n",
      "Epoch: [25][299/365]\tBatch 154.248 (120.707)\tData 153.903 (120.345)\tLoss 0.85534 (1.71565)\tAcc@1  75.00 ( 55.95)\tAcc@5  92.58 ( 80.32)\tLearningRate 0.00728 (0.05815)\n",
      "0:03:03.610187 elapsed for 26\n",
      "Epoch: [26][ 99/365]\tBatch 60.104 (120.131)\tData 59.682 (119.769)\tLoss 0.65676 (1.69872)\tAcc@1  80.08 ( 56.36)\tAcc@5  94.14 ( 80.57)\tLearningRate 0.00582 (0.05727)\n",
      "Epoch: [26][199/365]\tBatch 107.232 (119.750)\tData 106.893 (119.389)\tLoss 0.79631 (1.68850)\tAcc@1  76.17 ( 56.61)\tAcc@5  95.31 ( 80.72)\tLearningRate 0.00501 (0.05673)\n",
      "Epoch: [26][299/365]\tBatch 156.036 (119.873)\tData 155.643 (119.511)\tLoss 0.74950 (1.67835)\tAcc@1  79.30 ( 56.86)\tAcc@5  96.48 ( 80.87)\tLearningRate 0.00426 (0.05620)\n",
      "0:03:03.560888 elapsed for 27\n",
      "Epoch: [27][ 99/365]\tBatch 58.675 (119.344)\tData 58.322 (118.982)\tLoss 0.55462 (1.66140)\tAcc@1  85.16 ( 57.27)\tAcc@5  95.70 ( 81.12)\tLearningRate 0.00314 (0.05533)\n",
      "Epoch: [27][199/365]\tBatch 107.631 (118.985)\tData 107.259 (118.623)\tLoss 0.72909 (1.65130)\tAcc@1  79.69 ( 57.52)\tAcc@5  94.53 ( 81.26)\tLearningRate 0.00255 (0.05481)\n",
      "Epoch: [27][299/365]\tBatch 158.182 (119.122)\tData 157.743 (118.760)\tLoss 0.70962 (1.64123)\tAcc@1  78.12 ( 57.76)\tAcc@5  95.31 ( 81.41)\tLearningRate 0.00201 (0.05429)\n",
      "0:03:06.837398 elapsed for 28\n",
      "Epoch: [28][ 99/365]\tBatch 58.850 (118.642)\tData 58.500 (118.280)\tLoss 0.58839 (1.62460)\tAcc@1  83.59 ( 58.18)\tAcc@5  94.92 ( 81.64)\tLearningRate 0.00127 (0.05345)\n",
      "Epoch: [28][199/365]\tBatch 108.774 (118.303)\tData 108.382 (117.941)\tLoss 0.54140 (1.61473)\tAcc@1  86.33 ( 58.42)\tAcc@5  96.48 ( 81.78)\tLearningRate 0.00090 (0.05294)\n",
      "Epoch: [28][299/365]\tBatch 156.766 (118.441)\tData 156.338 (118.079)\tLoss 0.64927 (1.60483)\tAcc@1  80.47 ( 58.67)\tAcc@5  96.09 ( 81.92)\tLearningRate 0.00059 (0.05245)\n",
      "0:03:04.849559 elapsed for 29\n",
      "Epoch: [29][ 99/365]\tBatch 61.408 (117.993)\tData 61.008 (117.631)\tLoss 0.58161 (1.58888)\tAcc@1  83.98 ( 59.06)\tAcc@5  95.70 ( 82.14)\tLearningRate 0.00022 (0.05164)\n",
      "Epoch: [29][199/365]\tBatch 107.904 (117.682)\tData 107.500 (117.320)\tLoss 0.51532 (1.57931)\tAcc@1  87.11 ( 59.30)\tAcc@5  96.88 ( 82.27)\tLearningRate 0.00009 (0.05117)\n",
      "Epoch: [29][299/365]\tBatch 155.180 (117.815)\tData 154.824 (117.453)\tLoss 0.44820 (1.56997)\tAcc@1  87.89 ( 59.54)\tAcc@5  97.66 ( 82.40)\tLearningRate 0.00001 (0.05070)\n",
      "0:03:06.835692 elapsed for 30\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (_, inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/resnet50_fp16_sconv_ep030.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            _, images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_results = [classification_val(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7856330699556754"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7881094623241472,\n",
       " 0.7854114472923492,\n",
       " 0.7869531701676623,\n",
       " 0.7803044902678743,\n",
       " 0.7797263441896319,\n",
       " 0.7850260165735209,\n",
       " 0.7908074773559453,\n",
       " 0.7862786664097129,\n",
       " 0.7879167469647331,\n",
       " 0.7857968780111775]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7725380612834843"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.425454679560811e-06"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025348480584762496"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (_, images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result = [val_retrieval(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6885912507226826"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.684428598959337,\n",
       " 0.6872229716708421,\n",
       " 0.693582578531509,\n",
       " 0.6871266139911351,\n",
       " 0.6907882058200039,\n",
       " 0.6866448255925998,\n",
       " 0.6872229716708421,\n",
       " 0.690980921179418,\n",
       " 0.6823087300057814,\n",
       " 0.6956060898053575]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6178358065137791"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0038908581994208"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6151474272499519,\n",
       " 0.6104259009443053,\n",
       " 0.615436500289073,\n",
       " 0.6237232607438813,\n",
       " 0.6200616689150126,\n",
       " 0.6146656388514165,\n",
       " 0.6214106764309115,\n",
       " 0.6160146463673155,\n",
       " 0.6200616689150126,\n",
       " 0.6214106764309115]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
