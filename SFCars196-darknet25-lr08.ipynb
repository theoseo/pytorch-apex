{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        #self.se = SEBlock(self.inplanes, ratio=9)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        #output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(2, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(2, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(2, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)           \n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20449700"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n",
    "        #[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        #transform.append(transforms.RandomRotation(30))\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "        transform.append(transforms.ColorJitter(hue=.05, saturation=.05),)\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "class CarsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \n",
    "        self.pd_csv = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pd_csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "    \n",
    "        img_name = os.path.join(self.root_dir, \n",
    "                                self.pd_csv.iloc[index, 1])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        target = self.pd_csv.iloc[index, 0]\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CarsDataset('./data/sfcars/train.csv', './data/sfcars/train/',\n",
    "                                 transform=get_transform(random_crop=True))\n",
    "test_dataset = CarsDataset('./data/sfcars/test.csv', './data/sfcars/test/',\n",
    "                                 transform=get_transform(random_crop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "tr_loader = data.DataLoader(dataset=train_dataset,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=test_dataset,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,                            \n",
    "                             num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.8, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=100, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][31/32]\tBatch 28.875 (21.408)\tData 28.631 (21.075)\tLoss 6.19981 (5.67700)\tAcc@1   0.00 (  0.63)\tAcc@5   2.88 (  2.84)\tLearningRate 0.03674 (0.03366)\n",
      "0:00:29.042181 elapsed for 1\n",
      "Epoch: [1][31/32]\tBatch 29.320 (21.515)\tData 29.076 (21.182)\tLoss 5.36153 (5.59030)\tAcc@1   1.44 (  0.61)\tAcc@5   2.40 (  2.88)\tLearningRate 0.05085 (0.03845)\n",
      "0:00:29.470504 elapsed for 2\n",
      "Epoch: [2][31/32]\tBatch 28.925 (21.532)\tData 28.680 (21.200)\tLoss 5.31257 (5.50571)\tAcc@1   0.48 (  0.74)\tAcc@5   1.44 (  3.38)\tLearningRate 0.07398 (0.04632)\n",
      "0:00:29.072081 elapsed for 3\n",
      "Epoch: [3][31/32]\tBatch 28.983 (21.563)\tData 28.740 (21.230)\tLoss 5.18537 (5.43793)\tAcc@1   0.96 (  0.85)\tAcc@5   6.25 (  3.80)\tLearningRate 0.10556 (0.05714)\n",
      "0:00:29.121567 elapsed for 4\n",
      "Epoch: [4][31/32]\tBatch 29.134 (21.543)\tData 28.889 (21.209)\tLoss 5.10694 (5.38675)\tAcc@1   2.40 (  0.97)\tAcc@5  11.54 (  4.35)\tLearningRate 0.14480 (0.07075)\n",
      "0:00:29.282533 elapsed for 5\n",
      "Epoch: [5][31/32]\tBatch 28.539 (21.527)\tData 28.295 (21.193)\tLoss 5.17493 (5.34738)\tAcc@1   0.96 (  1.07)\tAcc@5   5.77 (  4.69)\tLearningRate 0.19075 (0.08695)\n",
      "0:00:28.665828 elapsed for 6\n",
      "Epoch: [6][31/32]\tBatch 28.322 (21.422)\tData 28.078 (21.089)\tLoss 5.08848 (5.31350)\tAcc@1   0.48 (  1.18)\tAcc@5   7.21 (  5.15)\tLearningRate 0.24226 (0.10552)\n",
      "0:00:28.454873 elapsed for 7\n",
      "Epoch: [7][31/32]\tBatch 28.759 (21.482)\tData 28.516 (21.148)\tLoss 5.12571 (5.28496)\tAcc@1   0.48 (  1.30)\tAcc@5   5.77 (  5.51)\tLearningRate 0.29806 (0.12617)\n",
      "0:00:28.896016 elapsed for 8\n",
      "Epoch: [8][31/32]\tBatch 28.539 (21.412)\tData 28.294 (21.080)\tLoss 5.11016 (5.25823)\tAcc@1   3.37 (  1.38)\tAcc@5   9.62 (  5.88)\tLearningRate 0.35677 (0.14861)\n",
      "0:00:28.671661 elapsed for 9\n",
      "Epoch: [9][31/32]\tBatch 28.103 (21.341)\tData 27.859 (21.009)\tLoss 5.10454 (5.23468)\tAcc@1   2.40 (  1.50)\tAcc@5   8.17 (  6.22)\tLearningRate 0.41694 (0.17252)\n",
      "0:00:28.233535 elapsed for 10\n",
      "Epoch: [10][31/32]\tBatch 28.287 (21.277)\tData 28.043 (20.945)\tLoss 4.89591 (5.21536)\tAcc@1   7.21 (  1.60)\tAcc@5  17.31 (  6.55)\tLearningRate 0.47710 (0.19757)\n",
      "0:00:28.415124 elapsed for 11\n",
      "Epoch: [11][31/32]\tBatch 28.079 (21.265)\tData 27.836 (20.933)\tLoss 4.97551 (5.19484)\tAcc@1   2.40 (  1.73)\tAcc@5   9.13 (  6.89)\tLearningRate 0.53574 (0.22340)\n",
      "0:00:28.206181 elapsed for 12\n",
      "Epoch: [12][31/32]\tBatch 28.586 (21.240)\tData 28.341 (20.907)\tLoss 4.91718 (5.17588)\tAcc@1   3.37 (  1.81)\tAcc@5  10.58 (  7.21)\tLearningRate 0.59142 (0.24966)\n",
      "0:00:28.709990 elapsed for 13\n",
      "Epoch: [13][31/32]\tBatch 29.637 (21.369)\tData 29.393 (21.037)\tLoss 4.89156 (5.15711)\tAcc@1   2.40 (  1.92)\tAcc@5  13.46 (  7.53)\tLearningRate 0.64278 (0.27599)\n",
      "0:00:29.775508 elapsed for 14\n",
      "Epoch: [14][31/32]\tBatch 28.569 (21.419)\tData 28.326 (21.087)\tLoss 4.85849 (5.13842)\tAcc@1   2.88 (  2.02)\tAcc@5  10.58 (  7.86)\tLearningRate 0.68853 (0.30205)\n",
      "0:00:28.694596 elapsed for 15\n",
      "Epoch: [15][31/32]\tBatch 28.314 (21.357)\tData 28.071 (21.025)\tLoss 4.76563 (5.12156)\tAcc@1   2.88 (  2.12)\tAcc@5  16.83 (  8.18)\tLearningRate 0.72755 (0.32750)\n",
      "0:00:28.447051 elapsed for 16\n",
      "Epoch: [16][31/32]\tBatch 27.938 (21.322)\tData 27.693 (20.989)\tLoss 4.89814 (5.10420)\tAcc@1   6.73 (  2.25)\tAcc@5  17.79 (  8.54)\tLearningRate 0.75887 (0.35202)\n",
      "0:00:28.068475 elapsed for 17\n",
      "Epoch: [17][31/32]\tBatch 29.293 (21.378)\tData 29.050 (21.046)\tLoss 4.84838 (5.08683)\tAcc@1   6.25 (  2.36)\tAcc@5  14.42 (  8.85)\tLearningRate 0.78173 (0.37532)\n",
      "0:00:29.414353 elapsed for 18\n",
      "Epoch: [18][31/32]\tBatch 28.314 (21.335)\tData 28.070 (21.003)\tLoss 4.75969 (5.06999)\tAcc@1   3.37 (  2.48)\tAcc@5  11.06 (  9.16)\tLearningRate 0.79555 (0.39712)\n",
      "0:00:28.442242 elapsed for 19\n",
      "Epoch: [19][31/32]\tBatch 28.740 (21.332)\tData 28.497 (20.999)\tLoss 4.65383 (5.05234)\tAcc@1   6.73 (  2.60)\tAcc@5  18.75 (  9.52)\tLearningRate 0.80000 (0.41720)\n",
      "0:00:28.869630 elapsed for 20\n",
      "Epoch: [20][31/32]\tBatch 28.225 (21.321)\tData 27.980 (20.989)\tLoss 4.85928 (5.03502)\tAcc@1   4.33 (  2.72)\tAcc@5  12.98 (  9.89)\tLearningRate 0.79967 (0.43542)\n",
      "0:00:28.354098 elapsed for 21\n",
      "Epoch: [21][31/32]\tBatch 28.429 (21.284)\tData 28.184 (20.950)\tLoss 4.71644 (5.01721)\tAcc@1   4.33 (  2.84)\tAcc@5  11.06 ( 10.27)\tLearningRate 0.79873 (0.45196)\n",
      "0:00:28.557442 elapsed for 22\n",
      "Epoch: [22][31/32]\tBatch 27.822 (21.264)\tData 27.578 (20.931)\tLoss 4.63006 (4.99935)\tAcc@1   3.37 (  2.98)\tAcc@5  17.79 ( 10.67)\tLearningRate 0.79717 (0.46700)\n",
      "0:00:27.948286 elapsed for 23\n",
      "Epoch: [23][31/32]\tBatch 29.108 (21.275)\tData 28.865 (20.942)\tLoss 4.48078 (4.98025)\tAcc@1   7.69 (  3.13)\tAcc@5  24.04 ( 11.09)\tLearningRate 0.79500 (0.48072)\n",
      "0:00:29.241199 elapsed for 24\n",
      "Epoch: [24][31/32]\tBatch 27.577 (21.220)\tData 27.334 (20.887)\tLoss 4.47183 (4.96127)\tAcc@1   9.13 (  3.29)\tAcc@5  19.23 ( 11.50)\tLearningRate 0.79222 (0.49323)\n",
      "0:00:27.705621 elapsed for 25\n",
      "Epoch: [25][31/32]\tBatch 27.534 (21.179)\tData 27.289 (20.847)\tLoss 4.55023 (4.94139)\tAcc@1   4.33 (  3.46)\tAcc@5  16.83 ( 11.92)\tLearningRate 0.78883 (0.50467)\n",
      "0:00:27.666502 elapsed for 26\n",
      "Epoch: [26][31/32]\tBatch 28.239 (21.153)\tData 27.996 (20.820)\tLoss 4.40742 (4.92129)\tAcc@1   6.73 (  3.63)\tAcc@5  21.15 ( 12.35)\tLearningRate 0.78485 (0.51512)\n",
      "0:00:28.372996 elapsed for 27\n",
      "Epoch: [27][31/32]\tBatch 28.501 (21.133)\tData 28.257 (20.800)\tLoss 4.34198 (4.90051)\tAcc@1  10.10 (  3.82)\tAcc@5  27.88 ( 12.82)\tLearningRate 0.78027 (0.52467)\n",
      "0:00:28.630308 elapsed for 28\n",
      "Epoch: [28][31/32]\tBatch 28.600 (21.115)\tData 28.356 (20.782)\tLoss 4.25844 (4.87938)\tAcc@1  11.54 (  4.00)\tAcc@5  28.85 ( 13.28)\tLearningRate 0.77511 (0.53339)\n",
      "0:00:28.732157 elapsed for 29\n",
      "Epoch: [29][31/32]\tBatch 29.261 (21.144)\tData 29.015 (20.812)\tLoss 4.30680 (4.85765)\tAcc@1  10.10 (  4.22)\tAcc@5  24.52 ( 13.78)\tLearningRate 0.76936 (0.54135)\n",
      "0:00:29.400226 elapsed for 30\n",
      "Epoch: [30][31/32]\tBatch 28.310 (21.136)\tData 28.066 (20.804)\tLoss 4.06115 (4.83514)\tAcc@1  15.38 (  4.43)\tAcc@5  34.62 ( 14.30)\tLearningRate 0.76305 (0.54860)\n",
      "0:00:28.451509 elapsed for 31\n",
      "Epoch: [31][31/32]\tBatch 28.215 (21.125)\tData 27.971 (20.793)\tLoss 3.97092 (4.81332)\tAcc@1  12.02 (  4.64)\tAcc@5  33.65 ( 14.79)\tLearningRate 0.75618 (0.55520)\n",
      "0:00:28.336985 elapsed for 32\n",
      "Epoch: [32][31/32]\tBatch 28.961 (21.118)\tData 28.716 (20.785)\tLoss 4.04531 (4.78868)\tAcc@1  13.46 (  4.88)\tAcc@5  31.25 ( 15.35)\tLearningRate 0.74876 (0.56117)\n",
      "0:00:29.083565 elapsed for 33\n",
      "Epoch: [33][31/32]\tBatch 28.330 (21.110)\tData 28.087 (20.777)\tLoss 4.04539 (4.76200)\tAcc@1  12.02 (  5.15)\tAcc@5  29.33 ( 15.97)\tLearningRate 0.74080 (0.56657)\n",
      "0:00:28.455877 elapsed for 34\n",
      "Epoch: [34][31/32]\tBatch 29.054 (21.123)\tData 28.810 (20.790)\tLoss 3.74793 (4.73450)\tAcc@1  14.90 (  5.43)\tAcc@5  41.35 ( 16.62)\tLearningRate 0.73232 (0.57142)\n",
      "0:00:29.177366 elapsed for 35\n",
      "Epoch: [35][31/32]\tBatch 27.740 (21.099)\tData 27.494 (20.767)\tLoss 3.70288 (4.70682)\tAcc@1  20.19 (  5.74)\tAcc@5  43.27 ( 17.28)\tLearningRate 0.72332 (0.57577)\n",
      "0:00:27.871427 elapsed for 36\n",
      "Epoch: [36][31/32]\tBatch 28.491 (21.112)\tData 28.248 (20.780)\tLoss 4.02782 (4.67725)\tAcc@1  10.10 (  6.08)\tAcc@5  34.13 ( 18.00)\tLearningRate 0.71382 (0.57962)\n",
      "0:00:28.625984 elapsed for 37\n",
      "Epoch: [37][31/32]\tBatch 27.844 (21.097)\tData 27.599 (20.764)\tLoss 3.52460 (4.64581)\tAcc@1  22.60 (  6.47)\tAcc@5  46.63 ( 18.77)\tLearningRate 0.70384 (0.58302)\n",
      "0:00:27.981493 elapsed for 38\n",
      "Epoch: [38][31/32]\tBatch 28.548 (21.099)\tData 28.304 (20.766)\tLoss 3.33208 (4.61393)\tAcc@1  25.00 (  6.87)\tAcc@5  48.56 ( 19.52)\tLearningRate 0.69340 (0.58598)\n",
      "0:00:28.679910 elapsed for 39\n",
      "Epoch: [39][31/32]\tBatch 28.540 (21.096)\tData 28.296 (20.763)\tLoss 3.36994 (4.57955)\tAcc@1  24.04 (  7.32)\tAcc@5  51.44 ( 20.36)\tLearningRate 0.68250 (0.58853)\n",
      "0:00:28.664503 elapsed for 40\n",
      "Epoch: [40][31/32]\tBatch 29.142 (21.121)\tData 28.899 (20.788)\tLoss 3.00779 (4.54579)\tAcc@1  24.04 (  7.76)\tAcc@5  62.98 ( 21.16)\tLearningRate 0.67116 (0.59068)\n",
      "0:00:29.270400 elapsed for 41\n",
      "Epoch: [41][31/32]\tBatch 27.990 (21.100)\tData 27.746 (20.768)\tLoss 3.28247 (4.51392)\tAcc@1  22.60 (  8.18)\tAcc@5  53.85 ( 21.92)\tLearningRate 0.65941 (0.59245)\n",
      "0:00:28.117280 elapsed for 42\n",
      "Epoch: [42][31/32]\tBatch 28.120 (21.084)\tData 27.876 (20.751)\tLoss 2.90180 (4.47902)\tAcc@1  28.85 (  8.65)\tAcc@5  60.10 ( 22.74)\tLearningRate 0.64725 (0.59386)\n",
      "0:00:28.246433 elapsed for 43\n",
      "Epoch: [43][31/32]\tBatch 27.665 (21.073)\tData 27.422 (20.740)\tLoss 2.87755 (4.44251)\tAcc@1  30.29 (  9.16)\tAcc@5  62.02 ( 23.59)\tLearningRate 0.63472 (0.59493)\n",
      "0:00:27.792213 elapsed for 44\n",
      "Epoch: [44][31/32]\tBatch 27.947 (21.062)\tData 27.701 (20.729)\tLoss 2.72343 (4.40539)\tAcc@1  35.10 (  9.70)\tAcc@5  60.10 ( 24.45)\tLearningRate 0.62182 (0.59567)\n",
      "0:00:28.093379 elapsed for 45\n",
      "Epoch: [45][31/32]\tBatch 28.805 (21.057)\tData 28.560 (20.724)\tLoss 2.65675 (4.36778)\tAcc@1  34.62 ( 10.25)\tAcc@5  64.90 ( 25.31)\tLearningRate 0.60858 (0.59609)\n",
      "0:00:28.943028 elapsed for 46\n",
      "Epoch: [46][31/32]\tBatch 30.156 (21.088)\tData 29.911 (20.755)\tLoss 2.50982 (4.33003)\tAcc@1  38.46 ( 10.82)\tAcc@5  66.83 ( 26.18)\tLearningRate 0.59502 (0.59620)\n",
      "0:00:30.286445 elapsed for 47\n",
      "Epoch: [47][31/32]\tBatch 29.858 (21.115)\tData 29.613 (20.781)\tLoss 2.64040 (4.29170)\tAcc@1  38.94 ( 11.41)\tAcc@5  63.46 ( 27.05)\tLearningRate 0.58116 (0.59603)\n",
      "0:00:29.990425 elapsed for 48\n",
      "Epoch: [48][31/32]\tBatch 29.625 (21.138)\tData 29.380 (20.804)\tLoss 2.54726 (4.25423)\tAcc@1  37.98 ( 11.99)\tAcc@5  67.31 ( 27.91)\tLearningRate 0.56702 (0.59558)\n",
      "0:00:29.768929 elapsed for 49\n",
      "Epoch: [49][31/32]\tBatch 29.531 (21.154)\tData 29.286 (20.821)\tLoss 2.25633 (4.21592)\tAcc@1  44.23 ( 12.60)\tAcc@5  70.19 ( 28.77)\tLearningRate 0.55262 (0.59486)\n",
      "0:00:29.674838 elapsed for 50\n",
      "Epoch: [50][31/32]\tBatch 29.295 (21.156)\tData 29.051 (20.823)\tLoss 2.56135 (4.17797)\tAcc@1  38.46 ( 13.21)\tAcc@5  66.83 ( 29.62)\tLearningRate 0.53799 (0.59388)\n",
      "0:00:29.419756 elapsed for 51\n",
      "Epoch: [51][31/32]\tBatch 28.431 (21.155)\tData 28.186 (20.821)\tLoss 2.45095 (4.14074)\tAcc@1  37.98 ( 13.80)\tAcc@5  68.27 ( 30.44)\tLearningRate 0.52314 (0.59266)\n",
      "0:00:28.558858 elapsed for 52\n",
      "Epoch: [52][31/32]\tBatch 29.774 (21.163)\tData 29.529 (20.829)\tLoss 2.37855 (4.10424)\tAcc@1  42.31 ( 14.40)\tAcc@5  68.75 ( 31.25)\tLearningRate 0.50810 (0.59120)\n",
      "0:00:29.920846 elapsed for 53\n",
      "Epoch: [53][31/32]\tBatch 29.135 (21.168)\tData 28.890 (20.835)\tLoss 2.23054 (4.06820)\tAcc@1  46.15 ( 15.00)\tAcc@5  70.67 ( 32.03)\tLearningRate 0.49290 (0.58952)\n",
      "0:00:29.263115 elapsed for 54\n",
      "Epoch: [54][31/32]\tBatch 29.313 (21.174)\tData 29.069 (20.840)\tLoss 2.16698 (4.03191)\tAcc@1  45.19 ( 15.61)\tAcc@5  74.52 ( 32.82)\tLearningRate 0.47756 (0.58762)\n",
      "0:00:29.451466 elapsed for 55\n",
      "Epoch: [55][31/32]\tBatch 28.401 (21.168)\tData 28.157 (20.834)\tLoss 2.13362 (3.99559)\tAcc@1  47.12 ( 16.22)\tAcc@5  71.63 ( 33.60)\tLearningRate 0.46209 (0.58551)\n",
      "0:00:28.538728 elapsed for 56\n",
      "Epoch: [56][31/32]\tBatch 28.601 (21.165)\tData 28.355 (20.831)\tLoss 2.03431 (3.95929)\tAcc@1  50.00 ( 16.85)\tAcc@5  75.96 ( 34.37)\tLearningRate 0.44653 (0.58321)\n",
      "0:00:28.742088 elapsed for 57\n",
      "Epoch: [57][31/32]\tBatch 28.723 (21.175)\tData 28.479 (20.841)\tLoss 1.70821 (3.92326)\tAcc@1  55.29 ( 17.48)\tAcc@5  83.17 ( 35.14)\tLearningRate 0.43090 (0.58071)\n",
      "0:00:28.866009 elapsed for 58\n",
      "Epoch: [58][31/32]\tBatch 28.829 (21.170)\tData 28.585 (20.835)\tLoss 2.20050 (3.88741)\tAcc@1  51.44 ( 18.11)\tAcc@5  73.56 ( 35.90)\tLearningRate 0.41521 (0.57803)\n",
      "0:00:28.974943 elapsed for 59\n",
      "Epoch: [59][31/32]\tBatch 29.166 (21.186)\tData 28.922 (20.852)\tLoss 1.72137 (3.85250)\tAcc@1  56.25 ( 18.73)\tAcc@5  81.25 ( 36.64)\tLearningRate 0.39951 (0.57519)\n",
      "0:00:29.298536 elapsed for 60\n",
      "Epoch: [60][31/32]\tBatch 28.197 (21.182)\tData 27.953 (20.847)\tLoss 1.77593 (3.81752)\tAcc@1  53.85 ( 19.35)\tAcc@5  83.17 ( 37.37)\tLearningRate 0.38381 (0.57217)\n",
      "0:00:28.323451 elapsed for 61\n",
      "Epoch: [61][31/32]\tBatch 28.130 (21.169)\tData 27.887 (20.835)\tLoss 1.70715 (3.78340)\tAcc@1  55.77 ( 19.96)\tAcc@5  82.69 ( 38.08)\tLearningRate 0.36813 (0.56900)\n",
      "0:00:28.264789 elapsed for 62\n",
      "Epoch: [62][31/32]\tBatch 27.923 (21.154)\tData 27.677 (20.820)\tLoss 1.72272 (3.74999)\tAcc@1  56.25 ( 20.56)\tAcc@5  82.21 ( 38.77)\tLearningRate 0.35250 (0.56569)\n",
      "0:00:28.056954 elapsed for 63\n",
      "Epoch: [63][31/32]\tBatch 28.718 (21.160)\tData 28.474 (20.826)\tLoss 1.67327 (3.71657)\tAcc@1  61.54 ( 21.17)\tAcc@5  84.62 ( 39.47)\tLearningRate 0.33694 (0.56223)\n",
      "0:00:28.836064 elapsed for 64\n",
      "Epoch: [64][31/32]\tBatch 28.219 (21.155)\tData 27.974 (20.821)\tLoss 1.61140 (3.68322)\tAcc@1  60.10 ( 21.78)\tAcc@5  83.65 ( 40.15)\tLearningRate 0.32148 (0.55864)\n",
      "0:00:28.344627 elapsed for 65\n",
      "Epoch: [65][31/32]\tBatch 28.549 (21.154)\tData 28.305 (20.820)\tLoss 1.40857 (3.65057)\tAcc@1  64.90 ( 22.38)\tAcc@5  85.58 ( 40.82)\tLearningRate 0.30615 (0.55493)\n",
      "0:00:28.672051 elapsed for 66\n",
      "Epoch: [66][31/32]\tBatch 28.194 (21.142)\tData 27.949 (20.808)\tLoss 1.53024 (3.61780)\tAcc@1  62.98 ( 23.00)\tAcc@5  84.13 ( 41.48)\tLearningRate 0.29095 (0.55110)\n",
      "0:00:28.320948 elapsed for 67\n",
      "Epoch: [67][31/32]\tBatch 28.953 (21.144)\tData 28.708 (20.810)\tLoss 1.39423 (3.58553)\tAcc@1  66.83 ( 23.60)\tAcc@5  85.58 ( 42.12)\tLearningRate 0.27593 (0.54716)\n",
      "0:00:29.087067 elapsed for 68\n",
      "Epoch: [68][31/32]\tBatch 28.902 (21.158)\tData 28.657 (20.824)\tLoss 1.50562 (3.55404)\tAcc@1  59.13 ( 24.19)\tAcc@5  83.17 ( 42.75)\tLearningRate 0.26109 (0.54312)\n",
      "0:00:29.044068 elapsed for 69\n",
      "Epoch: [69][31/32]\tBatch 28.710 (21.162)\tData 28.465 (20.828)\tLoss 1.38982 (3.52225)\tAcc@1  68.75 ( 24.79)\tAcc@5  84.13 ( 43.39)\tLearningRate 0.24648 (0.53898)\n",
      "0:00:28.837354 elapsed for 70\n",
      "Epoch: [70][31/32]\tBatch 26.991 (21.143)\tData 26.747 (20.809)\tLoss 1.56186 (3.49104)\tAcc@1  58.65 ( 25.39)\tAcc@5  84.13 ( 44.00)\tLearningRate 0.23209 (0.53476)\n",
      "0:00:27.121019 elapsed for 71\n",
      "Epoch: [71][31/32]\tBatch 28.262 (21.134)\tData 28.018 (20.800)\tLoss 1.50431 (3.45992)\tAcc@1  60.58 ( 25.99)\tAcc@5  81.73 ( 44.60)\tLearningRate 0.21797 (0.53045)\n",
      "0:00:28.396281 elapsed for 72\n",
      "Epoch: [72][31/32]\tBatch 28.048 (21.131)\tData 27.804 (20.797)\tLoss 1.28881 (3.42940)\tAcc@1  67.79 ( 26.58)\tAcc@5  89.42 ( 45.20)\tLearningRate 0.20413 (0.52607)\n",
      "0:00:28.175734 elapsed for 73\n",
      "Epoch: [73][31/32]\tBatch 28.070 (21.119)\tData 27.826 (20.786)\tLoss 1.27885 (3.39919)\tAcc@1  67.31 ( 27.16)\tAcc@5  88.94 ( 45.78)\tLearningRate 0.19058 (0.52163)\n",
      "0:00:28.197817 elapsed for 74\n",
      "Epoch: [74][31/32]\tBatch 29.359 (21.132)\tData 29.115 (20.798)\tLoss 1.14928 (3.36943)\tAcc@1  73.08 ( 27.75)\tAcc@5  92.31 ( 46.36)\tLearningRate 0.17737 (0.51712)\n",
      "0:00:29.484358 elapsed for 75\n",
      "Epoch: [75][31/32]\tBatch 29.328 (21.150)\tData 29.085 (20.816)\tLoss 1.07569 (3.33972)\tAcc@1  73.08 ( 28.33)\tAcc@5  87.02 ( 46.92)\tLearningRate 0.16449 (0.51256)\n",
      "0:00:29.453012 elapsed for 76\n",
      "Epoch: [76][31/32]\tBatch 28.372 (21.150)\tData 28.127 (20.816)\tLoss 0.99255 (3.31034)\tAcc@1  76.44 ( 28.91)\tAcc@5  92.79 ( 47.47)\tLearningRate 0.15198 (0.50796)\n",
      "0:00:28.518098 elapsed for 77\n",
      "Epoch: [77][31/32]\tBatch 28.679 (21.152)\tData 28.435 (20.818)\tLoss 1.00402 (3.28119)\tAcc@1  74.04 ( 29.48)\tAcc@5  88.46 ( 48.02)\tLearningRate 0.13985 (0.50331)\n",
      "0:00:28.801913 elapsed for 78\n",
      "Epoch: [78][31/32]\tBatch 29.074 (21.158)\tData 28.830 (20.825)\tLoss 0.98946 (3.25266)\tAcc@1  74.52 ( 30.05)\tAcc@5  90.87 ( 48.56)\tLearningRate 0.12812 (0.49864)\n",
      "0:00:29.197641 elapsed for 79\n",
      "Epoch: [79][31/32]\tBatch 28.370 (21.154)\tData 28.125 (20.820)\tLoss 1.03587 (3.22406)\tAcc@1  76.44 ( 30.63)\tAcc@5  88.94 ( 49.09)\tLearningRate 0.11681 (0.49393)\n",
      "0:00:28.503296 elapsed for 80\n",
      "Epoch: [80][31/32]\tBatch 28.297 (21.148)\tData 28.054 (20.814)\tLoss 0.88668 (3.19540)\tAcc@1  78.37 ( 31.21)\tAcc@5  92.31 ( 49.61)\tLearningRate 0.10594 (0.48921)\n",
      "0:00:28.430615 elapsed for 81\n",
      "Epoch: [81][31/32]\tBatch 28.619 (21.147)\tData 28.374 (20.813)\tLoss 0.93919 (3.16715)\tAcc@1  75.96 ( 31.79)\tAcc@5  89.90 ( 50.13)\tLearningRate 0.09552 (0.48447)\n",
      "0:00:28.747399 elapsed for 82\n",
      "Epoch: [82][31/32]\tBatch 28.621 (21.147)\tData 28.377 (20.813)\tLoss 0.81146 (3.13886)\tAcc@1  80.29 ( 32.37)\tAcc@5  90.87 ( 50.64)\tLearningRate 0.08557 (0.47972)\n",
      "0:00:28.768951 elapsed for 83\n",
      "Epoch: [83][31/32]\tBatch 28.014 (21.149)\tData 27.770 (20.815)\tLoss 0.82339 (3.11105)\tAcc@1  80.29 ( 32.95)\tAcc@5  94.23 ( 51.14)\tLearningRate 0.07611 (0.47497)\n",
      "0:00:28.153195 elapsed for 84\n",
      "Epoch: [84][31/32]\tBatch 28.595 (21.146)\tData 28.350 (20.812)\tLoss 0.72948 (3.08337)\tAcc@1  80.29 ( 33.52)\tAcc@5  94.71 ( 51.63)\tLearningRate 0.06714 (0.47022)\n",
      "0:00:28.718289 elapsed for 85\n",
      "Epoch: [85][31/32]\tBatch 28.114 (21.137)\tData 27.870 (20.803)\tLoss 0.73453 (3.05558)\tAcc@1  81.73 ( 34.10)\tAcc@5  93.27 ( 52.12)\tLearningRate 0.05869 (0.46548)\n",
      "0:00:28.243999 elapsed for 86\n",
      "Epoch: [86][31/32]\tBatch 28.426 (21.135)\tData 28.183 (20.801)\tLoss 0.69925 (3.02814)\tAcc@1  81.73 ( 34.67)\tAcc@5  93.27 ( 52.60)\tLearningRate 0.05077 (0.46076)\n",
      "0:00:28.545244 elapsed for 87\n",
      "Epoch: [87][31/32]\tBatch 28.418 (21.130)\tData 28.175 (20.797)\tLoss 0.72280 (3.00090)\tAcc@1  82.69 ( 35.24)\tAcc@5  91.83 ( 53.07)\tLearningRate 0.04338 (0.45606)\n",
      "0:00:28.542720 elapsed for 88\n",
      "Epoch: [88][31/32]\tBatch 28.776 (21.130)\tData 28.531 (20.797)\tLoss 0.76037 (2.97425)\tAcc@1  85.10 ( 35.80)\tAcc@5  92.31 ( 53.54)\tLearningRate 0.03654 (0.45138)\n",
      "0:00:28.903569 elapsed for 89\n",
      "Epoch: [89][31/32]\tBatch 27.964 (21.120)\tData 27.721 (20.787)\tLoss 0.76211 (2.94774)\tAcc@1  83.17 ( 36.37)\tAcc@5  91.35 ( 54.00)\tLearningRate 0.03026 (0.44673)\n",
      "0:00:28.115477 elapsed for 90\n",
      "Epoch: [90][31/32]\tBatch 28.966 (21.123)\tData 28.723 (20.790)\tLoss 0.55600 (2.92127)\tAcc@1  88.46 ( 36.93)\tAcc@5  94.23 ( 54.45)\tLearningRate 0.02456 (0.44212)\n",
      "0:00:29.086264 elapsed for 91\n",
      "Epoch: [91][31/32]\tBatch 27.903 (21.116)\tData 27.659 (20.783)\tLoss 0.46038 (2.89524)\tAcc@1  90.38 ( 37.49)\tAcc@5  97.12 ( 54.89)\tLearningRate 0.01943 (0.43756)\n",
      "0:00:28.022374 elapsed for 92\n",
      "Epoch: [92][31/32]\tBatch 28.496 (21.115)\tData 28.253 (20.781)\tLoss 0.48747 (2.86960)\tAcc@1  87.98 ( 38.03)\tAcc@5  96.15 ( 55.32)\tLearningRate 0.01489 (0.43303)\n",
      "0:00:28.624997 elapsed for 93\n",
      "Epoch: [93][31/32]\tBatch 27.426 (21.105)\tData 27.181 (20.772)\tLoss 0.54615 (2.84410)\tAcc@1  87.02 ( 38.57)\tAcc@5  95.19 ( 55.75)\tLearningRate 0.01094 (0.42856)\n",
      "0:00:27.548619 elapsed for 94\n",
      "Epoch: [94][31/32]\tBatch 28.510 (21.103)\tData 28.266 (20.769)\tLoss 0.53652 (2.81916)\tAcc@1  88.46 ( 39.11)\tAcc@5  95.19 ( 56.17)\tLearningRate 0.00759 (0.42415)\n",
      "0:00:28.641772 elapsed for 95\n",
      "Epoch: [95][31/32]\tBatch 28.364 (21.103)\tData 28.120 (20.770)\tLoss 0.49533 (2.79454)\tAcc@1  88.94 ( 39.64)\tAcc@5  95.19 ( 56.58)\tLearningRate 0.00485 (0.41979)\n",
      "0:00:28.496980 elapsed for 96\n",
      "Epoch: [96][31/32]\tBatch 28.661 (21.103)\tData 28.418 (20.770)\tLoss 0.31530 (2.77017)\tAcc@1  93.75 ( 40.16)\tAcc@5  97.60 ( 56.99)\tLearningRate 0.00272 (0.41550)\n",
      "0:00:28.795228 elapsed for 97\n",
      "Epoch: [97][31/32]\tBatch 28.195 (21.099)\tData 27.950 (20.766)\tLoss 0.47782 (2.74628)\tAcc@1  87.98 ( 40.67)\tAcc@5  95.19 ( 57.39)\tLearningRate 0.00120 (0.41128)\n",
      "0:00:28.337288 elapsed for 98\n",
      "Epoch: [98][31/32]\tBatch 28.503 (21.098)\tData 28.258 (20.764)\tLoss 0.47553 (2.72295)\tAcc@1  89.42 ( 41.18)\tAcc@5  95.19 ( 57.78)\tLearningRate 0.00029 (0.40714)\n",
      "0:00:28.633927 elapsed for 99\n",
      "Epoch: [99][31/32]\tBatch 28.626 (21.100)\tData 28.382 (20.766)\tLoss 0.43018 (2.70009)\tAcc@1  90.38 ( 41.67)\tAcc@5  94.71 ( 58.16)\tLearningRate 0.00000 (0.40307)\n",
      "0:00:28.756884 elapsed for 100\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    progress.display(i)            \n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8005223230941425"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_val(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7285163536873523"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_retrieval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/sfcar_darknet25_ep100_lr08.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
