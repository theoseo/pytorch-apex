{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        #self.se = SEBlock(self.inplanes, ratio=9)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        #output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(2, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(2, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(2, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((1,3,224,224))\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 150])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20402550"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)\n",
    "\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image_id, sample, target) where target is class_index of\n",
    "                the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        #print(path)\n",
    "        #print(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        image_id = path.split('/')[-1]\n",
    "\n",
    "        return image_id, sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "data_dir = 'train/train_data'\n",
    "\n",
    "dataset = CustomDataset(data_dir, transform=get_transform(random_crop=True))\n",
    "\n",
    "split_size = int(len(dataset) * 0.9)\n",
    "train_set, valid_set = data.random_split(dataset, [split_size, len(dataset) - split_size])\n",
    "tr_loader = data.DataLoader(dataset=train_set,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=valid_set,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,\n",
    "                            pin_memory=True,                             \n",
    "                            num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    _, inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=30, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 99/365]\tBatch 55.866 (31.589)\tData 55.586 (31.303)\tLoss 3.98809 (4.38152)\tAcc@1   9.38 (  5.89)\tAcc@5  30.47 ( 19.79)\tLearningRate 0.00449 (0.00417)\n",
      "Epoch: [0][199/365]\tBatch 104.901 (55.990)\tData 104.627 (55.703)\tLoss 3.97781 (4.12283)\tAcc@1   8.59 (  8.80)\tAcc@5  34.38 ( 26.74)\tLearningRate 0.00596 (0.00466)\n",
      "Epoch: [0][299/365]\tBatch 152.162 (80.176)\tData 151.870 (79.889)\tLoss 3.41647 (3.94403)\tAcc@1  16.80 ( 11.22)\tAcc@5  47.66 ( 31.89)\tLearningRate 0.00838 (0.00548)\n",
      "0:02:59.416583 elapsed for 1\n",
      "Epoch: [1][ 99/365]\tBatch 58.665 (82.298)\tData 58.392 (82.012)\tLoss 3.23517 (3.71496)\tAcc@1  23.44 ( 14.64)\tAcc@5  50.39 ( 38.37)\tLearningRate 0.01430 (0.00750)\n",
      "Epoch: [1][199/365]\tBatch 108.497 (82.544)\tData 108.208 (82.258)\tLoss 2.83072 (3.59588)\tAcc@1  28.91 ( 16.58)\tAcc@5  63.67 ( 41.66)\tLearningRate 0.01893 (0.00910)\n",
      "Epoch: [1][299/365]\tBatch 156.077 (90.115)\tData 155.802 (89.829)\tLoss 2.80030 (3.48593)\tAcc@1  25.00 ( 18.53)\tAcc@5  60.16 ( 44.56)\tLearningRate 0.02425 (0.01098)\n",
      "0:03:03.172799 elapsed for 2\n",
      "Epoch: [2][ 99/365]\tBatch 57.774 (89.628)\tData 57.501 (89.342)\tLoss 2.46577 (3.32380)\tAcc@1  36.72 ( 21.46)\tAcc@5  73.05 ( 48.72)\tLearningRate 0.03421 (0.01459)\n",
      "Epoch: [2][199/365]\tBatch 109.720 (88.943)\tData 109.442 (88.656)\tLoss 2.44063 (3.23789)\tAcc@1  42.58 ( 23.08)\tAcc@5  71.09 ( 50.86)\tLearningRate 0.04077 (0.01705)\n",
      "Epoch: [2][299/365]\tBatch 157.586 (93.232)\tData 157.312 (92.945)\tLoss 2.22090 (3.15870)\tAcc@1  44.14 ( 24.61)\tAcc@5  75.00 ( 52.78)\tLearningRate 0.04756 (0.01969)\n",
      "0:03:05.982160 elapsed for 3\n",
      "Epoch: [3][ 99/365]\tBatch 57.825 (92.518)\tData 57.536 (92.232)\tLoss 2.18694 (3.04035)\tAcc@1  46.48 ( 26.88)\tAcc@5  73.83 ( 55.59)\tLearningRate 0.05890 (0.02433)\n",
      "Epoch: [3][199/365]\tBatch 106.837 (91.774)\tData 106.563 (91.487)\tLoss 2.07196 (2.97681)\tAcc@1  46.48 ( 28.14)\tAcc@5  77.34 ( 57.06)\tLearningRate 0.06562 (0.02726)\n",
      "Epoch: [3][299/365]\tBatch 157.276 (94.710)\tData 156.958 (94.422)\tLoss 2.05877 (2.91601)\tAcc@1  46.09 ( 29.37)\tAcc@5  79.30 ( 58.43)\tLearningRate 0.07207 (0.03024)\n",
      "0:03:06.060861 elapsed for 4\n",
      "Epoch: [4][ 99/365]\tBatch 58.333 (93.996)\tData 58.021 (93.709)\tLoss 1.86347 (2.82387)\tAcc@1  53.12 ( 31.24)\tAcc@5  79.69 ( 60.43)\tLearningRate 0.08173 (0.03519)\n",
      "Epoch: [4][199/365]\tBatch 109.405 (93.337)\tData 109.118 (93.050)\tLoss 1.83371 (2.77146)\tAcc@1  50.78 ( 32.30)\tAcc@5  82.81 ( 61.56)\tLearningRate 0.08682 (0.03816)\n",
      "Epoch: [4][299/365]\tBatch 157.645 (95.623)\tData 157.353 (95.335)\tLoss 2.03232 (2.72323)\tAcc@1  48.83 ( 33.28)\tAcc@5  75.39 ( 62.62)\tLearningRate 0.09119 (0.04105)\n",
      "0:03:03.997071 elapsed for 5\n",
      "Epoch: [5][ 99/365]\tBatch 57.363 (94.940)\tData 57.072 (94.653)\tLoss 1.86401 (2.64780)\tAcc@1  50.39 ( 34.88)\tAcc@5  81.64 ( 64.17)\tLearningRate 0.09660 (0.04560)\n",
      "Epoch: [5][199/365]\tBatch 107.258 (94.337)\tData 106.967 (94.049)\tLoss 1.85181 (2.60567)\tAcc@1  55.47 ( 35.77)\tAcc@5  77.73 ( 65.05)\tLearningRate 0.09868 (0.04817)\n",
      "Epoch: [5][299/365]\tBatch 155.335 (96.047)\tData 155.048 (95.760)\tLoss 1.60612 (2.56633)\tAcc@1  57.03 ( 36.60)\tAcc@5  84.77 ( 65.85)\tLearningRate 0.09980 (0.05058)\n",
      "0:03:01.075686 elapsed for 6\n",
      "Epoch: [6][ 99/365]\tBatch 58.459 (95.383)\tData 58.180 (95.096)\tLoss 1.63921 (2.50266)\tAcc@1  57.42 ( 37.99)\tAcc@5  83.98 ( 67.12)\tLearningRate 0.09997 (0.05414)\n",
      "Epoch: [6][199/365]\tBatch 108.276 (94.873)\tData 107.988 (94.586)\tLoss 1.68164 (2.46785)\tAcc@1  57.81 ( 38.75)\tAcc@5  78.91 ( 67.80)\tLearningRate 0.09987 (0.05605)\n",
      "Epoch: [6][299/365]\tBatch 157.047 (96.404)\tData 156.763 (96.117)\tLoss 1.65301 (2.43434)\tAcc@1  56.64 ( 39.48)\tAcc@5  83.98 ( 68.47)\tLearningRate 0.09971 (0.05781)\n",
      "0:03:02.817232 elapsed for 7\n",
      "Epoch: [7][ 99/365]\tBatch 58.006 (95.851)\tData 57.725 (95.564)\tLoss 1.47025 (2.38184)\tAcc@1  57.42 ( 40.63)\tAcc@5  85.94 ( 69.48)\tLearningRate 0.09930 (0.06040)\n",
      "Epoch: [7][199/365]\tBatch 106.340 (95.343)\tData 106.067 (95.056)\tLoss 1.41650 (2.35137)\tAcc@1  62.50 ( 41.31)\tAcc@5  83.98 ( 70.06)\tLearningRate 0.09897 (0.06181)\n",
      "Epoch: [7][299/365]\tBatch 155.678 (96.629)\tData 155.387 (96.341)\tLoss 1.69440 (2.32307)\tAcc@1  59.38 ( 41.93)\tAcc@5  80.86 ( 70.60)\tLearningRate 0.09858 (0.06310)\n",
      "0:03:02.949228 elapsed for 8\n",
      "Epoch: [8][ 99/365]\tBatch 55.744 (96.097)\tData 55.465 (95.810)\tLoss 1.30919 (2.27741)\tAcc@1  61.72 ( 42.95)\tAcc@5  87.11 ( 71.46)\tLearningRate 0.09780 (0.06502)\n",
      "Epoch: [8][199/365]\tBatch 109.288 (95.666)\tData 108.977 (95.378)\tLoss 1.35637 (2.25212)\tAcc@1  66.80 ( 43.50)\tAcc@5  88.67 ( 71.93)\tLearningRate 0.09724 (0.06606)\n",
      "Epoch: [8][299/365]\tBatch 155.575 (96.825)\tData 155.301 (96.537)\tLoss 1.38390 (2.22810)\tAcc@1  60.94 ( 44.03)\tAcc@5  84.38 ( 72.37)\tLearningRate 0.09662 (0.06702)\n",
      "0:03:03.304793 elapsed for 9\n",
      "Epoch: [9][ 99/365]\tBatch 62.007 (96.389)\tData 61.722 (96.102)\tLoss 1.33095 (2.18936)\tAcc@1  65.23 ( 44.90)\tAcc@5  86.72 ( 73.09)\tLearningRate 0.09547 (0.06844)\n",
      "Epoch: [9][199/365]\tBatch 108.712 (96.056)\tData 108.430 (95.769)\tLoss 1.48782 (2.16752)\tAcc@1  59.77 ( 45.38)\tAcc@5  87.50 ( 73.49)\tLearningRate 0.09470 (0.06920)\n",
      "Epoch: [9][299/365]\tBatch 157.896 (97.061)\tData 157.588 (96.773)\tLoss 1.38882 (2.14679)\tAcc@1  62.89 ( 45.85)\tAcc@5  85.16 ( 73.88)\tLearningRate 0.09386 (0.06990)\n",
      "0:03:05.022162 elapsed for 10\n",
      "Epoch: [10][ 99/365]\tBatch 58.066 (96.665)\tData 57.794 (96.378)\tLoss 1.27099 (2.11386)\tAcc@1  65.23 ( 46.59)\tAcc@5  87.50 ( 74.47)\tLearningRate 0.09237 (0.07092)\n",
      "Epoch: [10][199/365]\tBatch 108.261 (96.319)\tData 107.981 (96.032)\tLoss 1.54967 (2.09471)\tAcc@1  56.25 ( 47.02)\tAcc@5  88.28 ( 74.82)\tLearningRate 0.09139 (0.07147)\n",
      "Epoch: [10][299/365]\tBatch 158.257 (97.266)\tData 157.965 (96.978)\tLoss 1.40070 (2.07651)\tAcc@1  62.11 ( 47.44)\tAcc@5  87.50 ( 75.15)\tLearningRate 0.09036 (0.07196)\n",
      "0:03:05.441141 elapsed for 11\n",
      "Epoch: [11][ 99/365]\tBatch 59.518 (96.906)\tData 59.245 (96.618)\tLoss 1.33607 (2.04732)\tAcc@1  64.45 ( 48.08)\tAcc@5  89.06 ( 75.68)\tLearningRate 0.08854 (0.07266)\n",
      "Epoch: [11][199/365]\tBatch 107.144 (96.578)\tData 106.859 (96.290)\tLoss 1.29085 (2.03031)\tAcc@1  66.41 ( 48.47)\tAcc@5  90.23 ( 75.98)\tLearningRate 0.08737 (0.07302)\n",
      "Epoch: [11][299/365]\tBatch 156.254 (97.412)\tData 155.980 (97.124)\tLoss 1.34320 (2.01428)\tAcc@1  64.06 ( 48.83)\tAcc@5  90.23 ( 76.26)\tLearningRate 0.08616 (0.07334)\n",
      "0:03:04.447359 elapsed for 12\n",
      "Epoch: [12][ 99/365]\tBatch 59.267 (97.052)\tData 58.993 (96.764)\tLoss 1.16566 (1.98870)\tAcc@1  69.92 ( 49.41)\tAcc@5  91.02 ( 76.71)\tLearningRate 0.08405 (0.07377)\n",
      "Epoch: [12][199/365]\tBatch 108.068 (96.755)\tData 107.788 (96.468)\tLoss 1.51905 (1.97387)\tAcc@1  59.38 ( 49.75)\tAcc@5  86.72 ( 76.98)\tLearningRate 0.08272 (0.07398)\n",
      "Epoch: [12][299/365]\tBatch 157.813 (97.540)\tData 157.525 (97.253)\tLoss 1.05728 (1.95946)\tAcc@1  69.92 ( 50.08)\tAcc@5  94.92 ( 77.23)\tLearningRate 0.08134 (0.07416)\n",
      "0:03:05.210985 elapsed for 13\n",
      "Epoch: [13][ 99/365]\tBatch 57.910 (97.209)\tData 57.636 (96.922)\tLoss 1.20458 (1.93627)\tAcc@1  67.97 ( 50.60)\tAcc@5  90.23 ( 77.64)\tLearningRate 0.07898 (0.07436)\n",
      "Epoch: [13][199/365]\tBatch 110.571 (96.943)\tData 110.298 (96.655)\tLoss 1.24610 (1.92266)\tAcc@1  61.33 ( 50.91)\tAcc@5  89.84 ( 77.87)\tLearningRate 0.07750 (0.07444)\n",
      "Epoch: [13][299/365]\tBatch 156.407 (97.669)\tData 156.126 (97.381)\tLoss 1.49905 (1.90994)\tAcc@1  55.47 ( 51.20)\tAcc@5  87.11 ( 78.08)\tLearningRate 0.07599 (0.07448)\n",
      "0:03:05.324971 elapsed for 14\n",
      "Epoch: [14][ 99/365]\tBatch 60.278 (97.387)\tData 60.004 (97.099)\tLoss 1.14037 (1.88880)\tAcc@1  69.14 ( 51.69)\tAcc@5  92.58 ( 78.45)\tLearningRate 0.07342 (0.07449)\n",
      "Epoch: [14][199/365]\tBatch 107.384 (97.133)\tData 107.105 (96.845)\tLoss 1.31121 (1.87640)\tAcc@1  66.02 ( 51.98)\tAcc@5  89.84 ( 78.66)\tLearningRate 0.07182 (0.07446)\n",
      "Epoch: [14][299/365]\tBatch 158.513 (97.794)\tData 158.227 (97.506)\tLoss 1.17066 (1.86447)\tAcc@1  67.58 ( 52.26)\tAcc@5  91.41 ( 78.86)\tLearningRate 0.07019 (0.07439)\n",
      "0:03:04.117598 elapsed for 15\n",
      "Epoch: [15][ 99/365]\tBatch 58.011 (97.505)\tData 57.731 (97.217)\tLoss 1.13753 (1.84479)\tAcc@1  69.14 ( 52.71)\tAcc@5  90.62 ( 79.19)\tLearningRate 0.06745 (0.07423)\n",
      "Epoch: [15][199/365]\tBatch 106.043 (97.229)\tData 105.751 (96.941)\tLoss 1.34357 (1.83390)\tAcc@1  67.97 ( 52.96)\tAcc@5  85.55 ( 79.37)\tLearningRate 0.06576 (0.07409)\n",
      "Epoch: [15][299/365]\tBatch 156.126 (97.815)\tData 155.791 (97.527)\tLoss 1.15330 (1.82297)\tAcc@1  69.53 ( 53.22)\tAcc@5  88.67 ( 79.56)\tLearningRate 0.06405 (0.07393)\n",
      "0:03:03.682793 elapsed for 16\n",
      "Epoch: [16][ 99/365]\tBatch 59.712 (97.526)\tData 59.431 (97.238)\tLoss 1.04747 (1.80537)\tAcc@1  70.70 ( 53.64)\tAcc@5  93.75 ( 79.85)\tLearningRate 0.06118 (0.07362)\n",
      "Epoch: [16][199/365]\tBatch 109.616 (97.298)\tData 109.329 (97.011)\tLoss 1.07518 (1.79538)\tAcc@1  69.92 ( 53.87)\tAcc@5  91.41 ( 80.02)\tLearningRate 0.05943 (0.07340)\n",
      "Epoch: [16][299/365]\tBatch 156.996 (97.886)\tData 156.692 (97.598)\tLoss 1.14315 (1.78535)\tAcc@1  67.97 ( 54.09)\tAcc@5  91.02 ( 80.19)\tLearningRate 0.05766 (0.07316)\n",
      "0:03:05.156665 elapsed for 17\n",
      "Epoch: [17][ 99/365]\tBatch 57.197 (97.621)\tData 56.897 (97.333)\tLoss 1.17544 (1.76875)\tAcc@1  65.23 ( 54.49)\tAcc@5  91.41 ( 80.46)\tLearningRate 0.05473 (0.07271)\n",
      "Epoch: [17][199/365]\tBatch 106.966 (97.381)\tData 106.692 (97.093)\tLoss 1.24990 (1.75901)\tAcc@1  66.02 ( 54.72)\tAcc@5  89.45 ( 80.62)\tLearningRate 0.05294 (0.07242)\n",
      "Epoch: [17][299/365]\tBatch 157.135 (97.913)\tData 156.831 (97.626)\tLoss 1.30744 (1.74982)\tAcc@1  66.02 ( 54.94)\tAcc@5  87.89 ( 80.77)\tLearningRate 0.05115 (0.07210)\n",
      "0:03:04.387636 elapsed for 18\n",
      "Epoch: [18][ 99/365]\tBatch 59.549 (97.656)\tData 59.275 (97.368)\tLoss 1.06918 (1.73412)\tAcc@1  69.14 ( 55.31)\tAcc@5  90.23 ( 81.03)\tLearningRate 0.04819 (0.07155)\n",
      "Epoch: [18][199/365]\tBatch 109.164 (97.450)\tData 108.891 (97.162)\tLoss 1.05855 (1.72487)\tAcc@1  69.53 ( 55.52)\tAcc@5  94.14 ( 81.18)\tLearningRate 0.04640 (0.07119)\n",
      "Epoch: [18][299/365]\tBatch 158.099 (97.975)\tData 157.823 (97.687)\tLoss 1.13107 (1.71594)\tAcc@1  69.14 ( 55.74)\tAcc@5  89.84 ( 81.32)\tLearningRate 0.04461 (0.07082)\n",
      "0:03:04.654583 elapsed for 19\n",
      "Epoch: [19][ 99/365]\tBatch 58.169 (97.752)\tData 57.897 (97.464)\tLoss 1.10281 (1.70107)\tAcc@1  69.14 ( 56.09)\tAcc@5  91.80 ( 81.56)\tLearningRate 0.04168 (0.07017)\n",
      "Epoch: [19][199/365]\tBatch 106.560 (97.548)\tData 106.269 (97.261)\tLoss 1.11043 (1.69218)\tAcc@1  70.70 ( 56.30)\tAcc@5  92.19 ( 81.70)\tLearningRate 0.03992 (0.06976)\n",
      "Epoch: [19][299/365]\tBatch 158.108 (98.034)\tData 157.792 (97.746)\tLoss 0.96555 (1.68379)\tAcc@1  73.44 ( 56.49)\tAcc@5  92.58 ( 81.83)\tLearningRate 0.03817 (0.06933)\n",
      "0:03:04.796499 elapsed for 20\n",
      "Epoch: [20][ 99/365]\tBatch 58.852 (97.824)\tData 58.564 (97.536)\tLoss 1.14629 (1.66966)\tAcc@1  67.19 ( 56.83)\tAcc@5  93.75 ( 82.05)\tLearningRate 0.03532 (0.06860)\n",
      "Epoch: [20][199/365]\tBatch 107.126 (97.627)\tData 106.852 (97.339)\tLoss 0.97429 (1.66152)\tAcc@1  75.00 ( 57.02)\tAcc@5  93.75 ( 82.18)\tLearningRate 0.03361 (0.06815)\n",
      "Epoch: [20][299/365]\tBatch 156.988 (98.100)\tData 156.701 (97.813)\tLoss 1.13080 (1.65326)\tAcc@1  68.75 ( 57.22)\tAcc@5  89.06 ( 82.31)\tLearningRate 0.03193 (0.06768)\n",
      "0:03:04.273561 elapsed for 21\n",
      "Epoch: [21][ 99/365]\tBatch 58.521 (97.888)\tData 58.245 (97.600)\tLoss 0.98280 (1.63921)\tAcc@1  73.83 ( 57.56)\tAcc@5  92.19 ( 82.52)\tLearningRate 0.02921 (0.06689)\n",
      "Epoch: [21][199/365]\tBatch 109.284 (97.706)\tData 108.996 (97.418)\tLoss 0.93037 (1.63090)\tAcc@1  78.52 ( 57.76)\tAcc@5  93.36 ( 82.65)\tLearningRate 0.02759 (0.06640)\n",
      "Epoch: [21][299/365]\tBatch 157.450 (98.158)\tData 157.141 (97.871)\tLoss 1.01807 (1.62269)\tAcc@1  72.27 ( 57.95)\tAcc@5  92.58 ( 82.78)\tLearningRate 0.02600 (0.06591)\n",
      "0:03:06.114461 elapsed for 22\n",
      "Epoch: [22][ 99/365]\tBatch 59.780 (97.982)\tData 59.504 (97.695)\tLoss 0.95482 (1.60921)\tAcc@1  73.83 ( 58.28)\tAcc@5  94.53 ( 82.98)\tLearningRate 0.02345 (0.06507)\n",
      "Epoch: [22][199/365]\tBatch 109.386 (97.820)\tData 109.033 (97.533)\tLoss 0.84969 (1.60108)\tAcc@1  78.52 ( 58.47)\tAcc@5  91.41 ( 83.10)\tLearningRate 0.02195 (0.06456)\n",
      "Epoch: [22][299/365]\tBatch 157.185 (98.255)\tData 156.896 (97.967)\tLoss 1.01621 (1.59317)\tAcc@1  74.22 ( 58.67)\tAcc@5  91.80 ( 83.22)\tLearningRate 0.02048 (0.06404)\n",
      "0:03:05.036187 elapsed for 23\n",
      "Epoch: [23][ 99/365]\tBatch 58.226 (98.051)\tData 57.892 (97.763)\tLoss 0.90731 (1.57972)\tAcc@1  74.61 ( 58.99)\tAcc@5  94.53 ( 83.42)\tLearningRate 0.01815 (0.06317)\n",
      "Epoch: [23][199/365]\tBatch 106.295 (97.871)\tData 106.015 (97.583)\tLoss 0.83632 (1.57164)\tAcc@1  77.34 ( 59.19)\tAcc@5  94.53 ( 83.54)\tLearningRate 0.01678 (0.06263)\n",
      "Epoch: [23][299/365]\tBatch 157.719 (98.268)\tData 157.427 (97.980)\tLoss 0.80862 (1.56373)\tAcc@1  76.95 ( 59.38)\tAcc@5  94.53 ( 83.65)\tLearningRate 0.01547 (0.06210)\n",
      "0:03:04.478782 elapsed for 24\n",
      "Epoch: [24][ 99/365]\tBatch 59.325 (98.071)\tData 59.050 (97.784)\tLoss 0.65530 (1.55023)\tAcc@1  80.86 ( 59.71)\tAcc@5  95.70 ( 83.85)\tLearningRate 0.01339 (0.06121)\n",
      "Epoch: [24][199/365]\tBatch 106.085 (97.904)\tData 105.752 (97.616)\tLoss 0.93648 (1.54213)\tAcc@1  75.39 ( 59.91)\tAcc@5  92.19 ( 83.96)\tLearningRate 0.01219 (0.06067)\n",
      "Epoch: [24][299/365]\tBatch 152.968 (98.259)\tData 152.684 (97.971)\tLoss 0.75559 (1.53420)\tAcc@1  78.12 ( 60.11)\tAcc@5  94.92 ( 84.07)\tLearningRate 0.01104 (0.06013)\n",
      "0:03:02.817360 elapsed for 25\n",
      "Epoch: [25][ 99/365]\tBatch 59.083 (98.058)\tData 58.787 (97.771)\tLoss 0.67403 (1.52056)\tAcc@1  81.25 ( 60.44)\tAcc@5  95.70 ( 84.26)\tLearningRate 0.00926 (0.05924)\n",
      "Epoch: [25][199/365]\tBatch 107.383 (97.889)\tData 107.095 (97.602)\tLoss 0.65180 (1.51224)\tAcc@1  80.08 ( 60.65)\tAcc@5  96.09 ( 84.37)\tLearningRate 0.00824 (0.05869)\n",
      "Epoch: [25][299/365]\tBatch 155.757 (98.252)\tData 155.483 (97.965)\tLoss 0.72162 (1.50407)\tAcc@1  79.30 ( 60.85)\tAcc@5  97.27 ( 84.48)\tLearningRate 0.00728 (0.05815)\n",
      "0:03:03.724463 elapsed for 26\n",
      "Epoch: [26][ 99/365]\tBatch 58.081 (98.075)\tData 57.808 (97.787)\tLoss 0.68785 (1.49022)\tAcc@1  81.25 ( 61.19)\tAcc@5  96.48 ( 84.67)\tLearningRate 0.00582 (0.05727)\n",
      "Epoch: [26][199/365]\tBatch 108.429 (97.919)\tData 108.156 (97.631)\tLoss 0.63233 (1.48187)\tAcc@1  83.98 ( 61.40)\tAcc@5  95.31 ( 84.78)\tLearningRate 0.00501 (0.05673)\n",
      "Epoch: [26][299/365]\tBatch 155.120 (98.266)\tData 154.846 (97.979)\tLoss 0.70479 (1.47355)\tAcc@1  81.25 ( 61.61)\tAcc@5  94.53 ( 84.89)\tLearningRate 0.00426 (0.05620)\n",
      "0:03:03.870763 elapsed for 27\n",
      "Epoch: [27][ 99/365]\tBatch 57.336 (98.083)\tData 57.062 (97.795)\tLoss 0.57991 (1.45989)\tAcc@1  87.50 ( 61.95)\tAcc@5  94.14 ( 85.07)\tLearningRate 0.00314 (0.05533)\n",
      "Epoch: [27][199/365]\tBatch 106.399 (97.918)\tData 106.103 (97.630)\tLoss 0.67519 (1.45149)\tAcc@1  84.77 ( 62.17)\tAcc@5  94.53 ( 85.18)\tLearningRate 0.00255 (0.05481)\n",
      "Epoch: [27][299/365]\tBatch 157.271 (98.257)\tData 156.927 (97.969)\tLoss 0.68573 (1.44316)\tAcc@1  82.42 ( 62.38)\tAcc@5  94.92 ( 85.28)\tLearningRate 0.00201 (0.05429)\n",
      "0:03:06.611809 elapsed for 28\n",
      "Epoch: [28][ 99/365]\tBatch 60.075 (98.114)\tData 59.781 (97.826)\tLoss 0.72304 (1.42925)\tAcc@1  80.86 ( 62.73)\tAcc@5  94.53 ( 85.46)\tLearningRate 0.00127 (0.05345)\n",
      "Epoch: [28][199/365]\tBatch 109.811 (97.979)\tData 109.527 (97.692)\tLoss 0.48712 (1.42106)\tAcc@1  86.33 ( 62.94)\tAcc@5  97.27 ( 85.56)\tLearningRate 0.00090 (0.05294)\n",
      "Epoch: [28][299/365]\tBatch 158.305 (98.326)\tData 158.031 (98.038)\tLoss 0.54965 (1.41286)\tAcc@1  85.16 ( 63.15)\tAcc@5  96.88 ( 85.66)\tLearningRate 0.00059 (0.05245)\n",
      "0:03:06.313721 elapsed for 29\n",
      "Epoch: [29][ 99/365]\tBatch 60.993 (98.187)\tData 60.718 (97.899)\tLoss 0.43818 (1.39957)\tAcc@1  85.94 ( 63.49)\tAcc@5  98.44 ( 85.83)\tLearningRate 0.00022 (0.05164)\n",
      "Epoch: [29][199/365]\tBatch 109.159 (98.055)\tData 108.877 (97.767)\tLoss 0.47978 (1.39165)\tAcc@1  86.33 ( 63.69)\tAcc@5  98.05 ( 85.93)\tLearningRate 0.00009 (0.05117)\n",
      "Epoch: [29][299/365]\tBatch 159.168 (98.383)\tData 158.895 (98.095)\tLoss 0.61462 (1.38383)\tAcc@1  83.59 ( 63.89)\tAcc@5  94.53 ( 86.03)\tLearningRate 0.00001 (0.05070)\n",
      "0:03:05.911601 elapsed for 30\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (_, inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/darknet25_fp16_5e4_ep030.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            _, images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_results = [classification_val(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907592985160918"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7929273463095009,\n",
       " 0.791771054153016,\n",
       " 0.7870495278473695,\n",
       " 0.7908074773559453,\n",
       " 0.7934091347080362,\n",
       " 0.7900366159182887,\n",
       " 0.7900366159182887,\n",
       " 0.789169396800925,\n",
       " 0.7906147619965311,\n",
       " 0.791771054153016]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7828868760840239"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002308887647354633"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7867604548082482,\n",
       " 0.7841587974561572,\n",
       " 0.783773366737329,\n",
       " 0.7795336288302178,\n",
       " 0.7843515128155714,\n",
       " 0.7842551551358643,\n",
       " 0.7832915783387936,\n",
       " 0.7815571401040663,\n",
       " 0.7826170745808441,\n",
       " 0.778570052033147]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (_, images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result = [val_retrieval(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7046251686259396"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6999421853921758,\n",
       " 0.7049527847369436,\n",
       " 0.7031219888225092,\n",
       " 0.7024474850645597,\n",
       " 0.7053382154557718,\n",
       " 0.7046637116978223,\n",
       " 0.7087107342455193,\n",
       " 0.7011948352283677,\n",
       " 0.7120832530352669,\n",
       " 0.7037964925804586]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6494603969936404"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00523044716416355"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6546540759298516,\n",
       " 0.6394295625361341,\n",
       " 0.6442474465214878,\n",
       " 0.6489689728271343,\n",
       " 0.6478126806706495,\n",
       " 0.6587974561572557,\n",
       " 0.6544613605704375,\n",
       " 0.6481981113894777,\n",
       " 0.6481017537097706,\n",
       " 0.6499325496242051]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
