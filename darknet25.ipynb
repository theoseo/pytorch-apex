{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        #self.se = SEBlock(self.inplanes, ratio=9)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        #output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(2, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(2, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(2, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((1,3,224,224))\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 150])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20402550"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=1e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)\n",
    "\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image_id, sample, target) where target is class_index of\n",
    "                the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        #print(path)\n",
    "        #print(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        image_id = path.split('/')[-1]\n",
    "\n",
    "        return image_id, sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "data_dir = 'train/train_data'\n",
    "\n",
    "dataset = CustomDataset(data_dir, transform=get_transform(random_crop=True))\n",
    "\n",
    "split_size = int(len(dataset) * 0.9)\n",
    "train_set, valid_set = data.random_split(dataset, [split_size, len(dataset) - split_size])\n",
    "tr_loader = data.DataLoader(dataset=train_set,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=valid_set,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,\n",
    "                            pin_memory=True,                             \n",
    "                            num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    _, inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=30, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 99/365]\tBatch 404.694 (216.168)\tData 404.423 (215.891)\tLoss 4.05081 (4.35862)\tAcc@1  11.33 (  5.95)\tAcc@5  28.91 ( 19.76)\tLearningRate 0.00449 (0.00417)\n",
      "Epoch: [0][199/365]\tBatch 731.304 (388.025)\tData 731.032 (387.751)\tLoss 3.65536 (4.10221)\tAcc@1  15.62 (  8.94)\tAcc@5  39.84 ( 27.21)\tLearningRate 0.00596 (0.00466)\n",
      "Epoch: [0][299/365]\tBatch 1061.516 (559.224)\tData 1061.244 (558.950)\tLoss 3.25335 (3.92453)\tAcc@1  19.53 ( 11.37)\tAcc@5  54.30 ( 32.30)\tLearningRate 0.00838 (0.00548)\n",
      "0:20:57.284816 elapsed for 1\n",
      "Epoch: [1][ 99/365]\tBatch 59.696 (532.773)\tData 59.422 (532.497)\tLoss 3.04573 (3.69520)\tAcc@1  25.00 ( 14.95)\tAcc@5  60.16 ( 38.78)\tLearningRate 0.01430 (0.00750)\n",
      "Epoch: [1][199/365]\tBatch 108.896 (453.302)\tData 108.618 (453.024)\tLoss 3.21947 (3.58302)\tAcc@1  22.27 ( 16.79)\tAcc@5  55.86 ( 41.91)\tLearningRate 0.01893 (0.00910)\n",
      "Epoch: [1][299/365]\tBatch 156.349 (405.108)\tData 156.077 (404.828)\tLoss 2.85222 (3.47516)\tAcc@1  30.47 ( 18.66)\tAcc@5  60.55 ( 44.81)\tLearningRate 0.02425 (0.01098)\n",
      "0:03:04.337041 elapsed for 2\n",
      "Epoch: [2][ 99/365]\tBatch 57.528 (341.985)\tData 57.239 (341.705)\tLoss 2.63110 (3.31891)\tAcc@1  36.33 ( 21.51)\tAcc@5  63.28 ( 48.85)\tLearningRate 0.03421 (0.01459)\n",
      "Epoch: [2][199/365]\tBatch 107.277 (313.921)\tData 106.988 (313.640)\tLoss 2.35989 (3.23323)\tAcc@1  37.89 ( 23.11)\tAcc@5  73.05 ( 51.02)\tLearningRate 0.04077 (0.01705)\n",
      "Epoch: [2][299/365]\tBatch 155.479 (296.221)\tData 155.191 (295.939)\tLoss 2.60531 (3.15696)\tAcc@1  35.94 ( 24.57)\tAcc@5  68.36 ( 52.90)\tLearningRate 0.04756 (0.01969)\n",
      "0:03:05.383164 elapsed for 3\n",
      "Epoch: [3][ 99/365]\tBatch 56.270 (267.330)\tData 55.996 (267.048)\tLoss 2.11480 (3.03949)\tAcc@1  42.19 ( 26.90)\tAcc@5  73.05 ( 55.71)\tLearningRate 0.05890 (0.02433)\n",
      "Epoch: [3][199/365]\tBatch 107.530 (252.951)\tData 107.256 (252.669)\tLoss 2.13494 (2.97544)\tAcc@1  46.88 ( 28.17)\tAcc@5  75.78 ( 57.18)\tLearningRate 0.06562 (0.02726)\n",
      "Epoch: [3][299/365]\tBatch 154.504 (244.143)\tData 154.228 (243.860)\tLoss 2.13575 (2.91719)\tAcc@1  43.75 ( 29.35)\tAcc@5  76.95 ( 58.51)\tLearningRate 0.07207 (0.03024)\n",
      "0:03:04.084971 elapsed for 4\n",
      "Epoch: [4][ 99/365]\tBatch 58.522 (227.579)\tData 58.251 (227.296)\tLoss 1.96841 (2.82349)\tAcc@1  48.44 ( 31.28)\tAcc@5  80.08 ( 60.58)\tLearningRate 0.08173 (0.03519)\n",
      "Epoch: [4][199/365]\tBatch 108.434 (218.816)\tData 108.156 (218.533)\tLoss 1.80198 (2.77088)\tAcc@1  46.88 ( 32.36)\tAcc@5  80.08 ( 61.70)\tLearningRate 0.08682 (0.03816)\n",
      "Epoch: [4][299/365]\tBatch 155.345 (213.816)\tData 155.071 (213.532)\tLoss 1.81621 (2.72120)\tAcc@1  53.52 ( 33.40)\tAcc@5  81.64 ( 62.75)\tLearningRate 0.09119 (0.04105)\n",
      "0:03:04.646151 elapsed for 5\n",
      "Epoch: [5][ 99/365]\tBatch 58.028 (203.010)\tData 57.757 (202.726)\tLoss 1.69083 (2.64505)\tAcc@1  54.30 ( 35.03)\tAcc@5  80.47 ( 64.35)\tLearningRate 0.09660 (0.04560)\n",
      "Epoch: [5][199/365]\tBatch 106.164 (197.051)\tData 105.877 (196.767)\tLoss 1.78961 (2.60122)\tAcc@1  54.30 ( 35.96)\tAcc@5  80.86 ( 65.23)\tLearningRate 0.09868 (0.04817)\n",
      "Epoch: [5][299/365]\tBatch 157.420 (194.032)\tData 157.117 (193.747)\tLoss 1.63561 (2.55970)\tAcc@1  57.03 ( 36.86)\tAcc@5  85.94 ( 66.07)\tLearningRate 0.09980 (0.05058)\n",
      "0:03:03.707622 elapsed for 6\n",
      "Epoch: [6][ 99/365]\tBatch 57.148 (186.373)\tData 56.873 (186.089)\tLoss 1.67296 (2.49430)\tAcc@1  53.91 ( 38.27)\tAcc@5  83.59 ( 67.37)\tLearningRate 0.09997 (0.05414)\n",
      "Epoch: [6][199/365]\tBatch 105.532 (181.995)\tData 105.250 (181.711)\tLoss 1.72061 (2.45656)\tAcc@1  57.81 ( 39.07)\tAcc@5  81.25 ( 68.09)\tLearningRate 0.09987 (0.05605)\n",
      "Epoch: [6][299/365]\tBatch 153.849 (179.891)\tData 153.530 (179.607)\tLoss 1.62599 (2.42102)\tAcc@1  56.25 ( 39.85)\tAcc@5  83.59 ( 68.77)\tLearningRate 0.09971 (0.05781)\n",
      "0:03:03.201170 elapsed for 7\n",
      "Epoch: [7][ 99/365]\tBatch 55.234 (174.103)\tData 54.900 (173.818)\tLoss 1.54666 (2.36521)\tAcc@1  56.64 ( 41.07)\tAcc@5  85.55 ( 69.83)\tLearningRate 0.09930 (0.06040)\n",
      "Epoch: [7][199/365]\tBatch 107.411 (170.783)\tData 107.123 (170.499)\tLoss 1.34254 (2.33334)\tAcc@1  66.41 ( 41.77)\tAcc@5  87.89 ( 70.42)\tLearningRate 0.09897 (0.06181)\n",
      "Epoch: [7][299/365]\tBatch 156.467 (169.425)\tData 156.193 (169.140)\tLoss 1.50944 (2.30324)\tAcc@1  60.94 ( 42.43)\tAcc@5  85.94 ( 70.97)\tLearningRate 0.09858 (0.06310)\n",
      "0:03:06.052119 elapsed for 8\n",
      "Epoch: [8][ 99/365]\tBatch 57.349 (165.005)\tData 57.075 (164.720)\tLoss 1.53018 (2.25490)\tAcc@1  58.98 ( 43.51)\tAcc@5  86.72 ( 71.86)\tLearningRate 0.09780 (0.06502)\n",
      "Epoch: [8][199/365]\tBatch 108.555 (162.396)\tData 108.279 (162.111)\tLoss 1.23822 (2.22708)\tAcc@1  64.45 ( 44.13)\tAcc@5  89.45 ( 72.37)\tLearningRate 0.09724 (0.06606)\n",
      "Epoch: [8][299/365]\tBatch 157.201 (161.438)\tData 156.927 (161.153)\tLoss 1.47710 (2.20032)\tAcc@1  58.59 ( 44.73)\tAcc@5  85.55 ( 72.86)\tLearningRate 0.09662 (0.06702)\n",
      "0:03:05.188756 elapsed for 9\n",
      "Epoch: [9][ 99/365]\tBatch 56.331 (157.839)\tData 56.016 (157.554)\tLoss 1.39944 (2.15849)\tAcc@1  61.72 ( 45.68)\tAcc@5  88.28 ( 73.60)\tLearningRate 0.09547 (0.06844)\n",
      "Epoch: [9][199/365]\tBatch 105.064 (155.638)\tData 104.790 (155.353)\tLoss 1.46940 (2.13459)\tAcc@1  58.98 ( 46.21)\tAcc@5  87.11 ( 74.03)\tLearningRate 0.09470 (0.06920)\n",
      "Epoch: [9][299/365]\tBatch 157.314 (154.940)\tData 157.027 (154.655)\tLoss 1.20884 (2.11131)\tAcc@1  69.53 ( 46.75)\tAcc@5  91.02 ( 74.43)\tLearningRate 0.09386 (0.06990)\n",
      "0:03:03.832747 elapsed for 10\n",
      "Epoch: [10][ 99/365]\tBatch 59.323 (152.010)\tData 59.049 (151.725)\tLoss 1.14690 (2.07456)\tAcc@1  64.45 ( 47.58)\tAcc@5  92.19 ( 75.07)\tLearningRate 0.09237 (0.07092)\n",
      "Epoch: [10][199/365]\tBatch 108.925 (150.237)\tData 108.647 (149.952)\tLoss 1.30335 (2.05312)\tAcc@1  66.41 ( 48.07)\tAcc@5  87.89 ( 75.44)\tLearningRate 0.09139 (0.07147)\n",
      "Epoch: [10][299/365]\tBatch 155.712 (149.795)\tData 155.438 (149.509)\tLoss 1.20810 (2.03236)\tAcc@1  67.58 ( 48.54)\tAcc@5  89.84 ( 75.80)\tLearningRate 0.09036 (0.07196)\n",
      "0:03:04.654282 elapsed for 11\n",
      "Epoch: [11][ 99/365]\tBatch 59.205 (147.308)\tData 58.931 (147.023)\tLoss 1.06406 (1.99917)\tAcc@1  69.92 ( 49.29)\tAcc@5  92.19 ( 76.37)\tLearningRate 0.08854 (0.07266)\n",
      "Epoch: [11][199/365]\tBatch 106.970 (145.779)\tData 106.639 (145.494)\tLoss 1.26832 (1.98005)\tAcc@1  65.62 ( 49.72)\tAcc@5  87.89 ( 76.69)\tLearningRate 0.08737 (0.07302)\n",
      "Epoch: [11][299/365]\tBatch 155.752 (145.456)\tData 155.480 (145.171)\tLoss 1.12794 (1.96202)\tAcc@1  69.53 ( 50.13)\tAcc@5  91.41 ( 76.99)\tLearningRate 0.08616 (0.07334)\n",
      "0:03:03.433179 elapsed for 12\n",
      "Epoch: [12][ 99/365]\tBatch 60.085 (143.350)\tData 59.788 (143.065)\tLoss 1.27328 (1.93267)\tAcc@1  68.36 ( 50.81)\tAcc@5  88.67 ( 77.49)\tLearningRate 0.08405 (0.07377)\n",
      "Epoch: [12][199/365]\tBatch 110.324 (142.065)\tData 110.052 (141.779)\tLoss 1.25611 (1.91549)\tAcc@1  64.84 ( 51.20)\tAcc@5  89.45 ( 77.77)\tLearningRate 0.08272 (0.07398)\n",
      "Epoch: [12][299/365]\tBatch 154.813 (141.867)\tData 154.526 (141.582)\tLoss 1.40036 (1.89874)\tAcc@1  66.02 ( 51.59)\tAcc@5  85.16 ( 78.05)\tLearningRate 0.08134 (0.07416)\n",
      "0:03:03.304361 elapsed for 13\n",
      "Epoch: [13][ 99/365]\tBatch 60.577 (140.036)\tData 60.289 (139.750)\tLoss 1.35431 (1.87227)\tAcc@1  64.84 ( 52.19)\tAcc@5  87.50 ( 78.49)\tLearningRate 0.07898 (0.07436)\n",
      "Epoch: [13][199/365]\tBatch 107.949 (138.906)\tData 107.649 (138.620)\tLoss 1.09515 (1.85652)\tAcc@1  69.53 ( 52.56)\tAcc@5  89.84 ( 78.75)\tLearningRate 0.07750 (0.07444)\n",
      "Epoch: [13][299/365]\tBatch 156.765 (138.785)\tData 156.493 (138.499)\tLoss 0.91029 (1.84167)\tAcc@1  75.00 ( 52.90)\tAcc@5  94.14 ( 78.99)\tLearningRate 0.07599 (0.07448)\n",
      "0:03:04.643048 elapsed for 14\n",
      "Epoch: [14][ 99/365]\tBatch 60.985 (137.188)\tData 60.698 (136.903)\tLoss 0.98857 (1.81704)\tAcc@1  70.70 ( 53.47)\tAcc@5  93.75 ( 79.39)\tLearningRate 0.07342 (0.07449)\n",
      "Epoch: [14][199/365]\tBatch 110.357 (136.203)\tData 110.061 (135.918)\tLoss 1.15007 (1.80283)\tAcc@1  68.75 ( 53.81)\tAcc@5  92.58 ( 79.62)\tLearningRate 0.07182 (0.07446)\n",
      "Epoch: [14][299/365]\tBatch 156.881 (136.174)\tData 156.586 (135.888)\tLoss 1.08790 (1.78903)\tAcc@1  68.75 ( 54.13)\tAcc@5  91.80 ( 79.84)\tLearningRate 0.07019 (0.07439)\n",
      "0:03:05.617394 elapsed for 15\n",
      "Epoch: [15][ 99/365]\tBatch 60.755 (134.771)\tData 60.479 (134.486)\tLoss 0.94488 (1.76622)\tAcc@1  75.39 ( 54.66)\tAcc@5  93.36 ( 80.20)\tLearningRate 0.06745 (0.07423)\n",
      "Epoch: [15][199/365]\tBatch 111.756 (133.895)\tData 111.468 (133.610)\tLoss 0.84857 (1.75300)\tAcc@1  75.39 ( 54.97)\tAcc@5  93.75 ( 80.41)\tLearningRate 0.06576 (0.07409)\n",
      "Epoch: [15][299/365]\tBatch 161.160 (133.919)\tData 160.872 (133.634)\tLoss 0.96271 (1.74015)\tAcc@1  73.83 ( 55.28)\tAcc@5  92.58 ( 80.62)\tLearningRate 0.06405 (0.07393)\n",
      "0:03:04.025671 elapsed for 16\n",
      "Epoch: [16][ 99/365]\tBatch 59.917 (132.651)\tData 59.634 (132.366)\tLoss 0.75031 (1.71910)\tAcc@1  77.73 ( 55.77)\tAcc@5  94.92 ( 80.95)\tLearningRate 0.06118 (0.07362)\n",
      "Epoch: [16][199/365]\tBatch 108.877 (131.829)\tData 108.570 (131.543)\tLoss 1.02500 (1.70661)\tAcc@1  71.09 ( 56.06)\tAcc@5  91.80 ( 81.14)\tLearningRate 0.05943 (0.07340)\n",
      "Epoch: [16][299/365]\tBatch 156.969 (131.849)\tData 156.678 (131.564)\tLoss 1.03261 (1.69461)\tAcc@1  71.88 ( 56.34)\tAcc@5  91.80 ( 81.32)\tLearningRate 0.05766 (0.07316)\n",
      "0:03:05.217233 elapsed for 17\n",
      "Epoch: [17][ 99/365]\tBatch 59.614 (130.712)\tData 59.340 (130.426)\tLoss 1.08245 (1.67496)\tAcc@1  71.09 ( 56.81)\tAcc@5  91.02 ( 81.63)\tLearningRate 0.05473 (0.07271)\n",
      "Epoch: [17][199/365]\tBatch 108.392 (129.985)\tData 108.112 (129.699)\tLoss 0.93257 (1.66358)\tAcc@1  72.27 ( 57.08)\tAcc@5  93.36 ( 81.80)\tLearningRate 0.05294 (0.07242)\n",
      "Epoch: [17][299/365]\tBatch 155.388 (130.026)\tData 155.112 (129.740)\tLoss 0.78434 (1.65247)\tAcc@1  74.22 ( 57.34)\tAcc@5  96.88 ( 81.97)\tLearningRate 0.05115 (0.07210)\n",
      "0:03:04.754750 elapsed for 18\n",
      "Epoch: [18][ 99/365]\tBatch 61.563 (128.992)\tData 61.290 (128.706)\tLoss 0.84003 (1.63408)\tAcc@1  79.69 ( 57.78)\tAcc@5  94.92 ( 82.25)\tLearningRate 0.04819 (0.07155)\n",
      "Epoch: [18][199/365]\tBatch 109.064 (128.331)\tData 108.790 (128.045)\tLoss 0.95771 (1.62320)\tAcc@1  74.61 ( 58.04)\tAcc@5  94.14 ( 82.42)\tLearningRate 0.04640 (0.07119)\n",
      "Epoch: [18][299/365]\tBatch 156.449 (128.394)\tData 156.174 (128.108)\tLoss 1.08758 (1.61279)\tAcc@1  71.09 ( 58.28)\tAcc@5  88.28 ( 82.57)\tLearningRate 0.04461 (0.07082)\n",
      "0:03:04.708459 elapsed for 19\n",
      "Epoch: [19][ 99/365]\tBatch 59.281 (127.473)\tData 58.936 (127.187)\tLoss 0.92504 (1.59536)\tAcc@1  72.27 ( 58.70)\tAcc@5  94.14 ( 82.83)\tLearningRate 0.04168 (0.07017)\n",
      "Epoch: [19][199/365]\tBatch 107.977 (126.848)\tData 107.693 (126.562)\tLoss 0.98066 (1.58503)\tAcc@1  72.66 ( 58.94)\tAcc@5  91.02 ( 82.98)\tLearningRate 0.03992 (0.06976)\n",
      "Epoch: [19][299/365]\tBatch 157.020 (126.941)\tData 156.655 (126.655)\tLoss 0.73733 (1.57509)\tAcc@1  79.69 ( 59.18)\tAcc@5  94.92 ( 83.13)\tLearningRate 0.03817 (0.06933)\n",
      "0:03:04.154152 elapsed for 20\n",
      "Epoch: [20][ 99/365]\tBatch 59.354 (126.078)\tData 59.076 (125.791)\tLoss 0.83639 (1.55860)\tAcc@1  78.12 ( 59.58)\tAcc@5  92.97 ( 83.37)\tLearningRate 0.03532 (0.06860)\n",
      "Epoch: [20][199/365]\tBatch 108.291 (125.523)\tData 108.016 (125.237)\tLoss 0.71631 (1.54899)\tAcc@1  78.91 ( 59.81)\tAcc@5  95.31 ( 83.51)\tLearningRate 0.03361 (0.06815)\n",
      "Epoch: [20][299/365]\tBatch 154.962 (125.618)\tData 154.659 (125.331)\tLoss 0.70988 (1.53951)\tAcc@1  78.12 ( 60.04)\tAcc@5  95.31 ( 83.65)\tLearningRate 0.03193 (0.06768)\n",
      "0:03:02.999537 elapsed for 21\n",
      "Epoch: [21][ 99/365]\tBatch 58.815 (124.805)\tData 58.511 (124.519)\tLoss 1.07556 (1.52391)\tAcc@1  73.83 ( 60.41)\tAcc@5  92.19 ( 83.88)\tLearningRate 0.02921 (0.06689)\n",
      "Epoch: [21][199/365]\tBatch 106.438 (124.262)\tData 106.107 (123.976)\tLoss 0.71079 (1.51464)\tAcc@1  80.08 ( 60.64)\tAcc@5  96.09 ( 84.01)\tLearningRate 0.02759 (0.06640)\n",
      "Epoch: [21][299/365]\tBatch 153.573 (124.342)\tData 153.299 (124.056)\tLoss 0.79167 (1.50544)\tAcc@1  76.95 ( 60.85)\tAcc@5  95.31 ( 84.14)\tLearningRate 0.02600 (0.06591)\n",
      "0:03:01.713260 elapsed for 22\n",
      "Epoch: [22][ 99/365]\tBatch 57.741 (123.577)\tData 57.442 (123.291)\tLoss 0.74579 (1.49013)\tAcc@1  80.86 ( 61.23)\tAcc@5  95.70 ( 84.36)\tLearningRate 0.02345 (0.06507)\n",
      "Epoch: [22][199/365]\tBatch 104.860 (123.053)\tData 104.587 (122.767)\tLoss 0.61332 (1.48115)\tAcc@1  82.03 ( 61.45)\tAcc@5  96.88 ( 84.48)\tLearningRate 0.02195 (0.06456)\n",
      "Epoch: [22][299/365]\tBatch 155.571 (123.141)\tData 155.299 (122.854)\tLoss 0.75604 (1.47239)\tAcc@1  76.95 ( 61.66)\tAcc@5  94.53 ( 84.61)\tLearningRate 0.02048 (0.06404)\n",
      "0:03:01.707315 elapsed for 23\n",
      "Epoch: [23][ 99/365]\tBatch 57.749 (122.422)\tData 57.466 (122.136)\tLoss 0.72451 (1.45778)\tAcc@1  80.86 ( 62.01)\tAcc@5  95.70 ( 84.81)\tLearningRate 0.01815 (0.06317)\n",
      "Epoch: [23][199/365]\tBatch 106.431 (121.947)\tData 106.158 (121.661)\tLoss 0.67576 (1.44907)\tAcc@1  80.08 ( 62.23)\tAcc@5  94.53 ( 84.93)\tLearningRate 0.01678 (0.06263)\n",
      "Epoch: [23][299/365]\tBatch 156.045 (122.049)\tData 155.743 (121.763)\tLoss 0.77850 (1.44062)\tAcc@1  79.30 ( 62.43)\tAcc@5  95.31 ( 85.05)\tLearningRate 0.01547 (0.06210)\n",
      "0:03:04.762021 elapsed for 24\n",
      "Epoch: [24][ 99/365]\tBatch 57.309 (121.407)\tData 56.988 (121.121)\tLoss 0.60997 (1.42630)\tAcc@1  80.47 ( 62.78)\tAcc@5  96.48 ( 85.25)\tLearningRate 0.01339 (0.06121)\n",
      "Epoch: [24][199/365]\tBatch 107.787 (120.981)\tData 107.504 (120.695)\tLoss 0.73656 (1.41768)\tAcc@1  82.03 ( 63.00)\tAcc@5  94.92 ( 85.36)\tLearningRate 0.01219 (0.06067)\n",
      "Epoch: [24][299/365]\tBatch 155.790 (121.101)\tData 155.471 (120.814)\tLoss 0.60425 (1.40926)\tAcc@1  83.98 ( 63.21)\tAcc@5  94.53 ( 85.47)\tLearningRate 0.01104 (0.06013)\n",
      "0:03:02.154492 elapsed for 25\n",
      "Epoch: [25][ 99/365]\tBatch 59.577 (120.497)\tData 59.289 (120.211)\tLoss 0.62880 (1.39544)\tAcc@1  80.47 ( 63.55)\tAcc@5  96.48 ( 85.66)\tLearningRate 0.00926 (0.05924)\n",
      "Epoch: [25][199/365]\tBatch 106.038 (120.106)\tData 105.757 (119.820)\tLoss 0.75896 (1.38729)\tAcc@1  80.47 ( 63.75)\tAcc@5  94.53 ( 85.77)\tLearningRate 0.00824 (0.05869)\n",
      "Epoch: [25][299/365]\tBatch 158.079 (120.228)\tData 157.796 (119.941)\tLoss 0.64222 (1.37911)\tAcc@1  82.03 ( 63.96)\tAcc@5  94.92 ( 85.87)\tLearningRate 0.00728 (0.05815)\n",
      "0:03:03.286259 elapsed for 26\n",
      "Epoch: [26][ 99/365]\tBatch 58.974 (119.671)\tData 58.683 (119.385)\tLoss 0.50216 (1.36551)\tAcc@1  87.50 ( 64.30)\tAcc@5  96.09 ( 86.05)\tLearningRate 0.00582 (0.05727)\n",
      "Epoch: [26][199/365]\tBatch 106.994 (119.291)\tData 106.719 (119.004)\tLoss 0.62780 (1.35745)\tAcc@1  82.42 ( 64.50)\tAcc@5  96.09 ( 86.16)\tLearningRate 0.00501 (0.05673)\n",
      "Epoch: [26][299/365]\tBatch 154.838 (119.415)\tData 154.527 (119.128)\tLoss 0.54731 (1.34945)\tAcc@1  85.16 ( 64.70)\tAcc@5  96.48 ( 86.26)\tLearningRate 0.00426 (0.05620)\n",
      "0:03:02.603693 elapsed for 27\n",
      "Epoch: [27][ 99/365]\tBatch 56.570 (118.868)\tData 56.298 (118.582)\tLoss 0.54142 (1.33615)\tAcc@1  85.16 ( 65.04)\tAcc@5  94.53 ( 86.43)\tLearningRate 0.00314 (0.05533)\n",
      "Epoch: [27][199/365]\tBatch 105.893 (118.501)\tData 105.619 (118.215)\tLoss 0.53348 (1.32834)\tAcc@1  85.94 ( 65.23)\tAcc@5  97.27 ( 86.53)\tLearningRate 0.00255 (0.05481)\n",
      "Epoch: [27][299/365]\tBatch 153.497 (118.618)\tData 153.223 (118.331)\tLoss 0.53690 (1.32066)\tAcc@1  85.94 ( 65.43)\tAcc@5  96.88 ( 86.63)\tLearningRate 0.00201 (0.05429)\n",
      "0:03:02.229169 elapsed for 28\n",
      "Epoch: [28][ 99/365]\tBatch 57.591 (118.106)\tData 57.316 (117.819)\tLoss 0.54179 (1.30800)\tAcc@1  87.11 ( 65.75)\tAcc@5  96.09 ( 86.78)\tLearningRate 0.00127 (0.05345)\n",
      "Epoch: [28][199/365]\tBatch 106.842 (117.755)\tData 106.569 (117.469)\tLoss 0.51486 (1.30044)\tAcc@1  85.55 ( 65.94)\tAcc@5  98.05 ( 86.88)\tLearningRate 0.00090 (0.05294)\n",
      "Epoch: [28][299/365]\tBatch 156.367 (117.893)\tData 156.089 (117.607)\tLoss 0.57038 (1.29305)\tAcc@1  84.77 ( 66.13)\tAcc@5  96.48 ( 86.97)\tLearningRate 0.00059 (0.05245)\n",
      "0:03:05.361783 elapsed for 29\n",
      "Epoch: [29][ 99/365]\tBatch 59.051 (117.438)\tData 58.761 (117.152)\tLoss 0.66679 (1.28113)\tAcc@1  82.03 ( 66.44)\tAcc@5  94.92 ( 87.12)\tLearningRate 0.00022 (0.05164)\n",
      "Epoch: [29][199/365]\tBatch 109.776 (117.135)\tData 109.501 (116.848)\tLoss 0.46640 (1.27395)\tAcc@1  88.67 ( 66.62)\tAcc@5  97.27 ( 87.21)\tLearningRate 0.00009 (0.05117)\n",
      "Epoch: [29][299/365]\tBatch 161.204 (117.298)\tData 160.917 (117.011)\tLoss 0.52158 (1.26706)\tAcc@1  83.20 ( 66.80)\tAcc@5  98.05 ( 87.30)\tLearningRate 0.00001 (0.05070)\n",
      "0:03:05.670801 elapsed for 30\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (_, inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/darknet25_fp16_sconv_ep030.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            _, images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_results = [classification_val(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7828868760840239"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002308887647354633"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7867604548082482,\n",
       " 0.7841587974561572,\n",
       " 0.783773366737329,\n",
       " 0.7795336288302178,\n",
       " 0.7843515128155714,\n",
       " 0.7842551551358643,\n",
       " 0.7832915783387936,\n",
       " 0.7815571401040663,\n",
       " 0.7826170745808441,\n",
       " 0.778570052033147]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (_, images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result = [val_retrieval(model, val_loader) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6494603969936404"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00523044716416355"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(retrieval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6546540759298516,\n",
       " 0.6394295625361341,\n",
       " 0.6442474465214878,\n",
       " 0.6489689728271343,\n",
       " 0.6478126806706495,\n",
       " 0.6587974561572557,\n",
       " 0.6544613605704375,\n",
       " 0.6481981113894777,\n",
       " 0.6481017537097706,\n",
       " 0.6499325496242051]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
