{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=True):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        pad = 0\n",
    "        if padding :\n",
    "            pad = (self.kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=pad, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_planes, momentum=0.99)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv(x)\n",
    "        output = self.batchnorm(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarknetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_planes):\n",
    "\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.inplanes = out_planes * 2\n",
    "        self.conv1 = ConvBlock(self.inplanes, out_planes, 1)\n",
    "        self.conv2 = ConvBlock(out_planes, self.inplanes, 3)\n",
    "        #self.se = SEBlock(self.inplanes, ratio=9)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        #output = self.se(output)\n",
    "        output = output + shortcut\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "\n",
    "        super(Darknet, self).__init__()   \n",
    "\n",
    "        self.conv_block1 = ConvBlock(3, 32, 3, 1)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 3, 2)\n",
    "\n",
    "        self.dark_block1 = DarknetBlock(32)\n",
    "\n",
    "        self.conv_block3 = ConvBlock(64, 128, 3, 2)\n",
    "\n",
    "        self.dark_layer1 = self._make_blocks(2, 64)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(128, 256, 3, 2)\n",
    "\n",
    "        self.dark_layer2 = self._make_blocks(2, 128)\n",
    "\n",
    "        self.conv_block5 = ConvBlock(256, 512, 3, 2)\n",
    "\n",
    "        self.dark_layer3 = self._make_blocks(2, 256)\n",
    "\n",
    "        self.conv_block6 = ConvBlock(512, 1024, 3, 2)\n",
    "\n",
    "        self.dark_layer4 = self._make_blocks(2, 512)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)           \n",
    "\n",
    "    def _make_blocks(self, num_blocks, out_planes):\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(DarknetBlock(out_planes))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, feature=False):\n",
    "\n",
    "        output = self.conv_block1(x)\n",
    "        output = self.conv_block2(output)\n",
    "\n",
    "        output = self.dark_block1(output)\n",
    "\n",
    "        output = self.conv_block3(output)\n",
    "\n",
    "        output = self.dark_layer1(output)\n",
    "\n",
    "        output = self.conv_block4(output)\n",
    "\n",
    "        output = self.dark_layer2(output)\n",
    "\n",
    "        output = self.conv_block5(output)\n",
    "\n",
    "        output = self.dark_layer3(output)\n",
    "\n",
    "        output = self.conv_block6(output)\n",
    "\n",
    "        output = self.dark_layer4(output)\n",
    "\n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        if feature:\n",
    "            return output\n",
    "\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(num_classes=196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20449700"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n",
    "        #[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        #transform.append(transforms.RandomRotation(30))\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "        transform.append(transforms.ColorJitter(hue=.05, saturation=.05),)\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "class CarsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \n",
    "        self.pd_csv = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pd_csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "    \n",
    "        img_name = os.path.join(self.root_dir, \n",
    "                                self.pd_csv.iloc[index, 1])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        target = self.pd_csv.iloc[index, 0]\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CarsDataset('./data/sfcars/train.csv', './data/sfcars/train/',\n",
    "                                 transform=get_transform(random_crop=True))\n",
    "test_dataset = CarsDataset('./data/sfcars/test.csv', './data/sfcars/test/',\n",
    "                                 transform=get_transform(random_crop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "tr_loader = data.DataLoader(dataset=train_dataset,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=test_dataset,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,                            \n",
    "                             num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.5, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=100, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][31/32]\tBatch 29.050 (21.525)\tData 28.807 (21.198)\tLoss 5.41836 (5.47678)\tAcc@1   0.96 (  0.58)\tAcc@5   4.33 (  3.40)\tLearningRate 0.02296 (0.02104)\n",
      "0:00:29.218921 elapsed for 1\n",
      "Epoch: [1][31/32]\tBatch 29.020 (21.857)\tData 28.776 (21.520)\tLoss 5.31993 (5.44979)\tAcc@1   1.44 (  0.87)\tAcc@5   3.37 (  3.78)\tLearningRate 0.03178 (0.02403)\n",
      "0:00:29.156131 elapsed for 2\n",
      "Epoch: [2][31/32]\tBatch 27.791 (21.327)\tData 27.546 (20.990)\tLoss 5.22355 (5.37660)\tAcc@1   1.44 (  1.05)\tAcc@5   3.85 (  4.32)\tLearningRate 0.04624 (0.02895)\n",
      "0:00:27.945213 elapsed for 3\n",
      "Epoch: [3][31/32]\tBatch 28.723 (21.350)\tData 28.478 (21.014)\tLoss 5.20060 (5.32589)\tAcc@1   2.40 (  1.21)\tAcc@5   5.77 (  4.80)\tLearningRate 0.06597 (0.03571)\n",
      "0:00:28.870210 elapsed for 4\n",
      "Epoch: [4][31/32]\tBatch 28.701 (21.371)\tData 28.457 (21.034)\tLoss 5.28536 (5.28569)\tAcc@1   1.44 (  1.31)\tAcc@5   6.25 (  5.28)\tLearningRate 0.09050 (0.04422)\n",
      "0:00:28.847990 elapsed for 5\n",
      "Epoch: [5][31/32]\tBatch 29.238 (21.402)\tData 28.995 (21.066)\tLoss 5.10030 (5.25610)\tAcc@1   2.40 (  1.43)\tAcc@5   6.25 (  5.71)\tLearningRate 0.11922 (0.05435)\n",
      "0:00:29.367008 elapsed for 6\n",
      "Epoch: [6][31/32]\tBatch 28.958 (21.543)\tData 28.715 (21.208)\tLoss 5.08793 (5.22789)\tAcc@1   1.92 (  1.58)\tAcc@5   8.17 (  6.17)\tLearningRate 0.15141 (0.06595)\n",
      "0:00:29.089465 elapsed for 7\n",
      "Epoch: [7][31/32]\tBatch 28.302 (21.448)\tData 28.059 (21.113)\tLoss 5.07715 (5.20242)\tAcc@1   2.88 (  1.70)\tAcc@5  10.10 (  6.60)\tLearningRate 0.18628 (0.07886)\n",
      "0:00:28.441402 elapsed for 8\n",
      "Epoch: [8][31/32]\tBatch 28.499 (21.389)\tData 28.255 (21.054)\tLoss 5.04272 (5.18605)\tAcc@1   1.92 (  1.77)\tAcc@5   7.21 (  6.91)\tLearningRate 0.22298 (0.09288)\n",
      "0:00:28.625949 elapsed for 9\n",
      "Epoch: [9][31/32]\tBatch 29.251 (21.437)\tData 29.007 (21.102)\tLoss 4.97895 (5.16666)\tAcc@1   2.40 (  1.85)\tAcc@5  10.10 (  7.27)\tLearningRate 0.26059 (0.10783)\n",
      "0:00:29.375647 elapsed for 10\n",
      "Epoch: [10][31/32]\tBatch 30.189 (21.576)\tData 29.945 (21.242)\tLoss 4.98315 (5.14768)\tAcc@1   0.96 (  1.97)\tAcc@5   8.17 (  7.60)\tLearningRate 0.29819 (0.12348)\n",
      "0:00:30.305824 elapsed for 11\n",
      "Epoch: [11][31/32]\tBatch 29.246 (21.553)\tData 29.000 (21.219)\tLoss 4.89498 (5.13038)\tAcc@1   3.85 (  2.07)\tAcc@5  11.54 (  7.95)\tLearningRate 0.33484 (0.13962)\n",
      "0:00:29.375094 elapsed for 12\n",
      "Epoch: [12][31/32]\tBatch 29.381 (21.556)\tData 29.137 (21.221)\tLoss 4.90772 (5.11380)\tAcc@1   3.37 (  2.17)\tAcc@5  12.50 (  8.28)\tLearningRate 0.36964 (0.15604)\n",
      "0:00:29.513852 elapsed for 13\n",
      "Epoch: [13][31/32]\tBatch 29.725 (21.599)\tData 29.479 (21.265)\tLoss 4.93583 (5.09770)\tAcc@1   4.81 (  2.27)\tAcc@5  13.46 (  8.61)\tLearningRate 0.40174 (0.17249)\n",
      "0:00:29.861351 elapsed for 14\n",
      "Epoch: [14][31/32]\tBatch 29.564 (21.621)\tData 29.320 (21.287)\tLoss 4.95830 (5.08144)\tAcc@1   1.92 (  2.38)\tAcc@5   9.13 (  8.96)\tLearningRate 0.43033 (0.18878)\n",
      "0:00:29.701780 elapsed for 15\n",
      "Epoch: [15][31/32]\tBatch 30.571 (21.697)\tData 30.326 (21.363)\tLoss 5.00497 (5.06447)\tAcc@1   1.92 (  2.51)\tAcc@5   9.62 (  9.30)\tLearningRate 0.45472 (0.20469)\n",
      "0:00:30.701317 elapsed for 16\n",
      "Epoch: [16][31/32]\tBatch 29.776 (21.703)\tData 29.531 (21.369)\tLoss 4.86415 (5.04804)\tAcc@1   2.88 (  2.65)\tAcc@5  12.98 (  9.68)\tLearningRate 0.47430 (0.22001)\n",
      "0:00:29.910665 elapsed for 17\n",
      "Epoch: [17][31/32]\tBatch 28.814 (21.664)\tData 28.569 (21.330)\tLoss 4.79426 (5.03371)\tAcc@1   3.37 (  2.75)\tAcc@5  15.87 (  9.98)\tLearningRate 0.48858 (0.23457)\n",
      "0:00:28.953050 elapsed for 18\n",
      "Epoch: [18][31/32]\tBatch 28.861 (21.653)\tData 28.616 (21.318)\tLoss 4.86853 (5.01983)\tAcc@1   2.88 (  2.84)\tAcc@5  14.42 ( 10.24)\tLearningRate 0.49722 (0.24820)\n",
      "0:00:29.003454 elapsed for 19\n",
      "Epoch: [19][31/32]\tBatch 29.521 (21.679)\tData 29.276 (21.345)\tLoss 4.76768 (5.00373)\tAcc@1   4.81 (  2.96)\tAcc@5  18.27 ( 10.59)\tLearningRate 0.50000 (0.26075)\n",
      "0:00:29.666084 elapsed for 20\n",
      "Epoch: [20][31/32]\tBatch 28.873 (21.655)\tData 28.628 (21.321)\tLoss 4.56339 (4.98738)\tAcc@1   6.25 (  3.09)\tAcc@5  23.08 ( 10.92)\tLearningRate 0.49980 (0.27214)\n",
      "0:00:28.999557 elapsed for 21\n",
      "Epoch: [21][31/32]\tBatch 29.652 (21.665)\tData 29.407 (21.331)\tLoss 4.56628 (4.96975)\tAcc@1   8.65 (  3.22)\tAcc@5  20.67 ( 11.30)\tLearningRate 0.49921 (0.28247)\n",
      "0:00:29.783354 elapsed for 22\n",
      "Epoch: [22][31/32]\tBatch 29.650 (21.703)\tData 29.406 (21.369)\tLoss 4.69291 (4.95178)\tAcc@1   5.29 (  3.35)\tAcc@5  17.79 ( 11.68)\tLearningRate 0.49823 (0.29188)\n",
      "0:00:29.780080 elapsed for 23\n",
      "Epoch: [23][31/32]\tBatch 29.048 (21.689)\tData 28.804 (21.354)\tLoss 4.56575 (4.93294)\tAcc@1   8.65 (  3.53)\tAcc@5  20.67 ( 12.10)\tLearningRate 0.49687 (0.30045)\n",
      "0:00:29.173736 elapsed for 24\n",
      "Epoch: [24][31/32]\tBatch 29.381 (21.696)\tData 29.136 (21.362)\tLoss 4.47477 (4.91313)\tAcc@1   7.21 (  3.71)\tAcc@5  21.63 ( 12.54)\tLearningRate 0.49514 (0.30827)\n",
      "0:00:29.516762 elapsed for 25\n",
      "Epoch: [25][31/32]\tBatch 29.105 (21.705)\tData 28.860 (21.371)\tLoss 4.27859 (4.89238)\tAcc@1   8.17 (  3.87)\tAcc@5  25.00 ( 13.00)\tLearningRate 0.49302 (0.31542)\n",
      "0:00:29.240074 elapsed for 26\n",
      "Epoch: [26][31/32]\tBatch 30.378 (21.755)\tData 30.134 (21.421)\tLoss 4.31698 (4.87126)\tAcc@1  12.02 (  4.07)\tAcc@5  26.92 ( 13.48)\tLearningRate 0.49053 (0.32195)\n",
      "0:00:30.525201 elapsed for 27\n",
      "Epoch: [27][31/32]\tBatch 29.708 (21.787)\tData 29.464 (21.453)\tLoss 4.19478 (4.84958)\tAcc@1   8.65 (  4.29)\tAcc@5  26.92 ( 13.97)\tLearningRate 0.48767 (0.32792)\n",
      "0:00:29.845218 elapsed for 28\n",
      "Epoch: [28][31/32]\tBatch 29.918 (21.785)\tData 29.673 (21.451)\tLoss 4.08829 (4.82640)\tAcc@1  11.06 (  4.53)\tAcc@5  28.85 ( 14.49)\tLearningRate 0.48444 (0.33337)\n",
      "0:00:30.060857 elapsed for 29\n",
      "Epoch: [29][31/32]\tBatch 28.583 (21.773)\tData 28.338 (21.439)\tLoss 4.05841 (4.80313)\tAcc@1  12.50 (  4.75)\tAcc@5  32.69 ( 15.03)\tLearningRate 0.48085 (0.33834)\n",
      "0:00:28.715620 elapsed for 30\n",
      "Epoch: [30][31/32]\tBatch 29.373 (21.761)\tData 29.127 (21.427)\tLoss 4.15747 (4.77763)\tAcc@1  10.10 (  4.99)\tAcc@5  31.73 ( 15.62)\tLearningRate 0.47691 (0.34288)\n",
      "0:00:29.510656 elapsed for 31\n",
      "Epoch: [31][31/32]\tBatch 29.613 (21.770)\tData 29.368 (21.436)\tLoss 4.11645 (4.75137)\tAcc@1   9.62 (  5.24)\tAcc@5  31.25 ( 16.23)\tLearningRate 0.47261 (0.34700)\n",
      "0:00:29.760853 elapsed for 32\n",
      "Epoch: [32][31/32]\tBatch 29.761 (21.795)\tData 29.517 (21.461)\tLoss 3.77060 (4.72431)\tAcc@1  14.42 (  5.53)\tAcc@5  35.10 ( 16.87)\tLearningRate 0.46797 (0.35073)\n",
      "0:00:29.890365 elapsed for 33\n",
      "Epoch: [33][31/32]\tBatch 30.081 (21.821)\tData 29.835 (21.487)\tLoss 3.88193 (4.69522)\tAcc@1  15.38 (  5.85)\tAcc@5  37.98 ( 17.57)\tLearningRate 0.46300 (0.35411)\n",
      "0:00:30.214876 elapsed for 34\n",
      "Epoch: [34][31/32]\tBatch 29.659 (21.815)\tData 29.414 (21.481)\tLoss 3.75765 (4.66657)\tAcc@1  12.98 (  6.16)\tAcc@5  39.90 ( 18.25)\tLearningRate 0.45770 (0.35714)\n",
      "0:00:29.791601 elapsed for 35\n",
      "Epoch: [35][31/32]\tBatch 29.326 (21.802)\tData 29.081 (21.468)\tLoss 3.56082 (4.63585)\tAcc@1  20.19 (  6.54)\tAcc@5  43.75 ( 18.96)\tLearningRate 0.45207 (0.35985)\n",
      "0:00:29.458129 elapsed for 36\n",
      "Epoch: [36][31/32]\tBatch 29.797 (21.813)\tData 29.553 (21.479)\tLoss 3.41976 (4.60440)\tAcc@1  19.23 (  6.90)\tAcc@5  49.52 ( 19.72)\tLearningRate 0.44614 (0.36226)\n",
      "0:00:29.934116 elapsed for 37\n",
      "Epoch: [37][31/32]\tBatch 30.039 (21.826)\tData 29.795 (21.492)\tLoss 3.39222 (4.57280)\tAcc@1  24.04 (  7.29)\tAcc@5  47.60 ( 20.48)\tLearningRate 0.43990 (0.36439)\n",
      "0:00:30.173639 elapsed for 38\n",
      "Epoch: [38][31/32]\tBatch 29.262 (21.827)\tData 29.017 (21.492)\tLoss 3.10201 (4.53884)\tAcc@1  24.04 (  7.74)\tAcc@5  50.48 ( 21.29)\tLearningRate 0.43337 (0.36624)\n",
      "0:00:29.401352 elapsed for 39\n",
      "Epoch: [39][31/32]\tBatch 29.206 (21.815)\tData 28.962 (21.481)\tLoss 3.12613 (4.50429)\tAcc@1  27.40 (  8.19)\tAcc@5  56.73 ( 22.11)\tLearningRate 0.42656 (0.36783)\n",
      "0:00:29.337334 elapsed for 40\n",
      "Epoch: [40][31/32]\tBatch 29.464 (21.816)\tData 29.219 (21.482)\tLoss 2.92894 (4.46669)\tAcc@1  29.81 (  8.70)\tAcc@5  57.69 ( 22.99)\tLearningRate 0.41948 (0.36917)\n",
      "0:00:29.602997 elapsed for 41\n",
      "Epoch: [41][31/32]\tBatch 29.467 (21.819)\tData 29.222 (21.485)\tLoss 2.67219 (4.42914)\tAcc@1  35.10 (  9.24)\tAcc@5  69.71 ( 23.86)\tLearningRate 0.41213 (0.37028)\n",
      "0:00:29.597433 elapsed for 42\n",
      "Epoch: [42][31/32]\tBatch 29.484 (21.820)\tData 29.239 (21.486)\tLoss 2.76881 (4.39117)\tAcc@1  35.58 (  9.79)\tAcc@5  62.02 ( 24.75)\tLearningRate 0.40453 (0.37116)\n",
      "0:00:29.615401 elapsed for 43\n",
      "Epoch: [43][31/32]\tBatch 29.791 (21.847)\tData 29.546 (21.513)\tLoss 2.95884 (4.35285)\tAcc@1  30.29 ( 10.35)\tAcc@5  56.25 ( 25.64)\tLearningRate 0.39670 (0.37183)\n",
      "0:00:29.923555 elapsed for 44\n",
      "Epoch: [44][31/32]\tBatch 30.061 (21.842)\tData 29.817 (21.508)\tLoss 2.72341 (4.31386)\tAcc@1  35.58 ( 10.96)\tAcc@5  62.98 ( 26.55)\tLearningRate 0.38864 (0.37229)\n",
      "0:00:30.198867 elapsed for 45\n",
      "Epoch: [45][31/32]\tBatch 29.275 (21.827)\tData 29.030 (21.493)\tLoss 2.51452 (4.27354)\tAcc@1  40.38 ( 11.58)\tAcc@5  63.94 ( 27.46)\tLearningRate 0.38036 (0.37255)\n",
      "0:00:29.402948 elapsed for 46\n",
      "Epoch: [46][31/32]\tBatch 29.307 (21.819)\tData 29.061 (21.485)\tLoss 2.45223 (4.23285)\tAcc@1  42.79 ( 12.24)\tAcc@5  63.46 ( 28.37)\tLearningRate 0.37189 (0.37263)\n",
      "0:00:29.446115 elapsed for 47\n",
      "Epoch: [47][31/32]\tBatch 28.809 (21.811)\tData 28.565 (21.477)\tLoss 2.41048 (4.19260)\tAcc@1  38.94 ( 12.88)\tAcc@5  69.71 ( 29.27)\tLearningRate 0.36322 (0.37252)\n",
      "0:00:28.950767 elapsed for 48\n",
      "Epoch: [48][31/32]\tBatch 29.639 (21.808)\tData 29.394 (21.473)\tLoss 2.21392 (4.15189)\tAcc@1  46.63 ( 13.57)\tAcc@5  75.00 ( 30.18)\tLearningRate 0.35439 (0.37224)\n",
      "0:00:29.778884 elapsed for 49\n",
      "Epoch: [49][31/32]\tBatch 29.373 (21.797)\tData 29.128 (21.462)\tLoss 2.04140 (4.11086)\tAcc@1  44.23 ( 14.24)\tAcc@5  79.33 ( 31.08)\tLearningRate 0.34539 (0.37179)\n",
      "0:00:29.502661 elapsed for 50\n",
      "Epoch: [50][31/32]\tBatch 29.893 (21.806)\tData 29.648 (21.472)\tLoss 1.96360 (4.06921)\tAcc@1  52.40 ( 14.95)\tAcc@5  79.33 ( 31.98)\tLearningRate 0.33624 (0.37118)\n",
      "0:00:30.027657 elapsed for 51\n",
      "Epoch: [51][31/32]\tBatch 29.487 (21.796)\tData 29.242 (21.462)\tLoss 1.98912 (4.02922)\tAcc@1  49.52 ( 15.64)\tAcc@5  78.37 ( 32.85)\tLearningRate 0.32696 (0.37041)\n",
      "0:00:29.626116 elapsed for 52\n",
      "Epoch: [52][31/32]\tBatch 29.626 (21.806)\tData 29.382 (21.472)\tLoss 1.94176 (3.98967)\tAcc@1  52.88 ( 16.33)\tAcc@5  80.29 ( 33.70)\tLearningRate 0.31757 (0.36950)\n",
      "0:00:29.769002 elapsed for 53\n",
      "Epoch: [53][31/32]\tBatch 29.399 (21.802)\tData 29.154 (21.468)\tLoss 1.93757 (3.95029)\tAcc@1  51.92 ( 17.02)\tAcc@5  76.92 ( 34.54)\tLearningRate 0.30806 (0.36845)\n",
      "0:00:29.539497 elapsed for 54\n",
      "Epoch: [54][31/32]\tBatch 30.379 (21.818)\tData 30.134 (21.484)\tLoss 1.73551 (3.91040)\tAcc@1  53.37 ( 17.72)\tAcc@5  80.77 ( 35.38)\tLearningRate 0.29847 (0.36726)\n",
      "0:00:30.522705 elapsed for 55\n",
      "Epoch: [55][31/32]\tBatch 30.603 (21.834)\tData 30.359 (21.499)\tLoss 1.88563 (3.87127)\tAcc@1  51.92 ( 18.42)\tAcc@5  81.25 ( 36.20)\tLearningRate 0.28881 (0.36595)\n",
      "0:00:30.735586 elapsed for 56\n",
      "Epoch: [56][31/32]\tBatch 29.985 (21.834)\tData 29.739 (21.500)\tLoss 1.72150 (3.83276)\tAcc@1  56.73 ( 19.09)\tAcc@5  80.29 ( 37.00)\tLearningRate 0.27908 (0.36450)\n",
      "0:00:30.111176 elapsed for 57\n",
      "Epoch: [57][31/32]\tBatch 28.942 (21.824)\tData 28.697 (21.489)\tLoss 1.43881 (3.79480)\tAcc@1  63.94 ( 19.77)\tAcc@5  86.54 ( 37.78)\tLearningRate 0.26931 (0.36294)\n",
      "0:00:29.087479 elapsed for 58\n",
      "Epoch: [58][31/32]\tBatch 29.512 (21.827)\tData 29.266 (21.492)\tLoss 1.68871 (3.75750)\tAcc@1  57.21 ( 20.45)\tAcc@5  82.21 ( 38.54)\tLearningRate 0.25951 (0.36127)\n",
      "0:00:29.646303 elapsed for 59\n",
      "Epoch: [59][31/32]\tBatch 29.455 (21.819)\tData 29.210 (21.485)\tLoss 1.84685 (3.72030)\tAcc@1  49.52 ( 21.14)\tAcc@5  80.77 ( 39.29)\tLearningRate 0.24969 (0.35949)\n",
      "0:00:29.594120 elapsed for 60\n",
      "Epoch: [60][31/32]\tBatch 28.922 (21.813)\tData 28.677 (21.479)\tLoss 1.50503 (3.68337)\tAcc@1  64.42 ( 21.83)\tAcc@5  83.17 ( 40.04)\tLearningRate 0.23988 (0.35761)\n",
      "0:00:29.063777 elapsed for 61\n",
      "Epoch: [61][31/32]\tBatch 28.123 (21.793)\tData 27.879 (21.459)\tLoss 1.71323 (3.64711)\tAcc@1  55.29 ( 22.50)\tAcc@5  81.25 ( 40.76)\tLearningRate 0.23008 (0.35563)\n",
      "0:00:28.249452 elapsed for 62\n",
      "Epoch: [62][31/32]\tBatch 28.980 (21.805)\tData 28.736 (21.471)\tLoss 1.37938 (3.61129)\tAcc@1  63.46 ( 23.17)\tAcc@5  85.10 ( 41.48)\tLearningRate 0.22031 (0.35356)\n",
      "0:00:29.101174 elapsed for 63\n",
      "Epoch: [63][31/32]\tBatch 27.734 (21.782)\tData 27.489 (21.448)\tLoss 1.40568 (3.57609)\tAcc@1  65.87 ( 23.83)\tAcc@5  84.62 ( 42.19)\tLearningRate 0.21059 (0.35139)\n",
      "0:00:27.878391 elapsed for 64\n",
      "Epoch: [64][31/32]\tBatch 28.668 (21.776)\tData 28.423 (21.441)\tLoss 1.53135 (3.54135)\tAcc@1  62.98 ( 24.49)\tAcc@5  82.21 ( 42.87)\tLearningRate 0.20093 (0.34915)\n",
      "0:00:28.798679 elapsed for 65\n",
      "Epoch: [65][31/32]\tBatch 27.983 (21.759)\tData 27.739 (21.425)\tLoss 1.22866 (3.50668)\tAcc@1  65.38 ( 25.16)\tAcc@5  90.38 ( 43.55)\tLearningRate 0.19134 (0.34683)\n",
      "0:00:28.113372 elapsed for 66\n",
      "Epoch: [66][31/32]\tBatch 28.614 (21.748)\tData 28.370 (21.414)\tLoss 1.19228 (3.47321)\tAcc@1  71.63 ( 25.80)\tAcc@5  87.50 ( 44.21)\tLearningRate 0.18185 (0.34444)\n",
      "0:00:28.750138 elapsed for 67\n",
      "Epoch: [67][31/32]\tBatch 28.007 (21.733)\tData 27.762 (21.399)\tLoss 1.44840 (3.44027)\tAcc@1  66.35 ( 26.44)\tAcc@5  82.69 ( 44.85)\tLearningRate 0.17246 (0.34197)\n",
      "0:00:28.136648 elapsed for 68\n",
      "Epoch: [68][31/32]\tBatch 29.814 (21.737)\tData 29.569 (21.403)\tLoss 1.10948 (3.40761)\tAcc@1  70.19 ( 27.07)\tAcc@5  90.38 ( 45.49)\tLearningRate 0.16318 (0.33945)\n",
      "0:00:29.940938 elapsed for 69\n",
      "Epoch: [69][31/32]\tBatch 28.386 (21.716)\tData 28.141 (21.382)\tLoss 1.25593 (3.37472)\tAcc@1  69.23 ( 27.71)\tAcc@5  88.46 ( 46.12)\tLearningRate 0.15405 (0.33686)\n",
      "0:00:28.514029 elapsed for 70\n",
      "Epoch: [70][31/32]\tBatch 28.148 (21.698)\tData 27.902 (21.364)\tLoss 1.04923 (3.34246)\tAcc@1  72.12 ( 28.35)\tAcc@5  90.87 ( 46.73)\tLearningRate 0.14506 (0.33422)\n",
      "0:00:28.264321 elapsed for 71\n",
      "Epoch: [71][31/32]\tBatch 28.219 (21.691)\tData 27.975 (21.357)\tLoss 0.97391 (3.31012)\tAcc@1  75.96 ( 29.00)\tAcc@5  89.90 ( 47.34)\tLearningRate 0.13623 (0.33153)\n",
      "0:00:28.351152 elapsed for 72\n",
      "Epoch: [72][31/32]\tBatch 28.617 (21.675)\tData 28.373 (21.342)\tLoss 1.12363 (3.27863)\tAcc@1  70.19 ( 29.63)\tAcc@5  88.94 ( 47.93)\tLearningRate 0.12758 (0.32880)\n",
      "0:00:28.746801 elapsed for 73\n",
      "Epoch: [73][31/32]\tBatch 28.250 (21.661)\tData 28.006 (21.327)\tLoss 1.01221 (3.24739)\tAcc@1  71.63 ( 30.26)\tAcc@5  90.38 ( 48.51)\tLearningRate 0.11912 (0.32602)\n",
      "0:00:28.384516 elapsed for 74\n",
      "Epoch: [74][31/32]\tBatch 28.955 (21.665)\tData 28.709 (21.331)\tLoss 0.91519 (3.21669)\tAcc@1  79.33 ( 30.88)\tAcc@5  92.31 ( 49.08)\tLearningRate 0.11085 (0.32320)\n",
      "0:00:29.096933 elapsed for 75\n",
      "Epoch: [75][31/32]\tBatch 28.239 (21.651)\tData 27.995 (21.318)\tLoss 1.00031 (3.18626)\tAcc@1  75.96 ( 31.50)\tAcc@5  90.38 ( 49.64)\tLearningRate 0.10281 (0.32035)\n",
      "0:00:28.368989 elapsed for 76\n",
      "Epoch: [76][31/32]\tBatch 27.614 (21.638)\tData 27.369 (21.304)\tLoss 0.92019 (3.15662)\tAcc@1  79.33 ( 32.10)\tAcc@5  90.87 ( 50.18)\tLearningRate 0.09499 (0.31747)\n",
      "0:00:27.755624 elapsed for 77\n",
      "Epoch: [77][31/32]\tBatch 29.018 (21.639)\tData 28.772 (21.305)\tLoss 1.01345 (3.12731)\tAcc@1  76.44 ( 32.69)\tAcc@5  90.38 ( 50.72)\tLearningRate 0.08741 (0.31457)\n",
      "0:00:29.140186 elapsed for 78\n",
      "Epoch: [78][31/32]\tBatch 28.159 (21.630)\tData 27.913 (21.296)\tLoss 0.76177 (3.09820)\tAcc@1  80.29 ( 33.29)\tAcc@5  93.75 ( 51.24)\tLearningRate 0.08008 (0.31165)\n",
      "0:00:28.293511 elapsed for 79\n",
      "Epoch: [79][31/32]\tBatch 28.009 (21.619)\tData 27.764 (21.285)\tLoss 0.74801 (3.06979)\tAcc@1  81.25 ( 33.88)\tAcc@5  93.75 ( 51.75)\tLearningRate 0.07301 (0.30871)\n",
      "0:00:28.134898 elapsed for 80\n",
      "Epoch: [80][31/32]\tBatch 28.115 (21.612)\tData 27.872 (21.278)\tLoss 0.81070 (3.04118)\tAcc@1  83.65 ( 34.47)\tAcc@5  93.75 ( 52.26)\tLearningRate 0.06621 (0.30575)\n",
      "0:00:28.241962 elapsed for 81\n",
      "Epoch: [81][31/32]\tBatch 28.746 (21.604)\tData 28.501 (21.270)\tLoss 0.70778 (3.01299)\tAcc@1  85.10 ( 35.05)\tAcc@5  93.75 ( 52.77)\tLearningRate 0.05970 (0.30279)\n",
      "0:00:28.879885 elapsed for 82\n",
      "Epoch: [82][31/32]\tBatch 28.250 (21.595)\tData 28.007 (21.261)\tLoss 0.61247 (2.98498)\tAcc@1  86.06 ( 35.64)\tAcc@5  94.71 ( 53.26)\tLearningRate 0.05348 (0.29982)\n",
      "0:00:28.372395 elapsed for 83\n",
      "Epoch: [83][31/32]\tBatch 28.965 (21.600)\tData 28.721 (21.266)\tLoss 0.76075 (2.95739)\tAcc@1  79.33 ( 36.22)\tAcc@5  93.27 ( 53.75)\tLearningRate 0.04757 (0.29685)\n",
      "0:00:29.087145 elapsed for 84\n",
      "Epoch: [84][31/32]\tBatch 27.856 (21.584)\tData 27.612 (21.250)\tLoss 0.68897 (2.92998)\tAcc@1  85.10 ( 36.79)\tAcc@5  92.31 ( 54.22)\tLearningRate 0.04196 (0.29389)\n",
      "0:00:27.985142 elapsed for 85\n",
      "Epoch: [85][31/32]\tBatch 28.854 (21.576)\tData 28.608 (21.242)\tLoss 0.56749 (2.90293)\tAcc@1  87.02 ( 37.36)\tAcc@5  93.27 ( 54.69)\tLearningRate 0.03668 (0.29093)\n",
      "0:00:28.979657 elapsed for 86\n",
      "Epoch: [86][31/32]\tBatch 28.288 (21.564)\tData 28.044 (21.230)\tLoss 0.53703 (2.87624)\tAcc@1  88.46 ( 37.92)\tAcc@5  95.67 ( 55.16)\tLearningRate 0.03173 (0.28797)\n",
      "0:00:28.416378 elapsed for 87\n",
      "Epoch: [87][31/32]\tBatch 29.226 (21.561)\tData 28.980 (21.227)\tLoss 0.73795 (2.84968)\tAcc@1  83.65 ( 38.49)\tAcc@5  94.23 ( 55.61)\tLearningRate 0.02711 (0.28504)\n",
      "0:00:29.368159 elapsed for 88\n",
      "Epoch: [88][31/32]\tBatch 28.226 (21.547)\tData 27.982 (21.214)\tLoss 0.67801 (2.82362)\tAcc@1  85.58 ( 39.05)\tAcc@5  91.35 ( 56.05)\tLearningRate 0.02284 (0.28211)\n",
      "0:00:28.354154 elapsed for 89\n",
      "Epoch: [89][31/32]\tBatch 28.756 (21.548)\tData 28.512 (21.215)\tLoss 0.50041 (2.79809)\tAcc@1  89.90 ( 39.59)\tAcc@5  96.15 ( 56.49)\tLearningRate 0.01891 (0.27921)\n",
      "0:00:28.883979 elapsed for 90\n",
      "Epoch: [90][31/32]\tBatch 29.096 (21.551)\tData 28.853 (21.217)\tLoss 0.59742 (2.77265)\tAcc@1  85.10 ( 40.13)\tAcc@5  93.27 ( 56.92)\tLearningRate 0.01535 (0.27633)\n",
      "0:00:29.226563 elapsed for 91\n",
      "Epoch: [91][31/32]\tBatch 29.190 (21.566)\tData 28.945 (21.232)\tLoss 0.49734 (2.74749)\tAcc@1  88.94 ( 40.67)\tAcc@5  95.67 ( 57.34)\tLearningRate 0.01214 (0.27347)\n",
      "0:00:29.315175 elapsed for 92\n",
      "Epoch: [92][31/32]\tBatch 28.167 (21.555)\tData 27.922 (21.222)\tLoss 0.34065 (2.72270)\tAcc@1  91.83 ( 41.20)\tAcc@5  98.08 ( 57.75)\tLearningRate 0.00931 (0.27065)\n",
      "0:00:28.299978 elapsed for 93\n",
      "Epoch: [93][31/32]\tBatch 28.548 (21.550)\tData 28.303 (21.216)\tLoss 0.47443 (2.69827)\tAcc@1  90.38 ( 41.72)\tAcc@5  93.27 ( 58.16)\tLearningRate 0.00684 (0.26785)\n",
      "0:00:28.677773 elapsed for 94\n",
      "Epoch: [94][31/32]\tBatch 29.090 (21.554)\tData 28.844 (21.220)\tLoss 0.34334 (2.67417)\tAcc@1  93.75 ( 42.24)\tAcc@5  98.08 ( 58.57)\tLearningRate 0.00475 (0.26509)\n",
      "0:00:29.236170 elapsed for 95\n",
      "Epoch: [95][31/32]\tBatch 29.115 (21.550)\tData 28.871 (21.216)\tLoss 0.49949 (2.65052)\tAcc@1  91.35 ( 42.75)\tAcc@5  94.71 ( 58.96)\tLearningRate 0.00303 (0.26237)\n",
      "0:00:29.246260 elapsed for 96\n",
      "Epoch: [96][31/32]\tBatch 29.498 (21.555)\tData 29.253 (21.221)\tLoss 0.34163 (2.62711)\tAcc@1  91.83 ( 43.25)\tAcc@5  97.60 ( 59.35)\tLearningRate 0.00170 (0.25969)\n",
      "0:00:29.628321 elapsed for 97\n",
      "Epoch: [97][31/32]\tBatch 29.996 (21.569)\tData 29.752 (21.236)\tLoss 0.32285 (2.60432)\tAcc@1  92.79 ( 43.74)\tAcc@5  98.56 ( 59.72)\tLearningRate 0.00075 (0.25705)\n",
      "0:00:30.125921 elapsed for 98\n",
      "Epoch: [98][31/32]\tBatch 29.140 (21.576)\tData 28.896 (21.243)\tLoss 0.28667 (2.58191)\tAcc@1  92.79 ( 44.22)\tAcc@5  97.12 ( 60.10)\tLearningRate 0.00018 (0.25446)\n",
      "0:00:29.271998 elapsed for 99\n",
      "Epoch: [99][31/32]\tBatch 28.683 (21.577)\tData 28.439 (21.243)\tLoss 0.45661 (2.56008)\tAcc@1  90.38 ( 44.69)\tAcc@5  94.23 ( 60.45)\tLearningRate 0.00000 (0.25192)\n",
      "0:00:28.818388 elapsed for 100\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    progress.display(i)            \n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7744061683870166"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_val(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7026489242631514"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_retrieval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/sfcar_darknet25_ep100_lr05.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
