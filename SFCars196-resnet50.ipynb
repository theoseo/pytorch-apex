{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from torch import nn, optim\n",
    "from resnet import resnet50\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpus = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23909636"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transform(random_crop=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n",
    "        #[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(256))\n",
    "    if random_crop:\n",
    "        #transform.append(transforms.RandomRotation(30))\n",
    "        transform.append(transforms.RandomResizedCrop(224))\n",
    "        transform.append(transforms.RandomHorizontalFlip())\n",
    "        transform.append(transforms.ColorJitter(hue=.05, saturation=.05),)\n",
    "    else:\n",
    "        transform.append(transforms.CenterCrop(224))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(normalize)\n",
    "    return transforms.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "class CarsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \n",
    "        self.pd_csv = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pd_csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "    \n",
    "        img_name = os.path.join(self.root_dir, \n",
    "                                self.pd_csv.iloc[index, 1])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        target = self.pd_csv.iloc[index, 0]\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CarsDataset('./data/sfcars/train.csv', './data/sfcars/train/',\n",
    "                                 transform=get_transform(random_crop=True))\n",
    "test_dataset = CarsDataset('./data/sfcars/test.csv', './data/sfcars/test/',\n",
    "                                 transform=get_transform(random_crop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "tr_loader = data.DataLoader(dataset=train_dataset,\n",
    "                            batch_size=256,\n",
    "                            #sampler = RandomIdentitySampler(train_set, 4),\n",
    "                            shuffle=True,\n",
    "                            num_workers=16)\n",
    "\n",
    "val_loader = data.DataLoader(dataset=test_dataset,\n",
    "                             batch_size=256,\n",
    "                             shuffle=False,                            \n",
    "                             num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using apex synced BN\n"
     ]
    }
   ],
   "source": [
    "import apex\n",
    "print(\"using apex synced BN\")\n",
    "model = apex.parallel.convert_syncbn_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(tr_loader)\n",
    "                                                , epochs=100, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "from apex import amp, optimizers\n",
    "\n",
    "model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level='O3',keep_batchnorm_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    inputs, labels = next(iter(tr_loader))\n",
    "    print(1)\n",
    "    inputs = inputs.cuda(non_blocking=True)        \n",
    "    labels = labels.cuda(non_blocking=True)    \n",
    "    print(2)    \n",
    "    logits = model(inputs)\n",
    "    print(3)                       \n",
    "    loss = criterion(logits, labels)                   \n",
    "    print(4)                   \n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(5)                            \n",
    "    model.zero_grad()\n",
    "    print(10)                                \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][31/32]\tBatch 30.944 (22.541)\tData 30.646 (22.149)\tLoss 5.33298 (5.35310)\tAcc@1   0.00 (  0.58)\tAcc@5   1.44 (  2.50)\tLearningRate 0.00459 (0.00421)\n",
      "0:00:31.077045 elapsed for 1\n",
      "Epoch: [1][31/32]\tBatch 30.966 (22.607)\tData 30.667 (22.207)\tLoss 5.36528 (5.33306)\tAcc@1   0.00 (  0.60)\tAcc@5   1.44 (  2.85)\tLearningRate 0.00636 (0.00481)\n",
      "0:00:31.100765 elapsed for 2\n",
      "Epoch: [2][31/32]\tBatch 30.836 (22.497)\tData 30.538 (22.097)\tLoss 5.33622 (5.32884)\tAcc@1   1.44 (  0.71)\tAcc@5   5.29 (  3.25)\tLearningRate 0.00925 (0.00579)\n",
      "0:00:30.957811 elapsed for 3\n",
      "Epoch: [3][31/32]\tBatch 29.484 (22.276)\tData 29.185 (21.872)\tLoss 5.28964 (5.32946)\tAcc@1   0.00 (  0.76)\tAcc@5   2.88 (  3.51)\tLearningRate 0.01319 (0.00714)\n",
      "0:00:29.617346 elapsed for 4\n",
      "Epoch: [4][31/32]\tBatch 30.824 (22.239)\tData 30.526 (21.837)\tLoss 5.24532 (5.31708)\tAcc@1   1.44 (  0.89)\tAcc@5   7.69 (  3.89)\tLearningRate 0.01810 (0.00884)\n",
      "0:00:30.953374 elapsed for 5\n",
      "Epoch: [5][31/32]\tBatch 29.777 (22.165)\tData 29.478 (21.764)\tLoss 5.17746 (5.30289)\tAcc@1   1.44 (  0.97)\tAcc@5   6.73 (  4.17)\tLearningRate 0.02384 (0.01087)\n",
      "0:00:29.894179 elapsed for 6\n",
      "Epoch: [6][31/32]\tBatch 28.990 (22.050)\tData 28.692 (21.649)\tLoss 5.17800 (5.28352)\tAcc@1   3.37 (  1.08)\tAcc@5   6.25 (  4.52)\tLearningRate 0.03028 (0.01319)\n",
      "0:00:29.110447 elapsed for 7\n",
      "Epoch: [7][31/32]\tBatch 30.014 (21.971)\tData 29.717 (21.568)\tLoss 5.07606 (5.26435)\tAcc@1   2.88 (  1.19)\tAcc@5   8.65 (  4.91)\tLearningRate 0.03726 (0.01577)\n",
      "0:00:30.124723 elapsed for 8\n",
      "Epoch: [8][31/32]\tBatch 30.368 (22.002)\tData 30.070 (21.599)\tLoss 5.08751 (5.24433)\tAcc@1   1.92 (  1.31)\tAcc@5   7.69 (  5.30)\tLearningRate 0.04460 (0.01858)\n",
      "0:00:30.476353 elapsed for 9\n",
      "Epoch: [9][31/32]\tBatch 29.410 (21.954)\tData 29.112 (21.551)\tLoss 5.00022 (5.22330)\tAcc@1   2.40 (  1.43)\tAcc@5   8.65 (  5.68)\tLearningRate 0.05212 (0.02157)\n",
      "0:00:29.524643 elapsed for 10\n",
      "Epoch: [10][31/32]\tBatch 30.110 (22.022)\tData 29.813 (21.620)\tLoss 5.03093 (5.20233)\tAcc@1   2.40 (  1.53)\tAcc@5   9.62 (  6.07)\tLearningRate 0.05964 (0.02470)\n",
      "0:00:30.227876 elapsed for 11\n",
      "Epoch: [11][31/32]\tBatch 30.362 (22.085)\tData 30.062 (21.683)\tLoss 4.90085 (5.18262)\tAcc@1   3.85 (  1.64)\tAcc@5  12.02 (  6.44)\tLearningRate 0.06697 (0.02792)\n",
      "0:00:30.480704 elapsed for 12\n",
      "Epoch: [12][31/32]\tBatch 29.887 (22.078)\tData 29.589 (21.677)\tLoss 4.99365 (5.16346)\tAcc@1   1.44 (  1.75)\tAcc@5   9.62 (  6.79)\tLearningRate 0.07393 (0.03121)\n",
      "0:00:30.009329 elapsed for 13\n",
      "Epoch: [13][31/32]\tBatch 31.341 (22.161)\tData 31.042 (21.760)\tLoss 4.91830 (5.14343)\tAcc@1   3.85 (  1.88)\tAcc@5  11.54 (  7.20)\tLearningRate 0.08035 (0.03450)\n",
      "0:00:31.454043 elapsed for 14\n",
      "Epoch: [14][31/32]\tBatch 30.558 (22.199)\tData 30.260 (21.797)\tLoss 4.72680 (5.12336)\tAcc@1   3.37 (  2.01)\tAcc@5  12.50 (  7.60)\tLearningRate 0.08607 (0.03776)\n",
      "0:00:30.703121 elapsed for 15\n",
      "Epoch: [15][31/32]\tBatch 30.837 (22.217)\tData 30.538 (21.815)\tLoss 4.82421 (5.10164)\tAcc@1   1.92 (  2.15)\tAcc@5  12.50 (  8.03)\tLearningRate 0.09094 (0.04094)\n",
      "0:00:30.952979 elapsed for 16\n",
      "Epoch: [16][31/32]\tBatch 30.757 (22.233)\tData 30.460 (21.831)\tLoss 4.68427 (5.08041)\tAcc@1   4.81 (  2.30)\tAcc@5  13.46 (  8.47)\tLearningRate 0.09486 (0.04400)\n",
      "0:00:30.867568 elapsed for 17\n",
      "Epoch: [17][31/32]\tBatch 29.942 (22.216)\tData 29.643 (21.813)\tLoss 4.78441 (5.05851)\tAcc@1   5.29 (  2.46)\tAcc@5  15.38 (  8.94)\tLearningRate 0.09772 (0.04691)\n",
      "0:00:30.050286 elapsed for 18\n",
      "Epoch: [18][31/32]\tBatch 29.912 (22.199)\tData 29.613 (21.796)\tLoss 4.71184 (5.04007)\tAcc@1   5.77 (  2.60)\tAcc@5  14.90 (  9.31)\tLearningRate 0.09944 (0.04964)\n",
      "0:00:30.034641 elapsed for 19\n",
      "Epoch: [19][31/32]\tBatch 29.532 (22.178)\tData 29.234 (21.775)\tLoss 4.57491 (5.01949)\tAcc@1   4.81 (  2.75)\tAcc@5  17.79 (  9.74)\tLearningRate 0.10000 (0.05215)\n",
      "0:00:29.648085 elapsed for 20\n",
      "Epoch: [20][31/32]\tBatch 29.664 (22.171)\tData 29.366 (21.768)\tLoss 4.58327 (4.99679)\tAcc@1   4.81 (  2.92)\tAcc@5  19.71 ( 10.24)\tLearningRate 0.09996 (0.05443)\n",
      "0:00:29.789376 elapsed for 21\n",
      "Epoch: [21][31/32]\tBatch 30.084 (22.161)\tData 29.785 (21.758)\tLoss 4.65354 (4.97386)\tAcc@1   4.33 (  3.11)\tAcc@5  15.87 ( 10.73)\tLearningRate 0.09984 (0.05649)\n",
      "0:00:30.198260 elapsed for 22\n",
      "Epoch: [22][31/32]\tBatch 30.477 (22.193)\tData 30.178 (21.790)\tLoss 4.55966 (4.94867)\tAcc@1   8.65 (  3.31)\tAcc@5  19.23 ( 11.27)\tLearningRate 0.09965 (0.05838)\n",
      "0:00:30.590979 elapsed for 23\n",
      "Epoch: [23][31/32]\tBatch 30.569 (22.201)\tData 30.270 (21.798)\tLoss 4.35348 (4.92229)\tAcc@1   7.21 (  3.53)\tAcc@5  22.60 ( 11.85)\tLearningRate 0.09937 (0.06009)\n",
      "0:00:30.682715 elapsed for 24\n",
      "Epoch: [24][31/32]\tBatch 29.392 (22.150)\tData 29.093 (21.747)\tLoss 4.33584 (4.89661)\tAcc@1  10.10 (  3.75)\tAcc@5  26.44 ( 12.41)\tLearningRate 0.09903 (0.06165)\n",
      "0:00:29.510742 elapsed for 25\n",
      "Epoch: [25][31/32]\tBatch 30.394 (22.152)\tData 30.097 (21.750)\tLoss 4.14263 (4.86866)\tAcc@1   9.13 (  3.98)\tAcc@5  31.25 ( 13.03)\tLearningRate 0.09860 (0.06308)\n",
      "0:00:30.510019 elapsed for 26\n",
      "Epoch: [26][31/32]\tBatch 30.026 (22.160)\tData 29.727 (21.757)\tLoss 4.08321 (4.83949)\tAcc@1  10.10 (  4.24)\tAcc@5  29.81 ( 13.67)\tLearningRate 0.09811 (0.06439)\n",
      "0:00:30.137455 elapsed for 27\n",
      "Epoch: [27][31/32]\tBatch 29.414 (22.130)\tData 29.117 (21.727)\tLoss 4.19603 (4.81010)\tAcc@1  10.58 (  4.51)\tAcc@5  25.96 ( 14.35)\tLearningRate 0.09753 (0.06558)\n",
      "0:00:29.526139 elapsed for 28\n",
      "Epoch: [28][31/32]\tBatch 28.981 (22.074)\tData 28.683 (21.671)\tLoss 3.99038 (4.77849)\tAcc@1  13.46 (  4.83)\tAcc@5  33.65 ( 15.06)\tLearningRate 0.09689 (0.06667)\n",
      "0:00:29.093753 elapsed for 29\n",
      "Epoch: [29][31/32]\tBatch 30.723 (22.078)\tData 30.425 (21.675)\tLoss 3.77498 (4.74590)\tAcc@1  13.46 (  5.17)\tAcc@5  39.42 ( 15.81)\tLearningRate 0.09617 (0.06767)\n",
      "0:00:30.831994 elapsed for 30\n",
      "Epoch: [30][31/32]\tBatch 30.490 (22.084)\tData 30.193 (21.681)\tLoss 3.66687 (4.71237)\tAcc@1  14.90 (  5.51)\tAcc@5  45.67 ( 16.58)\tLearningRate 0.09538 (0.06858)\n",
      "0:00:30.610386 elapsed for 31\n",
      "Epoch: [31][31/32]\tBatch 29.675 (22.089)\tData 29.377 (21.686)\tLoss 3.62018 (4.67835)\tAcc@1  21.15 (  5.88)\tAcc@5  38.46 ( 17.38)\tLearningRate 0.09452 (0.06940)\n",
      "0:00:29.790726 elapsed for 32\n",
      "Epoch: [32][31/32]\tBatch 30.094 (22.087)\tData 29.795 (21.685)\tLoss 3.57829 (4.64155)\tAcc@1  19.71 (  6.31)\tAcc@5  42.79 ( 18.25)\tLearningRate 0.09359 (0.07015)\n",
      "0:00:30.212250 elapsed for 33\n",
      "Epoch: [33][31/32]\tBatch 30.564 (22.111)\tData 30.265 (21.708)\tLoss 3.44151 (4.60422)\tAcc@1  22.12 (  6.74)\tAcc@5  48.08 ( 19.14)\tLearningRate 0.09260 (0.07082)\n",
      "0:00:30.680227 elapsed for 34\n",
      "Epoch: [34][31/32]\tBatch 30.262 (22.117)\tData 29.963 (21.715)\tLoss 3.16332 (4.56562)\tAcc@1  25.00 (  7.21)\tAcc@5  52.88 ( 20.06)\tLearningRate 0.09154 (0.07143)\n",
      "0:00:30.380096 elapsed for 35\n",
      "Epoch: [35][31/32]\tBatch 31.040 (22.155)\tData 30.743 (21.753)\tLoss 3.23360 (4.52681)\tAcc@1  24.04 (  7.71)\tAcc@5  52.88 ( 20.98)\tLearningRate 0.09041 (0.07197)\n",
      "0:00:31.169016 elapsed for 36\n",
      "Epoch: [36][31/32]\tBatch 30.446 (22.167)\tData 30.148 (21.765)\tLoss 2.76872 (4.48580)\tAcc@1  31.73 (  8.25)\tAcc@5  63.94 ( 21.95)\tLearningRate 0.08923 (0.07245)\n",
      "0:00:30.564911 elapsed for 37\n",
      "Epoch: [37][31/32]\tBatch 31.262 (22.182)\tData 30.964 (21.780)\tLoss 3.16562 (4.44463)\tAcc@1  30.29 (  8.82)\tAcc@5  52.88 ( 22.93)\tLearningRate 0.08798 (0.07288)\n",
      "0:00:31.379018 elapsed for 38\n",
      "Epoch: [38][31/32]\tBatch 30.517 (22.199)\tData 30.219 (21.797)\tLoss 2.86706 (4.40155)\tAcc@1  31.73 (  9.43)\tAcc@5  61.06 ( 23.93)\tLearningRate 0.08667 (0.07325)\n",
      "0:00:30.630621 elapsed for 39\n",
      "Epoch: [39][31/32]\tBatch 30.497 (22.202)\tData 30.198 (21.801)\tLoss 2.76606 (4.35720)\tAcc@1  32.21 ( 10.11)\tAcc@5  61.06 ( 24.95)\tLearningRate 0.08531 (0.07357)\n",
      "0:00:30.611965 elapsed for 40\n",
      "Epoch: [40][31/32]\tBatch 30.349 (22.208)\tData 30.050 (21.806)\tLoss 2.42820 (4.31281)\tAcc@1  43.75 ( 10.77)\tAcc@5  67.79 ( 25.98)\tLearningRate 0.08390 (0.07383)\n",
      "0:00:30.472211 elapsed for 41\n",
      "Epoch: [41][31/32]\tBatch 29.613 (22.194)\tData 29.315 (21.792)\tLoss 2.61839 (4.26708)\tAcc@1  36.06 ( 11.50)\tAcc@5  65.38 ( 27.02)\tLearningRate 0.08243 (0.07406)\n",
      "0:00:29.725231 elapsed for 42\n",
      "Epoch: [42][31/32]\tBatch 30.000 (22.183)\tData 29.702 (21.782)\tLoss 2.44263 (4.22198)\tAcc@1  40.38 ( 12.20)\tAcc@5  68.75 ( 28.05)\tLearningRate 0.08091 (0.07423)\n",
      "0:00:30.123364 elapsed for 43\n",
      "Epoch: [43][31/32]\tBatch 30.459 (22.187)\tData 30.161 (21.785)\tLoss 2.31989 (4.17660)\tAcc@1  40.38 ( 12.92)\tAcc@5  70.67 ( 29.08)\tLearningRate 0.07934 (0.07437)\n",
      "0:00:30.572661 elapsed for 44\n",
      "Epoch: [44][31/32]\tBatch 30.962 (22.206)\tData 30.663 (21.804)\tLoss 1.99592 (4.13033)\tAcc@1  55.29 ( 13.70)\tAcc@5  76.92 ( 30.09)\tLearningRate 0.07773 (0.07446)\n",
      "0:00:31.076092 elapsed for 45\n",
      "Epoch: [45][31/32]\tBatch 30.913 (22.216)\tData 30.615 (21.815)\tLoss 2.08666 (4.08475)\tAcc@1  45.19 ( 14.46)\tAcc@5  75.48 ( 31.09)\tLearningRate 0.07607 (0.07451)\n",
      "0:00:31.032454 elapsed for 46\n",
      "Epoch: [46][31/32]\tBatch 30.419 (22.215)\tData 30.121 (21.813)\tLoss 1.97331 (4.03936)\tAcc@1  46.15 ( 15.23)\tAcc@5  75.96 ( 32.07)\tLearningRate 0.07438 (0.07453)\n",
      "0:00:30.538489 elapsed for 47\n",
      "Epoch: [47][31/32]\tBatch 29.824 (22.197)\tData 29.525 (21.795)\tLoss 1.99618 (3.99452)\tAcc@1  46.15 ( 16.00)\tAcc@5  76.44 ( 33.03)\tLearningRate 0.07264 (0.07450)\n",
      "0:00:29.934053 elapsed for 48\n",
      "Epoch: [48][31/32]\tBatch 30.346 (22.192)\tData 30.048 (21.790)\tLoss 1.81385 (3.94942)\tAcc@1  53.85 ( 16.79)\tAcc@5  78.37 ( 33.98)\tLearningRate 0.07088 (0.07445)\n",
      "0:00:30.459769 elapsed for 49\n",
      "Epoch: [49][31/32]\tBatch 30.391 (22.193)\tData 30.092 (21.791)\tLoss 1.76780 (3.90483)\tAcc@1  53.85 ( 17.57)\tAcc@5  80.77 ( 34.92)\tLearningRate 0.06908 (0.07436)\n",
      "0:00:30.506419 elapsed for 50\n",
      "Epoch: [50][31/32]\tBatch 30.926 (22.189)\tData 30.628 (21.788)\tLoss 1.77797 (3.86001)\tAcc@1  49.52 ( 18.36)\tAcc@5  77.88 ( 35.86)\tLearningRate 0.06725 (0.07424)\n",
      "0:00:31.039620 elapsed for 51\n",
      "Epoch: [51][31/32]\tBatch 29.494 (22.179)\tData 29.195 (21.777)\tLoss 1.69659 (3.81543)\tAcc@1  54.81 ( 19.17)\tAcc@5  80.29 ( 36.79)\tLearningRate 0.06539 (0.07408)\n",
      "0:00:29.611425 elapsed for 52\n",
      "Epoch: [52][31/32]\tBatch 29.581 (22.158)\tData 29.283 (21.756)\tLoss 1.73930 (3.77229)\tAcc@1  56.73 ( 19.98)\tAcc@5  82.69 ( 37.67)\tLearningRate 0.06351 (0.07390)\n",
      "0:00:29.697836 elapsed for 53\n",
      "Epoch: [53][31/32]\tBatch 29.607 (22.141)\tData 29.309 (21.740)\tLoss 1.34558 (3.72944)\tAcc@1  68.27 ( 20.77)\tAcc@5  86.54 ( 38.53)\tLearningRate 0.06161 (0.07369)\n",
      "0:00:29.721333 elapsed for 54\n",
      "Epoch: [54][31/32]\tBatch 29.916 (22.131)\tData 29.618 (21.729)\tLoss 1.47501 (3.68693)\tAcc@1  64.42 ( 21.56)\tAcc@5  86.54 ( 39.38)\tLearningRate 0.05969 (0.07345)\n",
      "0:00:30.032178 elapsed for 55\n",
      "Epoch: [55][31/32]\tBatch 29.630 (22.122)\tData 29.332 (21.721)\tLoss 1.29572 (3.64503)\tAcc@1  66.35 ( 22.35)\tAcc@5  87.50 ( 40.22)\tLearningRate 0.05776 (0.07319)\n",
      "0:00:29.742127 elapsed for 56\n",
      "Epoch: [56][31/32]\tBatch 29.540 (22.109)\tData 29.243 (21.708)\tLoss 1.59878 (3.60433)\tAcc@1  59.62 ( 23.12)\tAcc@5  81.73 ( 41.04)\tLearningRate 0.05582 (0.07290)\n",
      "0:00:29.649732 elapsed for 57\n",
      "Epoch: [57][31/32]\tBatch 29.846 (22.105)\tData 29.549 (21.703)\tLoss 1.21045 (3.56398)\tAcc@1  66.35 ( 23.89)\tAcc@5  88.46 ( 41.82)\tLearningRate 0.05386 (0.07259)\n",
      "0:00:29.956911 elapsed for 58\n",
      "Epoch: [58][31/32]\tBatch 28.672 (22.088)\tData 28.374 (21.686)\tLoss 1.19433 (3.52362)\tAcc@1  72.60 ( 24.67)\tAcc@5  87.98 ( 42.61)\tLearningRate 0.05190 (0.07225)\n",
      "0:00:28.787841 elapsed for 59\n",
      "Epoch: [59][31/32]\tBatch 29.127 (22.064)\tData 28.829 (21.662)\tLoss 1.21568 (3.48421)\tAcc@1  67.31 ( 25.44)\tAcc@5  87.98 ( 43.37)\tLearningRate 0.04994 (0.07190)\n",
      "0:00:29.256922 elapsed for 60\n",
      "Epoch: [60][31/32]\tBatch 29.797 (22.062)\tData 29.496 (21.660)\tLoss 1.23695 (3.44502)\tAcc@1  69.23 ( 26.20)\tAcc@5  87.02 ( 44.12)\tLearningRate 0.04798 (0.07152)\n",
      "0:00:29.924030 elapsed for 61\n",
      "Epoch: [61][31/32]\tBatch 31.147 (22.086)\tData 30.848 (21.684)\tLoss 1.20186 (3.40727)\tAcc@1  65.87 ( 26.94)\tAcc@5  88.94 ( 44.85)\tLearningRate 0.04602 (0.07113)\n",
      "0:00:31.261792 elapsed for 62\n",
      "Epoch: [62][31/32]\tBatch 29.141 (22.062)\tData 28.843 (21.661)\tLoss 1.02561 (3.36992)\tAcc@1  73.56 ( 27.67)\tAcc@5  91.83 ( 45.56)\tLearningRate 0.04406 (0.07071)\n",
      "0:00:29.253711 elapsed for 63\n",
      "Epoch: [63][31/32]\tBatch 29.785 (22.052)\tData 29.486 (21.650)\tLoss 1.05869 (3.33327)\tAcc@1  72.60 ( 28.40)\tAcc@5  87.50 ( 46.24)\tLearningRate 0.04212 (0.07028)\n",
      "0:00:29.899177 elapsed for 64\n",
      "Epoch: [64][31/32]\tBatch 30.388 (22.062)\tData 30.089 (21.661)\tLoss 0.78357 (3.29710)\tAcc@1  81.73 ( 29.12)\tAcc@5  94.71 ( 46.92)\tLearningRate 0.04019 (0.06983)\n",
      "0:00:30.507647 elapsed for 65\n",
      "Epoch: [65][31/32]\tBatch 30.525 (22.059)\tData 30.226 (21.657)\tLoss 0.90627 (3.26127)\tAcc@1  78.85 ( 29.84)\tAcc@5  92.31 ( 47.59)\tLearningRate 0.03827 (0.06937)\n",
      "0:00:30.655910 elapsed for 66\n",
      "Epoch: [66][31/32]\tBatch 30.488 (22.064)\tData 30.188 (21.662)\tLoss 0.68657 (3.22606)\tAcc@1  79.81 ( 30.55)\tAcc@5  93.75 ( 48.24)\tLearningRate 0.03637 (0.06889)\n",
      "0:00:30.611527 elapsed for 67\n",
      "Epoch: [67][31/32]\tBatch 31.091 (22.081)\tData 30.793 (21.680)\tLoss 0.95439 (3.19163)\tAcc@1  75.48 ( 31.25)\tAcc@5  91.83 ( 48.88)\tLearningRate 0.03449 (0.06839)\n",
      "0:00:31.210673 elapsed for 68\n",
      "Epoch: [68][31/32]\tBatch 30.104 (22.071)\tData 29.805 (21.670)\tLoss 0.93375 (3.15767)\tAcc@1  78.85 ( 31.94)\tAcc@5  90.87 ( 49.50)\tLearningRate 0.03264 (0.06789)\n",
      "0:00:30.221047 elapsed for 69\n",
      "Epoch: [69][31/32]\tBatch 29.813 (22.071)\tData 29.515 (21.670)\tLoss 0.90326 (3.12458)\tAcc@1  77.88 ( 32.61)\tAcc@5  89.42 ( 50.11)\tLearningRate 0.03081 (0.06737)\n",
      "0:00:29.924611 elapsed for 70\n",
      "Epoch: [70][31/32]\tBatch 29.928 (22.063)\tData 29.630 (21.661)\tLoss 0.73724 (3.09220)\tAcc@1  81.25 ( 33.27)\tAcc@5  92.79 ( 50.70)\tLearningRate 0.02901 (0.06684)\n",
      "0:00:30.042869 elapsed for 71\n",
      "Epoch: [71][31/32]\tBatch 29.892 (22.059)\tData 29.594 (21.657)\tLoss 0.59600 (3.05976)\tAcc@1  84.62 ( 33.94)\tAcc@5  96.15 ( 51.28)\tLearningRate 0.02725 (0.06631)\n",
      "0:00:30.007142 elapsed for 72\n",
      "Epoch: [72][31/32]\tBatch 29.470 (22.048)\tData 29.171 (21.646)\tLoss 0.91666 (3.02835)\tAcc@1  77.88 ( 34.58)\tAcc@5  90.38 ( 51.85)\tLearningRate 0.02552 (0.06576)\n",
      "0:00:29.582454 elapsed for 73\n",
      "Epoch: [73][31/32]\tBatch 30.153 (22.046)\tData 29.855 (21.644)\tLoss 0.87140 (2.99776)\tAcc@1  77.88 ( 35.21)\tAcc@5  90.87 ( 52.40)\tLearningRate 0.02382 (0.06520)\n",
      "0:00:30.273881 elapsed for 74\n",
      "Epoch: [74][31/32]\tBatch 30.174 (22.048)\tData 29.876 (21.647)\tLoss 0.68541 (2.96766)\tAcc@1  83.17 ( 35.83)\tAcc@5  95.19 ( 52.93)\tLearningRate 0.02217 (0.06464)\n",
      "0:00:30.288615 elapsed for 75\n",
      "Epoch: [75][31/32]\tBatch 29.863 (22.049)\tData 29.564 (21.648)\tLoss 0.80320 (2.93797)\tAcc@1  80.29 ( 36.45)\tAcc@5  91.35 ( 53.46)\tLearningRate 0.02056 (0.06407)\n",
      "0:00:29.979865 elapsed for 76\n",
      "Epoch: [76][31/32]\tBatch 30.541 (22.058)\tData 30.242 (21.657)\tLoss 0.80258 (2.90877)\tAcc@1  77.40 ( 37.06)\tAcc@5  91.83 ( 53.98)\tLearningRate 0.01900 (0.06349)\n",
      "0:00:30.663598 elapsed for 77\n",
      "Epoch: [77][31/32]\tBatch 29.486 (22.053)\tData 29.188 (21.651)\tLoss 0.79363 (2.87968)\tAcc@1  81.73 ( 37.67)\tAcc@5  92.79 ( 54.49)\tLearningRate 0.01748 (0.06291)\n",
      "0:00:29.605814 elapsed for 78\n",
      "Epoch: [78][31/32]\tBatch 29.887 (22.049)\tData 29.589 (21.648)\tLoss 0.72067 (2.85080)\tAcc@1  81.73 ( 38.27)\tAcc@5  92.31 ( 55.00)\tLearningRate 0.01602 (0.06233)\n",
      "0:00:30.002180 elapsed for 79\n",
      "Epoch: [79][31/32]\tBatch 30.037 (22.049)\tData 29.740 (21.648)\tLoss 0.63677 (2.82255)\tAcc@1  83.17 ( 38.87)\tAcc@5  96.15 ( 55.49)\tLearningRate 0.01460 (0.06174)\n",
      "0:00:30.153553 elapsed for 80\n",
      "Epoch: [80][31/32]\tBatch 29.688 (22.043)\tData 29.390 (21.641)\tLoss 0.64431 (2.79504)\tAcc@1  84.62 ( 39.45)\tAcc@5  93.27 ( 55.97)\tLearningRate 0.01324 (0.06115)\n",
      "0:00:29.801219 elapsed for 81\n",
      "Epoch: [81][31/32]\tBatch 30.182 (22.037)\tData 29.882 (21.636)\tLoss 0.53316 (2.76784)\tAcc@1  87.02 ( 40.02)\tAcc@5  95.67 ( 56.44)\tLearningRate 0.01194 (0.06056)\n",
      "0:00:30.294808 elapsed for 82\n",
      "Epoch: [82][31/32]\tBatch 29.527 (22.036)\tData 29.229 (21.635)\tLoss 0.55930 (2.74107)\tAcc@1  87.02 ( 40.59)\tAcc@5  94.71 ( 56.90)\tLearningRate 0.01070 (0.05996)\n",
      "0:00:29.641538 elapsed for 83\n",
      "Epoch: [83][31/32]\tBatch 30.667 (22.045)\tData 30.369 (21.644)\tLoss 0.53471 (2.71438)\tAcc@1  85.58 ( 41.16)\tAcc@5  96.15 ( 57.36)\tLearningRate 0.00951 (0.05937)\n",
      "0:00:30.780532 elapsed for 84\n",
      "Epoch: [84][31/32]\tBatch 29.608 (22.034)\tData 29.311 (21.633)\tLoss 0.48383 (2.68838)\tAcc@1  90.87 ( 41.71)\tAcc@5  95.19 ( 57.80)\tLearningRate 0.00839 (0.05878)\n",
      "0:00:29.728209 elapsed for 85\n",
      "Epoch: [85][31/32]\tBatch 30.101 (22.031)\tData 29.802 (21.630)\tLoss 0.60232 (2.66284)\tAcc@1  87.02 ( 42.26)\tAcc@5  91.83 ( 58.24)\tLearningRate 0.00734 (0.05819)\n",
      "0:00:30.212189 elapsed for 86\n",
      "Epoch: [86][31/32]\tBatch 29.721 (22.029)\tData 29.424 (21.627)\tLoss 0.45178 (2.63764)\tAcc@1  89.90 ( 42.80)\tAcc@5  96.15 ( 58.67)\tLearningRate 0.00635 (0.05759)\n",
      "0:00:29.860183 elapsed for 87\n",
      "Epoch: [87][31/32]\tBatch 30.652 (22.031)\tData 30.354 (21.630)\tLoss 0.49248 (2.61290)\tAcc@1  89.90 ( 43.33)\tAcc@5  93.75 ( 59.09)\tLearningRate 0.00542 (0.05701)\n",
      "0:00:30.765178 elapsed for 88\n",
      "Epoch: [88][31/32]\tBatch 30.139 (22.036)\tData 29.841 (21.635)\tLoss 0.47469 (2.58861)\tAcc@1  90.38 ( 43.85)\tAcc@5  95.19 ( 59.50)\tLearningRate 0.00457 (0.05642)\n",
      "0:00:30.250440 elapsed for 89\n",
      "Epoch: [89][31/32]\tBatch 29.697 (22.031)\tData 29.398 (21.630)\tLoss 0.42755 (2.56485)\tAcc@1  88.94 ( 44.36)\tAcc@5  96.15 ( 59.90)\tLearningRate 0.00378 (0.05584)\n",
      "0:00:29.816915 elapsed for 90\n",
      "Epoch: [90][31/32]\tBatch 30.080 (22.034)\tData 29.781 (21.632)\tLoss 0.29174 (2.54117)\tAcc@1  94.23 ( 44.87)\tAcc@5  97.12 ( 60.30)\tLearningRate 0.00307 (0.05527)\n",
      "0:00:30.191911 elapsed for 91\n",
      "Epoch: [91][31/32]\tBatch 30.366 (22.039)\tData 30.066 (21.638)\tLoss 0.49192 (2.51809)\tAcc@1  87.02 ( 45.36)\tAcc@5  93.75 ( 60.69)\tLearningRate 0.00243 (0.05469)\n",
      "0:00:30.474527 elapsed for 92\n",
      "Epoch: [92][31/32]\tBatch 30.181 (22.043)\tData 29.883 (21.642)\tLoss 0.43070 (2.49553)\tAcc@1  89.90 ( 45.85)\tAcc@5  96.15 ( 61.07)\tLearningRate 0.00186 (0.05413)\n",
      "0:00:30.296289 elapsed for 93\n",
      "Epoch: [93][31/32]\tBatch 30.728 (22.052)\tData 30.430 (21.651)\tLoss 0.46412 (2.47323)\tAcc@1  89.42 ( 46.33)\tAcc@5  95.19 ( 61.44)\tLearningRate 0.00137 (0.05357)\n",
      "0:00:30.845581 elapsed for 94\n",
      "Epoch: [94][31/32]\tBatch 29.481 (22.050)\tData 29.183 (21.648)\tLoss 0.38703 (2.45143)\tAcc@1  91.83 ( 46.80)\tAcc@5  97.60 ( 61.81)\tLearningRate 0.00095 (0.05302)\n",
      "0:00:29.600538 elapsed for 95\n",
      "Epoch: [95][31/32]\tBatch 30.234 (22.055)\tData 29.936 (21.653)\tLoss 0.55202 (2.43022)\tAcc@1  87.98 ( 47.25)\tAcc@5  94.23 ( 62.16)\tLearningRate 0.00061 (0.05247)\n",
      "0:00:30.349254 elapsed for 96\n",
      "Epoch: [96][31/32]\tBatch 29.766 (22.055)\tData 29.468 (21.654)\tLoss 0.36974 (2.40917)\tAcc@1  91.83 ( 47.71)\tAcc@5  96.63 ( 62.52)\tLearningRate 0.00034 (0.05194)\n",
      "0:00:29.878833 elapsed for 97\n",
      "Epoch: [97][31/32]\tBatch 30.539 (22.064)\tData 30.241 (21.663)\tLoss 0.46442 (2.38859)\tAcc@1  89.42 ( 48.15)\tAcc@5  96.15 ( 62.86)\tLearningRate 0.00015 (0.05141)\n",
      "0:00:30.651167 elapsed for 98\n",
      "Epoch: [98][31/32]\tBatch 30.536 (22.073)\tData 30.239 (21.671)\tLoss 0.43558 (2.36842)\tAcc@1  90.38 ( 48.59)\tAcc@5  96.15 ( 63.20)\tLearningRate 0.00004 (0.05089)\n",
      "0:00:30.657858 elapsed for 99\n",
      "Epoch: [99][31/32]\tBatch 30.142 (22.072)\tData 29.844 (21.671)\tLoss 0.31194 (2.34871)\tAcc@1  94.71 ( 49.01)\tAcc@5  96.15 ( 63.53)\tLearningRate 0.00000 (0.05038)\n",
      "0:00:30.265927 elapsed for 100\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "high = 0.0\n",
    "epoch_time = AverageMeter('Epoch', ':6.3f')\n",
    "batch_time = AverageMeter('Batch', ':6.3f')\n",
    "data_time = AverageMeter('Data', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.5f')\n",
    "learning_rates = AverageMeter('LearningRate', ':.5f')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    time_ = datetime.datetime.now()    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    progress = ProgressMeter(\n",
    "        len(tr_loader),\n",
    "        [batch_time, data_time, losses, top1, top5, learning_rates],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    end = time.time()    \n",
    "    for i, (inputs, labels) in enumerate(tr_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #loss.backward()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print statistics\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        learning_rates.update(scheduler.get_lr()[0])        \n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            progress.display(i)\n",
    "            #running_loss = 0.0\n",
    "    progress.display(i)            \n",
    "    elapsed = datetime.datetime.now() - time_\n",
    "    print('{} elapsed for {}'.format(elapsed, epoch+1))\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_val(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7163288148240269"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_val(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_retrieval(model, val_loader):\n",
    "    feats = None\n",
    "    data_ids = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            feat = model(images, feature=True)\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "\n",
    "            feat = feat/np.linalg.norm(feat, axis=1)[:, np.newaxis]\n",
    "\n",
    "            if feats is None:\n",
    "                feats = feat\n",
    "            else:\n",
    "                feats = np.append(feats, feat, axis=0)\n",
    "\n",
    "            if data_ids is None:\n",
    "                data_ids = labels\n",
    "            else:\n",
    "                data_ids = np.append(data_ids, labels, axis=0)\n",
    "\n",
    "        score_matrix = feats.dot(feats.T)\n",
    "        np.fill_diagonal(score_matrix, -np.inf)\n",
    "        top1_reference_indices = np.argmax(score_matrix, axis=1)\n",
    "\n",
    "        top1_reference_ids = [\n",
    "            [data_ids[idx], data_ids[top1_reference_indices[idx]]] for idx in\n",
    "            range(len(data_ids))]\n",
    "\n",
    "    total_count = len(top1_reference_ids)\n",
    "    correct = 0\n",
    "    for ids in top1_reference_ids:\n",
    "        if ids[0] == ids[1]:\n",
    "            correct += 1        \n",
    "    return correct/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6301455042905111"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_retrieval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,    \n",
    "    \n",
    "}, './checkpoint/sfcar_resnet50_ep100.b0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
